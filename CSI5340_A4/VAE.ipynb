{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchsummary import summary\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTING DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST\n",
    "\n",
    "- Each img: (1,28,28)\n",
    "- Training: 60,000\n",
    "- Testing: 10,000\n",
    "\n",
    "\n",
    "### CIFAR10\n",
    "\n",
    "- Each img: (3,32,32)\n",
    "- Training: 50, 000\n",
    "- Testing: 10,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "mnist_training = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = True,                         \n",
    "    transform = ToTensor(),\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "mnist_testing = datasets.MNIST(\n",
    "    root = 'data', \n",
    "    train = False, \n",
    "    transform = ToTensor(),\n",
    ")\n",
    "\n",
    "datasets.CIFAR10.url=\"http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "\n",
    "cifar10_training = datasets.CIFAR10(\n",
    "    root = 'data',\n",
    "    train = True,                         \n",
    "    transform = ToTensor(),\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "cifar10_testing = datasets.CIFAR10(\n",
    "    root = 'data', \n",
    "    train = False, \n",
    "    transform = ToTensor(), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loaders\n",
    "\n",
    "mnist_data_loader = {\n",
    "    'train' : DataLoader(mnist_training, batch_size=BATCH_SIZE, shuffle=True, num_workers=1),\n",
    "    'test'  : DataLoader(mnist_testing, batch_size=BATCH_SIZE, shuffle=True, num_workers=1),\n",
    "}\n",
    "\n",
    "cifar10_data_loader = {\n",
    "    'train' : DataLoader(cifar10_training, batch_size=BATCH_SIZE, shuffle=True, num_workers=1),\n",
    "    'test'  : DataLoader(cifar10_testing, batch_size=BATCH_SIZE, shuffle=True, num_workers=1),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayMNIST(dataset, index, save=False, title=\"Title\"):\n",
    "    \n",
    "    display_img = dataset.data[index]\n",
    "    display_lab = dataset.targets[index].numpy()\n",
    "    \n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(display_img, cmap='gray')\n",
    "    ax.text(2,2, 'Label='+str(display_lab), bbox={'facecolor': 'white', 'pad': 2}, fontsize=12)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    if save: plt.savefig(title+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "displayMNIST(mnist_training, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decodeCIFARLabel(target):\n",
    "    \n",
    "    return ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck'][target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayCIFAR10(dataset, index, save=False, title=\"Title\"):\n",
    "    \n",
    "    display_img = dataset.data[index]\n",
    "    display_lab = dataset.targets[index]\n",
    "    \n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(display_img, cmap='gray', interpolation='nearest')\n",
    "    ax.text(2,2, 'Label='+str(display_lab)+\" (\"+decodeCIFARLabel(display_lab)+\")\", bbox={'facecolor': 'white', 'pad': 2}, fontsize=12)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    if save: plt.savefig(title+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "displayCIFAR10(cifar10_training, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://github.com/lyeoni/pytorch-mnist-VAE\n",
    "\n",
    "class VAE2(nn.Module):\n",
    "    def __init__(self, img_dim, latent_size):\n",
    "        \n",
    "        super(VAE2, self).__init__()\n",
    "        \n",
    "        x_dim = img_dim * img_dim\n",
    "        h_dim1 = 512\n",
    "        h_dim2 = 256\n",
    "        \n",
    "        # encoder part\n",
    "        self.fc1 = nn.Linear(x_dim, h_dim1)\n",
    "        self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
    "        self.fc31 = nn.Linear(h_dim2, latent_size)\n",
    "        self.fc32 = nn.Linear(h_dim2, latent_size)\n",
    "        \n",
    "        # decoder part\n",
    "        self.fc4 = nn.Linear(latent_size, h_dim2)\n",
    "        self.fc5 = nn.Linear(h_dim2, h_dim1)\n",
    "        self.fc6 = nn.Linear(h_dim1, x_dim)\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return self.fc31(h), self.fc32(h) # mu, log_var\n",
    "    \n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu) # return z sample\n",
    "        \n",
    "    def decoder(self, z):\n",
    "        h = F.relu(self.fc4(z))\n",
    "        h = F.relu(self.fc5(h))\n",
    "        return torch.sigmoid(self.fc6(h)) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_dim = x.shape[2] ** 2\n",
    "        mu, log_var = self.encoder(x.view(-1, x_dim))\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z), mu, log_var\n",
    "    \n",
    "#vae2_latent2 = VAE2(latent_size=2)\n",
    "#summary(vae2_latent2, (1,28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://github.com/lyeoni/pytorch-mnist-VAE\n",
    "\n",
    "class VAE3(nn.Module):\n",
    "    def __init__(self, img_dim, latent_size):\n",
    "        \n",
    "        super(VAE3, self).__init__()\n",
    "        \n",
    "        x_dim = img_dim * img_dim\n",
    "        h_dim1 = 1024\n",
    "        h_dim2 = 512\n",
    "        h_dim3 = 256\n",
    "        \n",
    "        # encoder part\n",
    "        self.fc1 = nn.Linear(x_dim, h_dim1)\n",
    "        self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
    "        self.fc3 = nn.Linear(h_dim2, h_dim3)\n",
    "        self.fc41 = nn.Linear(h_dim3, latent_size)\n",
    "        self.fc42 = nn.Linear(h_dim3, latent_size)\n",
    "        \n",
    "        # decoder part\n",
    "        self.fc5 = nn.Linear(latent_size, h_dim3)\n",
    "        self.fc6 = nn.Linear(h_dim3, h_dim2)\n",
    "        self.fc7 = nn.Linear(h_dim2, h_dim1)\n",
    "        self.fc8 = nn.Linear(h_dim1, x_dim)\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        h = F.relu(self.fc3(h))\n",
    "        return self.fc41(h), self.fc42(h) # mu, log_var\n",
    "    \n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu) # return z sample\n",
    "        \n",
    "    def decoder(self, z):\n",
    "        h = F.relu(self.fc5(z))\n",
    "        h = F.relu(self.fc6(h))\n",
    "        h = F.relu(self.fc7(h))\n",
    "        return torch.sigmoid(self.fc8(h)) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_dim = x.shape[2] ** 2\n",
    "        mu, log_var = self.encoder(x.view(-1, x_dim))\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z), mu, log_var\n",
    "    \n",
    "#vae3_latent5 = VAE3(img_dim=32, latent_size=5)\n",
    "#summary(vae3_latent5, (3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://github.com/lyeoni/pytorch-mnist-VAE\n",
    "\n",
    "class VAE4(nn.Module):\n",
    "    def __init__(self, img_dim, latent_size):\n",
    "        \n",
    "        super(VAE4, self).__init__()\n",
    "        \n",
    "        x_dim = img_dim * img_dim\n",
    "        h_dim1 = 2048\n",
    "        h_dim2 = 1024\n",
    "        h_dim3 = 512\n",
    "        h_dim4 = 256\n",
    "        \n",
    "        # encoder part\n",
    "        self.fc1 = nn.Linear(x_dim, h_dim1)\n",
    "        self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
    "        self.fc3 = nn.Linear(h_dim2, h_dim3)\n",
    "        self.fc4 = nn.Linear(h_dim3, h_dim4)\n",
    "        self.fc51 = nn.Linear(h_dim4, latent_size)\n",
    "        self.fc52 = nn.Linear(h_dim4, latent_size)\n",
    "        \n",
    "        # decoder part\n",
    "        self.fc5 = nn.Linear(latent_size, h_dim4)\n",
    "        self.fc6 = nn.Linear(h_dim4, h_dim3)\n",
    "        self.fc7 = nn.Linear(h_dim3, h_dim2)\n",
    "        self.fc8 = nn.Linear(h_dim2, h_dim1)\n",
    "        self.fc9 = nn.Linear(h_dim1, x_dim)\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        h = F.relu(self.fc3(h))\n",
    "        h = F.relu(self.fc4(h))\n",
    "        return self.fc51(h), self.fc52(h) # mu, log_var\n",
    "    \n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu) # return z sample\n",
    "        \n",
    "    def decoder(self, z):\n",
    "        h = F.relu(self.fc5(z))\n",
    "        h = F.relu(self.fc6(h))\n",
    "        h = F.relu(self.fc7(h))\n",
    "        h = F.relu(self.fc8(h))\n",
    "        return torch.sigmoid(self.fc9(h)) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_dim = x.shape[2] ** 2\n",
    "        mu, log_var = self.encoder(x.view(-1, x_dim))\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z), mu, log_var\n",
    "\n",
    "#vae4_latent10 = VAE4(latent_size=10)\n",
    "#summary(vae4_latent10, (1,28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TRAIN_VAE(total_epochs, vae, optimizer, data_loader):\n",
    "    \n",
    "    print(\"_________________________________________________________________ TRAINING\")\n",
    "    \n",
    "    train_loader = data_loader['train']\n",
    "    test_loader = data_loader['test']\n",
    "    \n",
    "    for epoch in range(total_epochs):\n",
    "        print(\"_________________________________\")\n",
    "        training_iteration(epoch+1, vae, train_loader, optimizer)\n",
    "        test(vae, test_loader)\n",
    "    \n",
    "def training_iteration(epoch, vae, train_loader, optimizer):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon_batch, mu, log_var = vae(data)\n",
    "        loss = loss_function(recon_batch, data, mu, log_var)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item() / len(data)))\n",
    "            \n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))\n",
    "    \n",
    "def test(vae, test_loader):\n",
    "    vae.eval()\n",
    "    test_loss= 0\n",
    "    with torch.no_grad():\n",
    "        for data, _ in test_loader:\n",
    "            recon, mu, log_var = vae(data)\n",
    "            \n",
    "            # sum up batch loss\n",
    "            test_loss += loss_function(recon, data, mu, log_var).item()\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))\n",
    "    \n",
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    x_dim = x.shape[2] ** 2\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, x_dim), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_Sample(vae, img_dim, latent_size, title):\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(64, latent_size)\n",
    "        sample = vae.decoder(z)\n",
    "\n",
    "        save_image(sample.view(64, 1, img_dim, img_dim), 'output/' +title+ '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 512]         401,920\n",
      "            Linear-2                  [-1, 256]         131,328\n",
      "            Linear-3                    [-1, 2]             514\n",
      "            Linear-4                    [-1, 2]             514\n",
      "            Linear-5                  [-1, 256]             768\n",
      "            Linear-6                  [-1, 512]         131,584\n",
      "            Linear-7                  [-1, 784]         402,192\n",
      "================================================================\n",
      "Total params: 1,068,820\n",
      "Trainable params: 1,068,820\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 4.08\n",
      "Estimated Total Size (MB): 4.10\n",
      "----------------------------------------------------------------\n",
      "_________________________________________________________________ TRAINING\n",
      "_________________________________\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 545.771875\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 188.206035\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 178.186660\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 171.998184\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 169.362695\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 170.684258\n",
      "====> Epoch: 1 Average loss: 179.5975\n",
      "====> Test set loss: 161.4936\n",
      "_________________________________\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 163.857207\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 157.561074\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 154.408145\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 164.032969\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 148.598369\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 161.555254\n",
      "====> Epoch: 2 Average loss: 157.7177\n",
      "====> Test set loss: 154.1262\n",
      "_________________________________\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 145.849053\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 157.934863\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 143.619893\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 162.631436\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 154.509814\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 145.517451\n",
      "====> Epoch: 3 Average loss: 152.4543\n",
      "====> Test set loss: 150.2742\n",
      "_________________________________\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 146.652354\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 149.328398\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 149.723271\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 151.380391\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 144.804072\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 153.682295\n",
      "====> Epoch: 4 Average loss: 149.8504\n",
      "====> Test set loss: 147.9467\n",
      "_________________________________\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 150.661953\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 150.596133\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 148.126514\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 144.558135\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 151.648965\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 149.513477\n",
      "====> Epoch: 5 Average loss: 147.9406\n",
      "====> Test set loss: 146.8105\n",
      "_________________________________\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 144.157920\n",
      "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 144.732100\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 146.281123\n",
      "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 145.033965\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 151.007852\n",
      "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 148.933242\n",
      "====> Epoch: 6 Average loss: 146.5855\n",
      "====> Test set loss: 145.6320\n",
      "_________________________________\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 144.226768\n",
      "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 148.531299\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 138.426768\n",
      "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 151.775410\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 139.127627\n",
      "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 140.217285\n",
      "====> Epoch: 7 Average loss: 145.4567\n",
      "====> Test set loss: 144.8512\n",
      "_________________________________\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 139.167725\n",
      "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 147.593301\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 137.423389\n",
      "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 150.930557\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 137.066982\n",
      "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 150.331523\n",
      "====> Epoch: 8 Average loss: 144.4501\n",
      "====> Test set loss: 144.3055\n",
      "_________________________________\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 144.201836\n",
      "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 137.192598\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 140.267773\n",
      "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 146.850596\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 142.633096\n",
      "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 139.698721\n",
      "====> Epoch: 9 Average loss: 143.9021\n",
      "====> Test set loss: 143.3726\n",
      "_________________________________\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 149.603652\n",
      "Train Epoch: 10 [10000/60000 (17%)]\tLoss: 142.842139\n",
      "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 146.255430\n",
      "Train Epoch: 10 [30000/60000 (50%)]\tLoss: 142.597441\n",
      "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 146.973486\n",
      "Train Epoch: 10 [50000/60000 (83%)]\tLoss: 146.909043\n",
      "====> Epoch: 10 Average loss: 143.2520\n",
      "====> Test set loss: 142.5995\n"
     ]
    }
   ],
   "source": [
    "vae2_latent2 = VAE2(img_dim = 28, latent_size=2)\n",
    "optimizer2_latent2 = optim.Adam(vae2_latent2.parameters())\n",
    "summary(vae2_latent2, (1,28,28))\n",
    "\n",
    "TRAIN_VAE(EPOCHS, vae2_latent2, optimizer2_latent2, mnist_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_Sample(vae2_latent2, img_dim=28, latent_size=2, title='mnist_2_2_sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                  [-1, 512]         524,800\n",
      "            Linear-2                  [-1, 256]         131,328\n",
      "            Linear-3                    [-1, 5]           1,285\n",
      "            Linear-4                    [-1, 5]           1,285\n",
      "            Linear-5                  [-1, 256]           1,536\n",
      "            Linear-6                  [-1, 512]         131,584\n",
      "            Linear-7                 [-1, 1024]         525,312\n",
      "================================================================\n",
      "Total params: 1,317,130\n",
      "Trainable params: 1,317,130\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 5.02\n",
      "Estimated Total Size (MB): 5.06\n",
      "----------------------------------------------------------------\n",
      "_________________________________________________________________ TRAINING\n",
      "_________________________________\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2133.642969\n",
      "Train Epoch: 1 [10000/50000 (20%)]\tLoss: 1975.147969\n",
      "Train Epoch: 1 [20000/50000 (40%)]\tLoss: 1935.553594\n",
      "Train Epoch: 1 [30000/50000 (60%)]\tLoss: 1953.975156\n",
      "Train Epoch: 1 [40000/50000 (80%)]\tLoss: 1920.033750\n",
      "====> Epoch: 1 Average loss: 1948.1302\n",
      "====> Test set loss: 1920.8784\n",
      "_________________________________\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1940.034531\n",
      "Train Epoch: 2 [10000/50000 (20%)]\tLoss: 1914.023438\n",
      "Train Epoch: 2 [20000/50000 (40%)]\tLoss: 1919.276875\n",
      "Train Epoch: 2 [30000/50000 (60%)]\tLoss: 1900.466563\n",
      "Train Epoch: 2 [40000/50000 (80%)]\tLoss: 1931.582656\n",
      "====> Epoch: 2 Average loss: 1909.7297\n",
      "====> Test set loss: 1904.7906\n",
      "_________________________________\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 1910.486094\n",
      "Train Epoch: 3 [10000/50000 (20%)]\tLoss: 1924.087344\n",
      "Train Epoch: 3 [20000/50000 (40%)]\tLoss: 1924.987031\n",
      "Train Epoch: 3 [30000/50000 (60%)]\tLoss: 1898.681406\n",
      "Train Epoch: 3 [40000/50000 (80%)]\tLoss: 1904.766719\n",
      "====> Epoch: 3 Average loss: 1900.5255\n",
      "====> Test set loss: 1901.3524\n",
      "_________________________________\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 1900.029844\n",
      "Train Epoch: 4 [10000/50000 (20%)]\tLoss: 1870.494375\n",
      "Train Epoch: 4 [20000/50000 (40%)]\tLoss: 1905.878750\n",
      "Train Epoch: 4 [30000/50000 (60%)]\tLoss: 1853.344063\n",
      "Train Epoch: 4 [40000/50000 (80%)]\tLoss: 1889.885156\n",
      "====> Epoch: 4 Average loss: 1898.3451\n",
      "====> Test set loss: 1899.4665\n",
      "_________________________________\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 1913.225156\n",
      "Train Epoch: 5 [10000/50000 (20%)]\tLoss: 1901.856250\n",
      "Train Epoch: 5 [20000/50000 (40%)]\tLoss: 1896.881562\n",
      "Train Epoch: 5 [30000/50000 (60%)]\tLoss: 1920.269375\n",
      "Train Epoch: 5 [40000/50000 (80%)]\tLoss: 1919.521094\n",
      "====> Epoch: 5 Average loss: 1896.5266\n",
      "====> Test set loss: 1898.2908\n",
      "_________________________________\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 1920.520000\n",
      "Train Epoch: 6 [10000/50000 (20%)]\tLoss: 1885.242656\n",
      "Train Epoch: 6 [20000/50000 (40%)]\tLoss: 1890.851875\n",
      "Train Epoch: 6 [30000/50000 (60%)]\tLoss: 1909.566406\n",
      "Train Epoch: 6 [40000/50000 (80%)]\tLoss: 1893.590938\n",
      "====> Epoch: 6 Average loss: 1895.4512\n",
      "====> Test set loss: 1898.7489\n",
      "_________________________________\n",
      "Train Epoch: 7 [0/50000 (0%)]\tLoss: 1902.784531\n",
      "Train Epoch: 7 [10000/50000 (20%)]\tLoss: 1895.634375\n",
      "Train Epoch: 7 [20000/50000 (40%)]\tLoss: 1868.644375\n",
      "Train Epoch: 7 [30000/50000 (60%)]\tLoss: 1870.873906\n",
      "Train Epoch: 7 [40000/50000 (80%)]\tLoss: 1878.312187\n",
      "====> Epoch: 7 Average loss: 1894.8153\n",
      "====> Test set loss: 1896.8654\n",
      "_________________________________\n",
      "Train Epoch: 8 [0/50000 (0%)]\tLoss: 1924.406563\n",
      "Train Epoch: 8 [10000/50000 (20%)]\tLoss: 1915.943125\n",
      "Train Epoch: 8 [20000/50000 (40%)]\tLoss: 1900.038906\n",
      "Train Epoch: 8 [30000/50000 (60%)]\tLoss: 1857.443906\n",
      "Train Epoch: 8 [40000/50000 (80%)]\tLoss: 1886.705781\n",
      "====> Epoch: 8 Average loss: 1893.9001\n",
      "====> Test set loss: 1896.4387\n",
      "_________________________________\n",
      "Train Epoch: 9 [0/50000 (0%)]\tLoss: 1829.729844\n",
      "Train Epoch: 9 [10000/50000 (20%)]\tLoss: 1918.522344\n",
      "Train Epoch: 9 [20000/50000 (40%)]\tLoss: 1886.683125\n",
      "Train Epoch: 9 [30000/50000 (60%)]\tLoss: 1930.758125\n",
      "Train Epoch: 9 [40000/50000 (80%)]\tLoss: 1917.918125\n",
      "====> Epoch: 9 Average loss: 1893.6360\n",
      "====> Test set loss: 1895.6583\n",
      "_________________________________\n",
      "Train Epoch: 10 [0/50000 (0%)]\tLoss: 1860.229375\n",
      "Train Epoch: 10 [10000/50000 (20%)]\tLoss: 1867.011875\n",
      "Train Epoch: 10 [20000/50000 (40%)]\tLoss: 1894.181562\n",
      "Train Epoch: 10 [30000/50000 (60%)]\tLoss: 1902.350625\n",
      "Train Epoch: 10 [40000/50000 (80%)]\tLoss: 1874.401094\n",
      "====> Epoch: 10 Average loss: 1892.7194\n",
      "====> Test set loss: 1895.9383\n"
     ]
    }
   ],
   "source": [
    "vae2_latent5 = VAE2(img_dim = 32, latent_size=5)\n",
    "optimizer2_latent5 = optim.Adam(vae2_latent5.parameters())\n",
    "summary(vae2_latent5, (3,32,32))\n",
    "\n",
    "TRAIN_VAE(EPOCHS, vae2_latent5, optimizer2_latent5, cifar10_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_Sample(vae2_latent5, img_dim=32, latent_size=5, title='cifar10_2_5_sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                 [-1, 2048]       2,099,200\n",
      "            Linear-2                 [-1, 1024]       2,098,176\n",
      "            Linear-3                  [-1, 512]         524,800\n",
      "            Linear-4                  [-1, 256]         131,328\n",
      "            Linear-5                    [-1, 2]             514\n",
      "            Linear-6                    [-1, 2]             514\n",
      "            Linear-7                  [-1, 256]             768\n",
      "            Linear-8                  [-1, 512]         131,584\n",
      "            Linear-9                 [-1, 1024]         525,312\n",
      "           Linear-10                 [-1, 2048]       2,099,200\n",
      "           Linear-11                 [-1, 1024]       2,098,176\n",
      "================================================================\n",
      "Total params: 9,709,572\n",
      "Trainable params: 9,709,572\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 0.07\n",
      "Params size (MB): 37.04\n",
      "Estimated Total Size (MB): 37.12\n",
      "----------------------------------------------------------------\n",
      "_________________________________________________________________ TRAINING\n",
      "_________________________________\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2129.566406\n",
      "Train Epoch: 1 [10000/50000 (20%)]\tLoss: 1983.830156\n",
      "Train Epoch: 1 [20000/50000 (40%)]\tLoss: 1994.051406\n",
      "Train Epoch: 1 [30000/50000 (60%)]\tLoss: 1949.538906\n",
      "Train Epoch: 1 [40000/50000 (80%)]\tLoss: 1940.085625\n",
      "====> Epoch: 1 Average loss: 1989.8848\n",
      "====> Test set loss: 1954.7807\n",
      "_________________________________\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1939.286719\n",
      "Train Epoch: 2 [10000/50000 (20%)]\tLoss: 1982.819375\n",
      "Train Epoch: 2 [20000/50000 (40%)]\tLoss: 1987.710625\n",
      "Train Epoch: 2 [30000/50000 (60%)]\tLoss: 1945.767656\n",
      "Train Epoch: 2 [40000/50000 (80%)]\tLoss: 1945.105000\n",
      "====> Epoch: 2 Average loss: 1949.4826\n",
      "====> Test set loss: 1951.2133\n",
      "_________________________________\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: 1926.336250\n",
      "Train Epoch: 3 [10000/50000 (20%)]\tLoss: 1947.005781\n",
      "Train Epoch: 3 [20000/50000 (40%)]\tLoss: 1968.858281\n",
      "Train Epoch: 3 [30000/50000 (60%)]\tLoss: 1971.222188\n",
      "Train Epoch: 3 [40000/50000 (80%)]\tLoss: 1928.314063\n",
      "====> Epoch: 3 Average loss: 1946.4437\n",
      "====> Test set loss: 1946.4965\n",
      "_________________________________\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: 1927.367969\n",
      "Train Epoch: 4 [10000/50000 (20%)]\tLoss: 1929.344531\n",
      "Train Epoch: 4 [20000/50000 (40%)]\tLoss: 1965.523125\n",
      "Train Epoch: 4 [30000/50000 (60%)]\tLoss: 1935.217656\n",
      "Train Epoch: 4 [40000/50000 (80%)]\tLoss: 1948.533125\n",
      "====> Epoch: 4 Average loss: 1943.0930\n",
      "====> Test set loss: 1945.2197\n",
      "_________________________________\n",
      "Train Epoch: 5 [0/50000 (0%)]\tLoss: 1943.996250\n",
      "Train Epoch: 5 [10000/50000 (20%)]\tLoss: 1963.815781\n",
      "Train Epoch: 5 [20000/50000 (40%)]\tLoss: 1943.045312\n",
      "Train Epoch: 5 [30000/50000 (60%)]\tLoss: 1917.374063\n",
      "Train Epoch: 5 [40000/50000 (80%)]\tLoss: 1952.039375\n",
      "====> Epoch: 5 Average loss: 1941.2130\n",
      "====> Test set loss: 1936.5219\n",
      "_________________________________\n",
      "Train Epoch: 6 [0/50000 (0%)]\tLoss: 1959.499063\n",
      "Train Epoch: 6 [10000/50000 (20%)]\tLoss: 1902.976406\n",
      "Train Epoch: 6 [20000/50000 (40%)]\tLoss: 1931.808438\n",
      "Train Epoch: 6 [30000/50000 (60%)]\tLoss: 1969.416250\n",
      "Train Epoch: 6 [40000/50000 (80%)]\tLoss: 1949.305625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-193-89a639cc0d96>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mTRAIN_VAE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvae4_latent2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer4_latent2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcifar10_data_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-180-eb2718c377ff>\u001b[0m in \u001b[0;36mTRAIN_VAE\u001b[1;34m(total_epochs, vae, optimizer, data_loader)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_________________________________\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mtraining_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvae\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-180-eb2718c377ff>\u001b[0m in \u001b[0;36mtraining_iteration\u001b[1;34m(epoch, vae, train_loader, optimizer)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecon_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_var\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alexa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 396\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    397\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alexa\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vae4_latent2 = VAE4(img_dim = 32, latent_size=2)\n",
    "optimizer4_latent2 = optim.Adam(vae4_latent2.parameters())\n",
    "summary(vae4_latent2, (3,32,32))\n",
    "\n",
    "EPOCHS=100\n",
    "TRAIN_VAE(EPOCHS, vae4_latent2, optimizer4_latent2, cifar10_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_Sample(vae4_latent2, img_dim=32, latent_size=2, title='cifar10_4_2_sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
