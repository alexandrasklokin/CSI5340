{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"3wkBZPv0BFwe","executionInfo":{"status":"ok","timestamp":1669482637122,"user_tz":300,"elapsed":3091,"user":{"displayName":"Alexandra Sklokin","userId":"06841525493811584768"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy\n","\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","from torchsummary import summary\n","from torchvision.utils import save_image\n","import pandas as pd \n","from matplotlib import pyplot"]},{"cell_type":"code","source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Not connected to a GPU')\n","else:\n","  print(gpu_info)"],"metadata":{"id":"w018SMbdBTWi","executionInfo":{"status":"ok","timestamp":1669482637327,"user_tz":300,"elapsed":221,"user":{"displayName":"Alexandra Sklokin","userId":"06841525493811584768"}},"outputId":"2ca067ea-f1f3-4476-abf9-da0e43c75d6f","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Sat Nov 26 17:10:36 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   40C    P0    41W / 400W |      0MiB / 40536MiB |      0%      Default |\n","|                               |                      |             Disabled |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["from psutil import virtual_memory\n","ram_gb = virtual_memory().total / 1e9\n","print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n","\n","if ram_gb < 20:\n","  print('Not using a high-RAM runtime')\n","else:\n","  print('You are using a high-RAM runtime!')"],"metadata":{"id":"nFTzCbmYBea7","executionInfo":{"status":"ok","timestamp":1669482637329,"user_tz":300,"elapsed":13,"user":{"displayName":"Alexandra Sklokin","userId":"06841525493811584768"}},"outputId":"8aa23235-6bcb-4da3-a5be-7f52684c7027","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Your runtime has 89.6 gigabytes of available RAM\n","\n","You are using a high-RAM runtime!\n"]}]},{"cell_type":"markdown","metadata":{"id":"8uC8QRwVBFwj"},"source":["# PARAMETERS"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"ZuDX-IzYBFwn","executionInfo":{"status":"ok","timestamp":1669482637331,"user_tz":300,"elapsed":9,"user":{"displayName":"Alexandra Sklokin","userId":"06841525493811584768"}}},"outputs":[],"source":["MNIST_BATCH_SIZE = 100\n","CIFAR10_BATCH_SIZE = 100\n","MNIST_EPOCHS = 30\n","CIFAR10_EPOCHS = 200"]},{"cell_type":"markdown","metadata":{"id":"ewbxswI2BFwn"},"source":["# IMPORTING DATA"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QBApbWOoZ35o","executionInfo":{"status":"ok","timestamp":1669482654615,"user_tz":300,"elapsed":17292,"user":{"displayName":"Alexandra Sklokin","userId":"06841525493811584768"}},"outputId":"53b07b28-15c7-442e-937f-2d770099b473"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"QsVJNSvxBFwo"},"source":["### MNIST\n","\n","- Each img: (1,28,28)\n","- Training: 60,000\n","- Testing: 10,000\n","\n","\n","### CIFAR10\n","\n","- Each img: (3,32,32)\n","- Training: 50, 000\n","- Testing: 10,000"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"PN9ojt-1BFwq","executionInfo":{"status":"ok","timestamp":1669482665891,"user_tz":300,"elapsed":11299,"user":{"displayName":"Alexandra Sklokin","userId":"06841525493811584768"}},"outputId":"edc06658-3a4f-4daf-9b2f-efeda068966f","colab":{"base_uri":"https://localhost:8080/","height":489,"referenced_widgets":["c364964415704e23b9aa8c3ace60ea4f","983329236e074cbd9d4645b0fb2e42f7","625ecc0da96840bb99b4cdeda686b248","8cdf869306724d61b26d2a647a522154","2c94b530bf3543fe8552fd44ac416ff9","e137ddc40b95460d997fac0cdd026608","a8e83639a7b24fdf92c8094115139f52","a14295da453a418c8ae7ba969d3b1afc","09ac13897046446db3e5acf96c229a45","f94def75db9c49d6ad7c35155c3fdec6","aa8334f7c4f344f3b5c3e3b48411b71a","cd64e1ea6dda4c629e415ff7296f05ca","b112034aa6ae4287b775773f566304f4","07afb1549962437bb8b328250968fcb4","77f822ae4d3442f8befadcdd49ac715d","e1c2ebebe97042afad332aac9b91cfe5","4b15de020e3c44b08c229cc2884f26df","8a8df30d972f4237bda4d3445a4c787d","5517d976eb7e4b37968907b64f35ba2a","74681e7488b34f69aa5f4d64c916c8fa","0fe1368dd74f4ee3a8ccaa13c6e10b89","c020d78a300a4c99b02d706fba5a95ad","a417ca31637b40ffa7df456e31dfb92e","8aeb05876d9045fcb88919cfa8a507d0","fddfd58fde344d57a742cedab483cf8c","13e335379f03488e8222fca2cd6bf95d","d47e7f80464b4f5cb982079b707e6844","b252d07673f54419a805a3863f29cfce","f0894581f0204e44b6dc259eb5e06dde","599f17a549ba468e813b5fd8c1ccfc18","b203a662f99f47e6873f5b7aa611c81a","d170ed4d5ba3483b8d1132154902075b","38f6c8df65df41818551f9c8b75695ee","f6821420a79043018d5dc4e34cf9dc28","0dd75013e1604c229e390726b2f366f0","00dfbdd9725d430db844e518fe658ad7","26e785db62ed4f4285750f452e8fabb9","b98c3457db2b4fdcb7e2808b26a0ff82","2c0509387e744fc389773cffb5a60de4","e03ec87ea4e64940a94a095255ead0b6","4273fa76c3124dd7b8c01dd6ef80f664","6787d5907ef34d4184e426dd248ea1a5","5873a605b5544ffdb3d6e09b4a7adfaa","f6a82f6398a24e60a50da95d9fdca93b","9c5d12627ce14498b9fc110238dd3b31","57d39357cf3d4ff18668621ef92b46ab","1a5a3b28e30943d395bc57238f628f59","3447d84201c643b9b4d81d25baa79963","5f01e62f4406442e84b09d235d43281f","c57dbf51c32a445aab578600e804f57b","a7a7493969144a4ab0a4e0c244b96bdf","ab4bdb5c4a2347d5ac2d461546d1389d","f427149eeace48e1ade9d797d2b52ebe","1aa657bcffa849ac802bfa4ad8cc7cb9","83349211fa1d49019258c11f251e5d01"]}},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/9912422 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c364964415704e23b9aa8c3ace60ea4f"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/28881 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd64e1ea6dda4c629e415ff7296f05ca"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1648877 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a417ca31637b40ffa7df456e31dfb92e"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/4542 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6821420a79043018d5dc4e34cf9dc28"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n","\n","Downloading http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/170498071 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c5d12627ce14498b9fc110238dd3b31"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting data/cifar-10-python.tar.gz to data\n"]}],"source":["mnist_training = datasets.MNIST(\n","    root = 'data',\n","    train = True,                         \n","    transform = ToTensor(),\n","    download=True,\n",")\n","\n","mnist_testing = datasets.MNIST(\n","    root = 'data', \n","    train = False, \n","    transform = ToTensor(),\n",")\n","\n","datasets.CIFAR10.url=\"http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n","\n","cifar10_training = datasets.CIFAR10(\n","    root = 'data',\n","    train = True,                         \n","    transform = ToTensor(),\n","    download=True,\n",")\n","\n","cifar10_testing = datasets.CIFAR10(\n","    root = 'data', \n","    train = False, \n","    transform = ToTensor(), \n",")"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"Sfdi68J6BFws","executionInfo":{"status":"ok","timestamp":1669482665903,"user_tz":300,"elapsed":48,"user":{"displayName":"Alexandra Sklokin","userId":"06841525493811584768"}}},"outputs":[],"source":["# Data Loaders\n","\n","mnist_data_loader = {\n","    'train' : DataLoader(mnist_training, batch_size=MNIST_BATCH_SIZE, shuffle=True, num_workers=1),\n","    'test'  : DataLoader(mnist_testing, batch_size=MNIST_BATCH_SIZE, shuffle=True, num_workers=1),\n","}\n","\n","cifar10_data_loader = {\n","    'train' : DataLoader(cifar10_training, batch_size=CIFAR10_BATCH_SIZE, shuffle=True, num_workers=1),\n","    'test'  : DataLoader(cifar10_testing, batch_size=CIFAR10_BATCH_SIZE, shuffle=True, num_workers=1),\n","}"]},{"cell_type":"markdown","metadata":{"id":"WB_zfAjIBFwu"},"source":["# HELPER FUNCTIONS"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"IUl8b7NlBFwu","executionInfo":{"status":"ok","timestamp":1669482741467,"user_tz":300,"elapsed":171,"user":{"displayName":"Alexandra Sklokin","userId":"06841525493811584768"}}},"outputs":[],"source":["def displayMNIST(dataset, index, save=False, title=\"Title\"):\n","    \n","    display_img = dataset.data[index]\n","    display_lab = dataset.targets[index].numpy()\n","    \n","    fig, ax = plt.subplots(1)\n","    ax.imshow(display_img, cmap='gray')\n","    ax.text(2,2, 'Label='+str(display_lab), bbox={'facecolor': 'white', 'pad': 2}, fontsize=12)\n","    plt.axis('off')\n","    \n","    if save: plt.savefig(title+'.png')"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"q8OfwBhiBFww","executionInfo":{"status":"ok","timestamp":1669482742800,"user_tz":300,"elapsed":252,"user":{"displayName":"Alexandra Sklokin","userId":"06841525493811584768"}},"outputId":"74a46f08-f7d6-4c02-8add-9d6bd4b5347e","colab":{"base_uri":"https://localhost:8080/","height":248}},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["displayMNIST(mnist_training, 0)"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"N1zKoATkBFww","executionInfo":{"status":"ok","timestamp":1669482743864,"user_tz":300,"elapsed":173,"user":{"displayName":"Alexandra Sklokin","userId":"06841525493811584768"}}},"outputs":[],"source":["def decodeCIFARLabel(target):\n","    \n","    return ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck'][target]"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"M13eWvclBFwx","executionInfo":{"status":"ok","timestamp":1669482744758,"user_tz":300,"elapsed":8,"user":{"displayName":"Alexandra Sklokin","userId":"06841525493811584768"}}},"outputs":[],"source":["def displayCIFAR10(dataset, index, save=False, title=\"Title\"):\n","    \n","    display_img = dataset.data[index]\n","    display_lab = dataset.targets[index]\n","    \n","    fig, ax = plt.subplots(1)\n","    ax.imshow(display_img, cmap='gray', interpolation='nearest')\n","    ax.text(2,2, 'Label='+str(display_lab)+\" (\"+decodeCIFARLabel(display_lab)+\")\", bbox={'facecolor': 'white', 'pad': 2}, fontsize=12)\n","    plt.axis('off')\n","    \n","    if save: plt.savefig(title+'.png')"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"0oZ-khBFBFwz","executionInfo":{"status":"ok","timestamp":1669482746200,"user_tz":300,"elapsed":213,"user":{"displayName":"Alexandra Sklokin","userId":"06841525493811584768"}},"outputId":"135d26d4-fd71-493f-d417-59a80b7aa2a5","colab":{"base_uri":"https://localhost:8080/","height":248}},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["displayCIFAR10(cifar10_training, 0)"]},{"cell_type":"code","source":["displayCIFAR10(cifar10_training, 4)"],"metadata":{"id":"qP1wlUehBwmy","executionInfo":{"status":"ok","timestamp":1669482747990,"user_tz":300,"elapsed":258,"user":{"displayName":"Alexandra Sklokin","userId":"06841525493811584768"}},"outputId":"14cbcaa2-9b0b-48c3-fa07-487c2b176e90","colab":{"base_uri":"https://localhost:8080/","height":248}},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"OM0_NqnwBFw2"},"source":["# DIFFERENT VAE ARCHITECTURES"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"gr4Bq0yNBFw2","executionInfo":{"status":"ok","timestamp":1669482749414,"user_tz":300,"elapsed":176,"user":{"displayName":"Alexandra Sklokin","userId":"06841525493811584768"}}},"outputs":[],"source":["# Reference: https://github.com/lyeoni/pytorch-mnist-VAE\n","\n","class VAE2(nn.Module):\n","    def __init__(self, img_dim, latent_size):\n","        \n","        super(VAE2, self).__init__()\n","        \n","        x_dim = img_dim * img_dim\n","        h_dim1 = 512\n","        h_dim2 = 256\n","        \n","        # encoder part\n","        self.fc1 = nn.Linear(x_dim, h_dim1)\n","        self.fc2 = nn.Linear(h_dim1, h_dim2)\n","        self.fc31 = nn.Linear(h_dim2, latent_size)\n","        self.fc32 = nn.Linear(h_dim2, latent_size)\n","        \n","        # decoder part\n","        self.fc4 = nn.Linear(latent_size, h_dim2)\n","        self.fc5 = nn.Linear(h_dim2, h_dim1)\n","        self.fc6 = nn.Linear(h_dim1, x_dim)\n","        \n","    def encoder(self, x):\n","        h = F.relu(self.fc1(x))\n","        h = F.relu(self.fc2(h))\n","        return self.fc31(h), self.fc32(h) # mu, log_var\n","    \n","    def sampling(self, mu, log_var):\n","        std = torch.exp(0.5*log_var)\n","        eps = torch.randn_like(std)\n","        return eps.mul(std).add_(mu) # return z sample\n","        \n","    def decoder(self, z):\n","        h = F.relu(self.fc4(z))\n","        h = F.relu(self.fc5(h))\n","        return torch.sigmoid(self.fc6(h)) \n","    \n","    def forward(self, x):\n","        x_dim = x.shape[2] ** 2\n","        mu, log_var = self.encoder(x.view(-1, x_dim))\n","        z = self.sampling(mu, log_var)\n","        return self.decoder(z), mu, log_var\n","    \n","#vae2_latent2 = VAE2(latent_size=2)\n","#summary(vae2_latent2, (1,28,28))"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"LgfE73hhBFw2","executionInfo":{"status":"ok","timestamp":1669482750821,"user_tz":300,"elapsed":163,"user":{"displayName":"Alexandra Sklokin","userId":"06841525493811584768"}}},"outputs":[],"source":["# Reference: https://github.com/lyeoni/pytorch-mnist-VAE\n","\n","class VAE3(nn.Module):\n","    def __init__(self, img_dim, latent_size):\n","        \n","        super(VAE3, self).__init__()\n","        \n","        x_dim = img_dim * img_dim\n","        h_dim1 = 1024\n","        h_dim2 = 512\n","        h_dim3 = 256\n","        \n","        # encoder part\n","        self.fc1 = nn.Linear(x_dim, h_dim1)\n","        self.fc2 = nn.Linear(h_dim1, h_dim2)\n","        self.fc3 = nn.Linear(h_dim2, h_dim3)\n","        self.fc41 = nn.Linear(h_dim3, latent_size)\n","        self.fc42 = nn.Linear(h_dim3, latent_size)\n","        \n","        # decoder part\n","        self.fc5 = nn.Linear(latent_size, h_dim3)\n","        self.fc6 = nn.Linear(h_dim3, h_dim2)\n","        self.fc7 = nn.Linear(h_dim2, h_dim1)\n","        self.fc8 = nn.Linear(h_dim1, x_dim)\n","        \n","    def encoder(self, x):\n","        h = F.relu(self.fc1(x))\n","        h = F.relu(self.fc2(h))\n","        h = F.relu(self.fc3(h))\n","        return self.fc41(h), self.fc42(h) # mu, log_var\n","    \n","    def sampling(self, mu, log_var):\n","        std = torch.exp(0.5*log_var)\n","        eps = torch.randn_like(std)\n","        return eps.mul(std).add_(mu) # return z sample\n","        \n","    def decoder(self, z):\n","        h = F.relu(self.fc5(z))\n","        h = F.relu(self.fc6(h))\n","        h = F.relu(self.fc7(h))\n","        return torch.sigmoid(self.fc8(h)) \n","    \n","    def forward(self, x):\n","        x_dim = x.shape[2] ** 2\n","        mu, log_var = self.encoder(x.view(-1, x_dim))\n","        z = self.sampling(mu, log_var)\n","        return self.decoder(z), mu, log_var\n","    \n","#vae3_latent5 = VAE3(img_dim=32, latent_size=5)\n","#summary(vae3_latent5, (3,32,32))"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"yVVaCcFkBFw3","executionInfo":{"status":"ok","timestamp":1669482752934,"user_tz":300,"elapsed":8,"user":{"displayName":"Alexandra Sklokin","userId":"06841525493811584768"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e51d9855-f4ff-4037-861a-85e337107357"},"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Linear-1                 [-1, 2048]       1,607,680\n","            Linear-2                 [-1, 1024]       2,098,176\n","            Linear-3                  [-1, 512]         524,800\n","            Linear-4                  [-1, 256]         131,328\n","            Linear-5                    [-1, 5]           1,285\n","            Linear-6                    [-1, 5]           1,285\n","            Linear-7                  [-1, 256]           1,536\n","            Linear-8                  [-1, 512]         131,584\n","            Linear-9                 [-1, 1024]         525,312\n","           Linear-10                 [-1, 2048]       2,099,200\n","           Linear-11                  [-1, 784]       1,606,416\n","================================================================\n","Total params: 8,728,602\n","Trainable params: 8,728,602\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.06\n","Params size (MB): 33.30\n","Estimated Total Size (MB): 33.36\n","----------------------------------------------------------------\n"]}],"source":["# Reference: https://github.com/lyeoni/pytorch-mnist-VAE\n","\n","class VAE4(nn.Module):\n","    def __init__(self, img_dim, latent_size):\n","        \n","        super(VAE4, self).__init__()\n","        \n","        x_dim = img_dim * img_dim\n","        h_dim1 = 2048\n","        h_dim2 = 1024\n","        h_dim3 = 512\n","        h_dim4 = 256\n","        \n","        # encoder part\n","        self.fc1 = nn.Linear(x_dim, h_dim1)\n","        self.fc2 = nn.Linear(h_dim1, h_dim2)\n","        self.fc3 = nn.Linear(h_dim2, h_dim3)\n","        self.fc4 = nn.Linear(h_dim3, h_dim4)\n","        self.fc51 = nn.Linear(h_dim4, latent_size)\n","        self.fc52 = nn.Linear(h_dim4, latent_size)\n","        \n","        # decoder part\n","        self.fc5 = nn.Linear(latent_size, h_dim4)\n","        self.fc6 = nn.Linear(h_dim4, h_dim3)\n","        self.fc7 = nn.Linear(h_dim3, h_dim2)\n","        self.fc8 = nn.Linear(h_dim2, h_dim1)\n","        self.fc9 = nn.Linear(h_dim1, x_dim)\n","        \n","    def encoder(self, x):\n","        h = F.relu(self.fc1(x))\n","        h = F.relu(self.fc2(h))\n","        h = F.relu(self.fc3(h))\n","        h = F.relu(self.fc4(h))\n","        return self.fc51(h), self.fc52(h) # mu, log_var\n","    \n","    def sampling(self, mu, log_var):\n","        std = torch.exp(0.5*log_var)\n","        eps = torch.randn_like(std)\n","        return eps.mul(std).add_(mu) # return z sample\n","        \n","    def decoder(self, z):\n","        h = F.relu(self.fc5(z))\n","        h = F.relu(self.fc6(h))\n","        h = F.relu(self.fc7(h))\n","        h = F.relu(self.fc8(h))\n","        return torch.sigmoid(self.fc9(h)) \n","    \n","    def forward(self, x):\n","        x_dim = x.shape[2] ** 2\n","        mu, log_var = self.encoder(x.view(-1, x_dim))\n","        z = self.sampling(mu, log_var)\n","        return self.decoder(z), mu, log_var\n","\n","vae4_latent5 = VAE4(latent_size=5, img_dim=28)\n","vae4_latent5.cuda()\n","summary(vae4_latent5, (1,28,28))"]},{"cell_type":"code","source":["# Reference: https://github.com/SashaMalysheva/Pytorch-VAE/blob/master/model.py\n","\n","class ConvVAE2(nn.Module):\n","    def __init__(self, img_dim, latent_size, channels):\n","        \n","        super(ConvVAE2, self).__init__()\n","\n","        self.kernel_num = 128\n","        self.volume = self.kernel_num * ((img_dim // 8) ** 2)\n","        self.img_dim = img_dim\n","        \n","        # encoder part\n","        self.conv1 =  nn.Conv2d(channels, self.kernel_num // 2, kernel_size=4, stride=2, padding=1)\n","        self.conv2 =  nn.Conv2d(self.kernel_num // 2, self.kernel_num, kernel_size=4, stride=2, padding=1)\n","\n","        self.fc31 = nn.Linear(self.volume, latent_size)\n","        self.fc32 = nn.Linear(self.volume, latent_size)\n","        \n","        # decoder part\n","        self.project = nn.Linear(latent_size, self.volume)\n","        self.conv5 = nn.ConvTranspose2d(8, self.kernel_num // 2, kernel_size=4, stride=2, padding=1)\n","        self.conv6 = nn.ConvTranspose2d(self.kernel_num // 2, channels, kernel_size=4, stride=2, padding=1)\n","        self.fc7 = nn.MaxPool2d(4, 4)\n","        \n","        \n","    def encoder(self, x):\n","        h = F.relu(self.conv1(x))\n","        h = F.relu(self.conv2(h))\n","\n","        h_enrolled = h.view(-1, self.volume)       \n","        mu = self.fc31(h_enrolled)\n","        log_var = self.fc32(h_enrolled)\n","\n","        return mu, log_var\n","    \n","    def sampling(self, mu, log_var):\n","        std = torch.exp(0.5*log_var)\n","        eps = torch.randn_like(std)\n","        return eps.mul(std).add_(mu) # return z sample\n","        \n","        \n","    def decoder(self, z):\n","        h = self.project(z)\n","        h = h.view(-1, 8, self.img_dim, self.img_dim)\n","        h = F.relu(self.conv5(h))\n","        h = F.relu(self.conv6(h))\n","        h = F.relu(self.fc7(h))\n","        h = torch.sigmoid(h)\n","        return h\n","    \n","    def forward(self, x):\n","        mu, log_var = self.encoder(x)\n","        z = self.sampling(mu, log_var)\n","        \n","        return self.decoder(z), mu, log_var\n","    \n","vae2_latent2 = ConvVAE2(latent_size=2, img_dim=32, channels=3)\n","vae2_latent2.cuda()\n","summary(vae2_latent2, (3,32,32))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o0_J-3HxuNEF","executionInfo":{"status":"ok","timestamp":1669482756079,"user_tz":300,"elapsed":213,"user":{"displayName":"Alexandra Sklokin","userId":"06841525493811584768"}},"outputId":"89167d06-6838-4455-c20b-58865fa1ad17"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 64, 16, 16]           3,136\n","            Conv2d-2            [-1, 128, 8, 8]         131,200\n","            Linear-3                    [-1, 2]           4,098\n","            Linear-4                    [-1, 2]           4,098\n","            Linear-5                 [-1, 2048]           6,144\n","   ConvTranspose2d-6           [-1, 64, 64, 64]           8,256\n","   ConvTranspose2d-7          [-1, 3, 128, 128]           3,075\n","         MaxPool2d-8            [-1, 3, 32, 32]               0\n","================================================================\n","Total params: 160,007\n","Trainable params: 160,007\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 2.60\n","Params size (MB): 0.61\n","Estimated Total Size (MB): 3.22\n","----------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["# Reference: https://github.com/SashaMalysheva/Pytorch-VAE/blob/master/model.py\n","\n","class ConvVAE3(nn.Module):\n","    def __init__(self, img_dim, latent_size, channels):\n","        \n","        super(ConvVAE3, self).__init__()\n","\n","        self.kernel_num = 256\n","        self.volume = self.kernel_num * ((img_dim // 8) ** 2)\n","        self.img_dim = img_dim\n","        \n","        # encoder part\n","        self.conv1 =  nn.Conv2d(channels, self.kernel_num // 4, kernel_size=4, stride=2, padding=1)\n","        self.conv2 =  nn.Conv2d(self.kernel_num // 4, self.kernel_num // 2, kernel_size=4, stride=2, padding=1)\n","        self.conv3 =  nn.Conv2d(self.kernel_num // 2, self.kernel_num, kernel_size=4, stride=2, padding=1)\n","\n","        self.fc31 = nn.Linear(self.volume, latent_size)\n","        self.fc32 = nn.Linear(self.volume, latent_size)\n","        \n","        # decoder part\n","        self.project = nn.Linear(latent_size, self.volume)\n","        self.conv5 = nn.ConvTranspose2d(8, self.kernel_num // 4, kernel_size=4, stride=2, padding=1)\n","        self.conv6 = nn.ConvTranspose2d(self.kernel_num // 4, self.kernel_num // 2, kernel_size=4, stride=2, padding=1)\n","        self.conv7 = nn.ConvTranspose2d(self.kernel_num // 2, channels, kernel_size=4, stride=2, padding=1)\n","        self.fc8 = nn.MaxPool2d(8, 8)\n","        \n","        \n","    def encoder(self, x):\n","        h = F.relu(self.conv1(x))\n","        h = F.relu(self.conv2(h))\n","        h = F.relu(self.conv3(h))\n","\n","        h_enrolled = h.view(-1, self.volume)       \n","        mu = self.fc31(h_enrolled)\n","        log_var = self.fc32(h_enrolled)\n","\n","        return mu, log_var\n","    \n","    def sampling(self, mu, log_var):\n","        std = torch.exp(0.5*log_var)\n","        eps = torch.randn_like(std)\n","        return eps.mul(std).add_(mu) # return z sample\n","        \n","        \n","    def decoder(self, z):\n","        h = self.project(z)\n","        h = h.view(-1, 8, self.img_dim, self.img_dim)\n","        h = F.relu(self.conv5(h))\n","        h = F.relu(self.conv6(h))\n","        h = F.relu(self.conv7(h))\n","        h = F.relu(self.fc8(h))\n","        h = torch.sigmoid(h)\n","        return h\n","    \n","    def forward(self, x):\n","        mu, log_var = self.encoder(x)\n","        z = self.sampling(mu, log_var)\n","        \n","        return self.decoder(z), mu, log_var\n","    \n","vae3_latent5 = ConvVAE2(latent_size=5, img_dim=32, channels=3)\n","vae3_latent5.cuda()\n","summary(vae3_latent5, (3,32,32))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KH4fd8nr2wyX","executionInfo":{"status":"ok","timestamp":1669482758439,"user_tz":300,"elapsed":175,"user":{"displayName":"Alexandra Sklokin","userId":"06841525493811584768"}},"outputId":"f461ee00-bd7d-42d7-e5e8-1534463e991a"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 64, 16, 16]           3,136\n","            Conv2d-2            [-1, 128, 8, 8]         131,200\n","            Linear-3                    [-1, 5]          10,245\n","            Linear-4                    [-1, 5]          10,245\n","            Linear-5                 [-1, 2048]          12,288\n","   ConvTranspose2d-6           [-1, 64, 64, 64]           8,256\n","   ConvTranspose2d-7          [-1, 3, 128, 128]           3,075\n","         MaxPool2d-8            [-1, 3, 32, 32]               0\n","================================================================\n","Total params: 178,445\n","Trainable params: 178,445\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 2.60\n","Params size (MB): 0.68\n","Estimated Total Size (MB): 3.29\n","----------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["# Reference: https://github.com/SashaMalysheva/Pytorch-VAE/blob/master/model.py\n","\n","class ConvVAE4(nn.Module):\n","    def __init__(self, img_dim, latent_size, channels):\n","        \n","        super(ConvVAE4, self).__init__()\n","\n","        self.kernel_num = 256\n","        self.volume = (self.kernel_num) * ((img_dim // 16) ** 2)\n","        self.img_dim = img_dim\n","        \n","        # encoder part\n","        self.conv1 =  nn.Conv2d(channels, self.kernel_num // 8, kernel_size=4, stride=2, padding=1)\n","        self.conv2 =  nn.Conv2d(self.kernel_num // 8, self.kernel_num // 4, kernel_size=4, stride=2, padding=1)\n","        self.conv3 =  nn.Conv2d(self.kernel_num // 4, self.kernel_num //2, kernel_size=4, stride=2, padding=1)\n","        self.conv4 =  nn.Conv2d(self.kernel_num // 2, self.kernel_num, kernel_size=4, stride=2, padding=1)\n","\n","        self.fc31 = nn.Linear(self.volume, latent_size)\n","        self.fc32 = nn.Linear(self.volume, latent_size)\n","        \n","        # decoder part\n","        self.project = nn.Linear(latent_size, self.volume)\n","        self.conv5 = nn.ConvTranspose2d(2, self.kernel_num // 8, kernel_size=4, stride=2, padding=1)\n","        self.conv6 = nn.ConvTranspose2d(self.kernel_num // 8, self.kernel_num // 4, kernel_size=4, stride=2, padding=1)\n","        self.conv7 = nn.ConvTranspose2d(self.kernel_num // 4, self.kernel_num // 2, kernel_size=4, stride=2, padding=1)\n","        self.conv8 = nn.ConvTranspose2d(self.kernel_num // 2, channels, kernel_size=4, stride=2, padding=1)\n","        self.fc9 = nn.MaxPool2d(16, 16)\n","        \n","        \n","    def encoder(self, x):\n","        h = F.relu(self.conv1(x))\n","        h = F.relu(self.conv2(h))\n","        h = F.relu(self.conv3(h))\n","        h = F.relu(self.conv4(h))\n","\n","        h_enrolled = h.view(-1, self.volume)       \n","        mu = self.fc31(h_enrolled)\n","        log_var = self.fc32(h_enrolled)\n","\n","        return mu, log_var\n","    \n","    def sampling(self, mu, log_var):\n","        std = torch.exp(0.5*log_var)\n","        eps = torch.randn_like(std)\n","        return eps.mul(std).add_(mu) # return z sample\n","        \n","        \n","    def decoder(self, z):\n","        h = self.project(z)\n","        h = h.view(-1, 2, self.img_dim, self.img_dim)\n","        h = F.relu(self.conv5(h))\n","        h = F.relu(self.conv6(h))\n","        h = F.relu(self.conv7(h))\n","        h = F.relu(self.conv8(h))\n","        h = F.relu(self.fc9(h))\n","        h = torch.sigmoid(h)\n","        return h\n","    \n","    def forward(self, x):\n","        mu, log_var = self.encoder(x)\n","        z = self.sampling(mu, log_var)\n","        \n","        return self.decoder(z), mu, log_var\n","    \n","vae4_latent10 = ConvVAE4(latent_size=10, img_dim=32, channels=3)\n","vae4_latent10.cuda()\n","summary(vae4_latent10, (3,32,32))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tZfxVtUp3kf-","executionInfo":{"status":"ok","timestamp":1669482761123,"user_tz":300,"elapsed":160,"user":{"displayName":"Alexandra Sklokin","userId":"06841525493811584768"}},"outputId":"ac7acba2-adc4-41f5-8b09-8b59bfe9e617"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 32, 16, 16]           1,568\n","            Conv2d-2             [-1, 64, 8, 8]          32,832\n","            Conv2d-3            [-1, 128, 4, 4]         131,200\n","            Conv2d-4            [-1, 256, 2, 2]         524,544\n","            Linear-5                   [-1, 10]          10,250\n","            Linear-6                   [-1, 10]          10,250\n","            Linear-7                 [-1, 1024]          11,264\n","   ConvTranspose2d-8           [-1, 32, 64, 64]           1,056\n","   ConvTranspose2d-9         [-1, 64, 128, 128]          32,832\n","  ConvTranspose2d-10        [-1, 128, 256, 256]         131,200\n","  ConvTranspose2d-11          [-1, 3, 512, 512]           6,147\n","        MaxPool2d-12            [-1, 3, 32, 32]               0\n","================================================================\n","Total params: 893,143\n","Trainable params: 893,143\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 79.15\n","Params size (MB): 3.41\n","Estimated Total Size (MB): 82.57\n","----------------------------------------------------------------\n"]}]},{"cell_type":"markdown","metadata":{"id":"NbHaz1amBFw3"},"source":["# Experiment"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"1CGhtZ1zBFw4","executionInfo":{"status":"ok","timestamp":1669482763953,"user_tz":300,"elapsed":178,"user":{"displayName":"Alexandra Sklokin","userId":"06841525493811584768"}}},"outputs":[],"source":["def TRAIN_VAE(total_epochs, vae, optimizer, data_loader, mnist):\n","    \n","    print(\"_________________________________________________________________ TRAINING\")\n","    \n","    train_loader = data_loader['train']\n","    test_loader = data_loader['test']\n","\n","    training_loss = []\n","    testing_loss = []\n","    \n","    for epoch in range(total_epochs):\n","        print(\"_________________________________\")\n","        trainloss = training_iteration(epoch+1, vae, train_loader, optimizer, mnist)\n","        testloss = test(vae, test_loader, mnist)\n","\n","        training_loss.append(trainloss)\n","        testing_loss.append(testloss)\n","\n","    return (training_loss, testing_loss)\n","    \n","def training_iteration(epoch, vae, train_loader, optimizer, mnist):\n","\n","    vae.train()\n","    train_loss = 0\n","    \n","    for batch_idx, (data, _) in enumerate(train_loader):\n","        data = data.cuda()\n","        optimizer.zero_grad()\n","        \n","        recon_batch, mu, log_var = vae(data)\n","        if mnist:\n","          loss = loss_function_MNIST(recon_batch, data, mu, log_var)\n","        else:\n","          loss = loss_function_CIFAR10(recon_batch, data, mu, log_var)\n","        \n","        loss.backward()\n","        train_loss += loss.item()\n","        optimizer.step()\n","        \n","        if batch_idx % 100 == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), loss.item() / len(data)))\n","            \n","    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))\n","\n","    return train_loss / len(train_loader.dataset)\n","    \n","def test(vae, test_loader, mnist):\n","    vae.eval()\n","    test_loss= 0\n","    with torch.no_grad():\n","        for data, _ in test_loader:\n","            data = data.cuda()\n","            recon, mu, log_var = vae(data)\n","            \n","            # sum up batch loss\n","            if mnist:\n","              test_loss += loss_function_MNIST(recon, data, mu, log_var).item()\n","            else:\n","              test_loss += loss_function_CIFAR10(recon, data, mu, log_var).item()\n","        \n","    test_loss /= len(test_loader.dataset)\n","    print('====> Test set loss: {:.4f}'.format(test_loss))\n","\n","    return test_loss\n","    \n","def loss_function_MNIST(recon_x, x, mu, log_var):\n","    x_dim = x.shape[2] ** 2\n","    x = x.view(-1, x_dim)\n","    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n","    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n","    return BCE + KLD\n","\n","def loss_function_CIFAR10(recon_x, x, mu, log_var):\n","    x = x.view(CIFAR10_BATCH_SIZE, 3, 32, 32)\n","    #x = x[0:50]\n","    #print(recon_x.shape)\n","    #print(x.shape)\n","    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n","    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n","    return BCE + KLD"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"taaMv1OgBFw4","executionInfo":{"status":"ok","timestamp":1669482767265,"user_tz":300,"elapsed":268,"user":{"displayName":"Alexandra Sklokin","userId":"06841525493811584768"}}},"outputs":[],"source":["def save_Sample_MNIST(vae, img_dim, latent_size, title):\n","    with torch.no_grad():\n","        z = torch.randn(64, latent_size).cuda()\n","        sample = vae.decoder(z).cuda()\n","        #print(sample.shape)\n","        save_image(sample.view(64, 1, img_dim, img_dim), '/content/drive/MyDrive/Colab Notebooks/CSI5340_A4/output/' +title+ '.png')\n","\n","def save_Sample_CIFAR10(vae, img_dim, latent_size, title):\n","    with torch.no_grad():\n","        z = torch.randn(64, latent_size).cuda()\n","        sample = vae.decoder(z).cuda()\n","        #print(sample.shape)\n","        save_image(sample.view(16, 3, img_dim, img_dim), '/content/drive/MyDrive/Colab Notebooks/CSI5340_A4/output/' +title+ '.png')"]},{"cell_type":"markdown","source":["# Analysis of Effect of Model Complexity and Latent Space Size"],"metadata":{"id":"UqNkWVejE-LS"}},{"cell_type":"code","source":["LATENT_SIZES = [2, 5, 10]\n","MODEL_COMPLEXITY = [VAE2, VAE3, VAE4]\n","\n","vaes = {}\n","training_losses = {}\n","testing_losses = {}\n","\n","for LS in LATENT_SIZES:\n","  for MODEL in MODEL_COMPLEXITY:\n","\n","    mnist_vae = MODEL(img_dim = 28, latent_size=LS)\n","    mnist_vae.cuda()\n","    mnist_optimizer = optim.Adam(mnist_vae.parameters())\n","    summary(mnist_vae, (1,28,28))\n","\n","    (mnist_train, mnist_test) = TRAIN_VAE(MNIST_EPOCHS, mnist_vae, mnist_optimizer, mnist_data_loader)\n","\n","    numpy.savetxt(\"/content/drive/MyDrive/Colab Notebooks/CSI5340_A4/output/vae/mnist_trainingloss_\"+MODEL.__name__+\"_latentsize\"+str(LS)+\".csv\", numpy.asarray(mnist_train), delimiter=\", \")\n","\n","    save_Sample(mnist_vae, img_dim=28, latent_size=LS, title=\"vae/mnist_\"+MODEL.__name__+\"_\"+str(LS)+\"_sample\")"],"metadata":{"id":"z78DtgeUGVdf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# MNIST MODEL"],"metadata":{"id":"zZHdDrHxwY6v"}},{"cell_type":"code","source":["LS = 10\n","MODEL = VAE3\n","\n","mnist_vae = MODEL(img_dim = 28, latent_size=LS)\n","mnist_vae.cuda()\n","mnist_optimizer = optim.Adam(mnist_vae.parameters())\n","summary(mnist_vae, (1,28,28))\n","\n","(mnist_train, mnist_test) = TRAIN_VAE(MNIST_EPOCHS, mnist_vae, mnist_optimizer, mnist_data_loader, mnist=True)\n","\n","numpy.savetxt(\"/content/drive/MyDrive/Colab Notebooks/CSI5340_A4/output/vae/mnist_trainingLoss.csv\", numpy.asarray(mnist_train), delimiter=\", \")\n","\n","save_Sample_MNIST(mnist_vae, img_dim=28, latent_size=LS, title=\"vae/mnist_sample\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v_5rdUamwbjC","executionInfo":{"status":"ok","timestamp":1669482967533,"user_tz":300,"elapsed":197969,"user":{"displayName":"Alexandra Sklokin","userId":"06841525493811584768"}},"outputId":"87fc0026-1f26-4e54-998a-3639025fca05"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Linear-1                 [-1, 1024]         803,840\n","            Linear-2                  [-1, 512]         524,800\n","            Linear-3                  [-1, 256]         131,328\n","            Linear-4                   [-1, 10]           2,570\n","            Linear-5                   [-1, 10]           2,570\n","            Linear-6                  [-1, 256]           2,816\n","            Linear-7                  [-1, 512]         131,584\n","            Linear-8                 [-1, 1024]         525,312\n","            Linear-9                  [-1, 784]         803,600\n","================================================================\n","Total params: 2,928,420\n","Trainable params: 2,928,420\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.03\n","Params size (MB): 11.17\n","Estimated Total Size (MB): 11.21\n","----------------------------------------------------------------\n","_________________________________________________________________ TRAINING\n","_________________________________\n","Train Epoch: 1 [0/60000 (0%)]\tLoss: 544.263594\n","Train Epoch: 1 [10000/60000 (17%)]\tLoss: 199.130313\n","Train Epoch: 1 [20000/60000 (33%)]\tLoss: 185.646777\n","Train Epoch: 1 [30000/60000 (50%)]\tLoss: 169.313379\n","Train Epoch: 1 [40000/60000 (67%)]\tLoss: 163.434609\n","Train Epoch: 1 [50000/60000 (83%)]\tLoss: 147.862734\n","====> Epoch: 1 Average loss: 176.6293\n","====> Test set loss: 143.4580\n","_________________________________\n","Train Epoch: 2 [0/60000 (0%)]\tLoss: 133.180977\n","Train Epoch: 2 [10000/60000 (17%)]\tLoss: 134.546719\n","Train Epoch: 2 [20000/60000 (33%)]\tLoss: 139.585889\n","Train Epoch: 2 [30000/60000 (50%)]\tLoss: 133.758574\n","Train Epoch: 2 [40000/60000 (67%)]\tLoss: 132.753555\n","Train Epoch: 2 [50000/60000 (83%)]\tLoss: 131.953975\n","====> Epoch: 2 Average loss: 133.2478\n","====> Test set loss: 127.7891\n","_________________________________\n","Train Epoch: 3 [0/60000 (0%)]\tLoss: 126.284863\n","Train Epoch: 3 [10000/60000 (17%)]\tLoss: 131.097480\n","Train Epoch: 3 [20000/60000 (33%)]\tLoss: 130.591865\n","Train Epoch: 3 [30000/60000 (50%)]\tLoss: 129.550869\n","Train Epoch: 3 [40000/60000 (67%)]\tLoss: 130.871680\n","Train Epoch: 3 [50000/60000 (83%)]\tLoss: 120.553760\n","====> Epoch: 3 Average loss: 125.6168\n","====> Test set loss: 123.3245\n","_________________________________\n","Train Epoch: 4 [0/60000 (0%)]\tLoss: 123.704229\n","Train Epoch: 4 [10000/60000 (17%)]\tLoss: 116.024629\n","Train Epoch: 4 [20000/60000 (33%)]\tLoss: 124.998018\n","Train Epoch: 4 [30000/60000 (50%)]\tLoss: 120.736543\n","Train Epoch: 4 [40000/60000 (67%)]\tLoss: 123.612412\n","Train Epoch: 4 [50000/60000 (83%)]\tLoss: 123.134707\n","====> Epoch: 4 Average loss: 122.2551\n","====> Test set loss: 120.8230\n","_________________________________\n","Train Epoch: 5 [0/60000 (0%)]\tLoss: 119.973750\n","Train Epoch: 5 [10000/60000 (17%)]\tLoss: 120.957471\n","Train Epoch: 5 [20000/60000 (33%)]\tLoss: 121.668135\n","Train Epoch: 5 [30000/60000 (50%)]\tLoss: 108.517656\n","Train Epoch: 5 [40000/60000 (67%)]\tLoss: 115.414180\n","Train Epoch: 5 [50000/60000 (83%)]\tLoss: 119.101035\n","====> Epoch: 5 Average loss: 119.9274\n","====> Test set loss: 119.1238\n","_________________________________\n","Train Epoch: 6 [0/60000 (0%)]\tLoss: 117.911289\n","Train Epoch: 6 [10000/60000 (17%)]\tLoss: 120.108672\n","Train Epoch: 6 [20000/60000 (33%)]\tLoss: 117.877041\n","Train Epoch: 6 [30000/60000 (50%)]\tLoss: 114.639287\n","Train Epoch: 6 [40000/60000 (67%)]\tLoss: 116.140400\n","Train Epoch: 6 [50000/60000 (83%)]\tLoss: 119.939932\n","====> Epoch: 6 Average loss: 118.2380\n","====> Test set loss: 118.2239\n","_________________________________\n","Train Epoch: 7 [0/60000 (0%)]\tLoss: 115.569902\n","Train Epoch: 7 [10000/60000 (17%)]\tLoss: 115.846836\n","Train Epoch: 7 [20000/60000 (33%)]\tLoss: 123.542676\n","Train Epoch: 7 [30000/60000 (50%)]\tLoss: 110.427607\n","Train Epoch: 7 [40000/60000 (67%)]\tLoss: 115.497568\n","Train Epoch: 7 [50000/60000 (83%)]\tLoss: 115.805771\n","====> Epoch: 7 Average loss: 116.8990\n","====> Test set loss: 116.6141\n","_________________________________\n","Train Epoch: 8 [0/60000 (0%)]\tLoss: 120.334014\n","Train Epoch: 8 [10000/60000 (17%)]\tLoss: 118.591621\n","Train Epoch: 8 [20000/60000 (33%)]\tLoss: 110.637539\n","Train Epoch: 8 [30000/60000 (50%)]\tLoss: 113.090977\n","Train Epoch: 8 [40000/60000 (67%)]\tLoss: 118.567861\n","Train Epoch: 8 [50000/60000 (83%)]\tLoss: 115.437021\n","====> Epoch: 8 Average loss: 114.9945\n","====> Test set loss: 114.1021\n","_________________________________\n","Train Epoch: 9 [0/60000 (0%)]\tLoss: 113.835195\n","Train Epoch: 9 [10000/60000 (17%)]\tLoss: 112.809687\n","Train Epoch: 9 [20000/60000 (33%)]\tLoss: 111.273750\n","Train Epoch: 9 [30000/60000 (50%)]\tLoss: 110.529941\n","Train Epoch: 9 [40000/60000 (67%)]\tLoss: 118.765410\n","Train Epoch: 9 [50000/60000 (83%)]\tLoss: 116.378428\n","====> Epoch: 9 Average loss: 112.7892\n","====> Test set loss: 112.3307\n","_________________________________\n","Train Epoch: 10 [0/60000 (0%)]\tLoss: 110.286680\n","Train Epoch: 10 [10000/60000 (17%)]\tLoss: 112.037969\n","Train Epoch: 10 [20000/60000 (33%)]\tLoss: 113.034346\n","Train Epoch: 10 [30000/60000 (50%)]\tLoss: 107.854639\n","Train Epoch: 10 [40000/60000 (67%)]\tLoss: 108.495322\n","Train Epoch: 10 [50000/60000 (83%)]\tLoss: 110.882090\n","====> Epoch: 10 Average loss: 111.2818\n","====> Test set loss: 111.9292\n","_________________________________\n","Train Epoch: 11 [0/60000 (0%)]\tLoss: 112.006016\n","Train Epoch: 11 [10000/60000 (17%)]\tLoss: 105.416035\n","Train Epoch: 11 [20000/60000 (33%)]\tLoss: 110.324443\n","Train Epoch: 11 [30000/60000 (50%)]\tLoss: 115.736367\n","Train Epoch: 11 [40000/60000 (67%)]\tLoss: 110.170527\n","Train Epoch: 11 [50000/60000 (83%)]\tLoss: 105.698984\n","====> Epoch: 11 Average loss: 110.3547\n","====> Test set loss: 111.1627\n","_________________________________\n","Train Epoch: 12 [0/60000 (0%)]\tLoss: 109.348145\n","Train Epoch: 12 [10000/60000 (17%)]\tLoss: 112.372100\n","Train Epoch: 12 [20000/60000 (33%)]\tLoss: 108.368389\n","Train Epoch: 12 [30000/60000 (50%)]\tLoss: 106.103350\n","Train Epoch: 12 [40000/60000 (67%)]\tLoss: 107.874326\n","Train Epoch: 12 [50000/60000 (83%)]\tLoss: 108.000000\n","====> Epoch: 12 Average loss: 109.4751\n","====> Test set loss: 110.8562\n","_________________________________\n","Train Epoch: 13 [0/60000 (0%)]\tLoss: 110.768477\n","Train Epoch: 13 [10000/60000 (17%)]\tLoss: 111.149658\n","Train Epoch: 13 [20000/60000 (33%)]\tLoss: 110.316387\n","Train Epoch: 13 [30000/60000 (50%)]\tLoss: 108.180166\n","Train Epoch: 13 [40000/60000 (67%)]\tLoss: 112.550303\n","Train Epoch: 13 [50000/60000 (83%)]\tLoss: 112.452959\n","====> Epoch: 13 Average loss: 108.8125\n","====> Test set loss: 110.4254\n","_________________________________\n","Train Epoch: 14 [0/60000 (0%)]\tLoss: 108.885273\n","Train Epoch: 14 [10000/60000 (17%)]\tLoss: 105.973945\n","Train Epoch: 14 [20000/60000 (33%)]\tLoss: 109.609668\n","Train Epoch: 14 [30000/60000 (50%)]\tLoss: 105.349502\n","Train Epoch: 14 [40000/60000 (67%)]\tLoss: 106.654883\n","Train Epoch: 14 [50000/60000 (83%)]\tLoss: 105.233770\n","====> Epoch: 14 Average loss: 108.2661\n","====> Test set loss: 110.5672\n","_________________________________\n","Train Epoch: 15 [0/60000 (0%)]\tLoss: 112.096172\n","Train Epoch: 15 [10000/60000 (17%)]\tLoss: 108.550732\n","Train Epoch: 15 [20000/60000 (33%)]\tLoss: 107.568984\n","Train Epoch: 15 [30000/60000 (50%)]\tLoss: 114.766768\n","Train Epoch: 15 [40000/60000 (67%)]\tLoss: 108.487813\n","Train Epoch: 15 [50000/60000 (83%)]\tLoss: 103.164346\n","====> Epoch: 15 Average loss: 107.7497\n","====> Test set loss: 109.8763\n","_________________________________\n","Train Epoch: 16 [0/60000 (0%)]\tLoss: 105.096865\n","Train Epoch: 16 [10000/60000 (17%)]\tLoss: 114.242256\n","Train Epoch: 16 [20000/60000 (33%)]\tLoss: 111.354131\n","Train Epoch: 16 [30000/60000 (50%)]\tLoss: 114.292402\n","Train Epoch: 16 [40000/60000 (67%)]\tLoss: 104.459648\n","Train Epoch: 16 [50000/60000 (83%)]\tLoss: 112.268906\n","====> Epoch: 16 Average loss: 107.2638\n","====> Test set loss: 110.0503\n","_________________________________\n","Train Epoch: 17 [0/60000 (0%)]\tLoss: 105.040117\n","Train Epoch: 17 [10000/60000 (17%)]\tLoss: 107.518809\n","Train Epoch: 17 [20000/60000 (33%)]\tLoss: 111.191719\n","Train Epoch: 17 [30000/60000 (50%)]\tLoss: 106.329785\n","Train Epoch: 17 [40000/60000 (67%)]\tLoss: 105.454629\n","Train Epoch: 17 [50000/60000 (83%)]\tLoss: 115.434043\n","====> Epoch: 17 Average loss: 106.9285\n","====> Test set loss: 108.7937\n","_________________________________\n","Train Epoch: 18 [0/60000 (0%)]\tLoss: 106.508232\n","Train Epoch: 18 [10000/60000 (17%)]\tLoss: 112.431094\n","Train Epoch: 18 [20000/60000 (33%)]\tLoss: 108.723672\n","Train Epoch: 18 [30000/60000 (50%)]\tLoss: 108.256260\n","Train Epoch: 18 [40000/60000 (67%)]\tLoss: 111.270195\n","Train Epoch: 18 [50000/60000 (83%)]\tLoss: 99.788750\n","====> Epoch: 18 Average loss: 106.5401\n","====> Test set loss: 109.3328\n","_________________________________\n","Train Epoch: 19 [0/60000 (0%)]\tLoss: 105.491348\n","Train Epoch: 19 [10000/60000 (17%)]\tLoss: 105.295332\n","Train Epoch: 19 [20000/60000 (33%)]\tLoss: 102.814473\n","Train Epoch: 19 [30000/60000 (50%)]\tLoss: 99.948594\n","Train Epoch: 19 [40000/60000 (67%)]\tLoss: 103.922617\n","Train Epoch: 19 [50000/60000 (83%)]\tLoss: 115.393311\n","====> Epoch: 19 Average loss: 106.2461\n","====> Test set loss: 108.9345\n","_________________________________\n","Train Epoch: 20 [0/60000 (0%)]\tLoss: 100.927842\n","Train Epoch: 20 [10000/60000 (17%)]\tLoss: 103.865938\n","Train Epoch: 20 [20000/60000 (33%)]\tLoss: 105.150635\n","Train Epoch: 20 [30000/60000 (50%)]\tLoss: 107.490859\n","Train Epoch: 20 [40000/60000 (67%)]\tLoss: 107.323975\n","Train Epoch: 20 [50000/60000 (83%)]\tLoss: 104.174551\n","====> Epoch: 20 Average loss: 105.8593\n","====> Test set loss: 108.9669\n","_________________________________\n","Train Epoch: 21 [0/60000 (0%)]\tLoss: 99.404805\n","Train Epoch: 21 [10000/60000 (17%)]\tLoss: 106.163574\n","Train Epoch: 21 [20000/60000 (33%)]\tLoss: 103.882432\n","Train Epoch: 21 [30000/60000 (50%)]\tLoss: 107.828340\n","Train Epoch: 21 [40000/60000 (67%)]\tLoss: 108.708350\n","Train Epoch: 21 [50000/60000 (83%)]\tLoss: 111.084668\n","====> Epoch: 21 Average loss: 105.6936\n","====> Test set loss: 108.9364\n","_________________________________\n","Train Epoch: 22 [0/60000 (0%)]\tLoss: 104.052393\n","Train Epoch: 22 [10000/60000 (17%)]\tLoss: 108.841953\n","Train Epoch: 22 [20000/60000 (33%)]\tLoss: 108.722617\n","Train Epoch: 22 [30000/60000 (50%)]\tLoss: 108.142012\n","Train Epoch: 22 [40000/60000 (67%)]\tLoss: 104.881230\n","Train Epoch: 22 [50000/60000 (83%)]\tLoss: 105.184287\n","====> Epoch: 22 Average loss: 105.3914\n","====> Test set loss: 108.6890\n","_________________________________\n","Train Epoch: 23 [0/60000 (0%)]\tLoss: 108.719551\n","Train Epoch: 23 [10000/60000 (17%)]\tLoss: 101.628926\n","Train Epoch: 23 [20000/60000 (33%)]\tLoss: 108.477129\n","Train Epoch: 23 [30000/60000 (50%)]\tLoss: 104.709404\n","Train Epoch: 23 [40000/60000 (67%)]\tLoss: 101.641104\n","Train Epoch: 23 [50000/60000 (83%)]\tLoss: 100.457295\n","====> Epoch: 23 Average loss: 105.0837\n","====> Test set loss: 108.7080\n","_________________________________\n","Train Epoch: 24 [0/60000 (0%)]\tLoss: 104.130889\n","Train Epoch: 24 [10000/60000 (17%)]\tLoss: 103.702891\n","Train Epoch: 24 [20000/60000 (33%)]\tLoss: 106.633154\n","Train Epoch: 24 [30000/60000 (50%)]\tLoss: 104.142305\n","Train Epoch: 24 [40000/60000 (67%)]\tLoss: 103.311484\n","Train Epoch: 24 [50000/60000 (83%)]\tLoss: 103.067324\n","====> Epoch: 24 Average loss: 104.9331\n","====> Test set loss: 108.8059\n","_________________________________\n","Train Epoch: 25 [0/60000 (0%)]\tLoss: 97.181982\n","Train Epoch: 25 [10000/60000 (17%)]\tLoss: 100.458457\n","Train Epoch: 25 [20000/60000 (33%)]\tLoss: 105.621289\n","Train Epoch: 25 [30000/60000 (50%)]\tLoss: 103.714785\n","Train Epoch: 25 [40000/60000 (67%)]\tLoss: 99.384014\n","Train Epoch: 25 [50000/60000 (83%)]\tLoss: 106.972158\n","====> Epoch: 25 Average loss: 104.7332\n","====> Test set loss: 108.8848\n","_________________________________\n","Train Epoch: 26 [0/60000 (0%)]\tLoss: 95.887354\n","Train Epoch: 26 [10000/60000 (17%)]\tLoss: 108.521797\n","Train Epoch: 26 [20000/60000 (33%)]\tLoss: 107.128867\n","Train Epoch: 26 [30000/60000 (50%)]\tLoss: 106.620078\n","Train Epoch: 26 [40000/60000 (67%)]\tLoss: 110.557500\n","Train Epoch: 26 [50000/60000 (83%)]\tLoss: 109.518262\n","====> Epoch: 26 Average loss: 104.4658\n","====> Test set loss: 108.7548\n","_________________________________\n","Train Epoch: 27 [0/60000 (0%)]\tLoss: 100.734629\n","Train Epoch: 27 [10000/60000 (17%)]\tLoss: 103.290176\n","Train Epoch: 27 [20000/60000 (33%)]\tLoss: 96.652021\n","Train Epoch: 27 [30000/60000 (50%)]\tLoss: 105.426631\n","Train Epoch: 27 [40000/60000 (67%)]\tLoss: 106.530703\n","Train Epoch: 27 [50000/60000 (83%)]\tLoss: 103.280029\n","====> Epoch: 27 Average loss: 104.2933\n","====> Test set loss: 108.5397\n","_________________________________\n","Train Epoch: 28 [0/60000 (0%)]\tLoss: 101.308496\n","Train Epoch: 28 [10000/60000 (17%)]\tLoss: 104.417129\n","Train Epoch: 28 [20000/60000 (33%)]\tLoss: 108.936055\n","Train Epoch: 28 [30000/60000 (50%)]\tLoss: 98.131182\n","Train Epoch: 28 [40000/60000 (67%)]\tLoss: 104.368545\n","Train Epoch: 28 [50000/60000 (83%)]\tLoss: 100.848516\n","====> Epoch: 28 Average loss: 104.0916\n","====> Test set loss: 108.4183\n","_________________________________\n","Train Epoch: 29 [0/60000 (0%)]\tLoss: 107.849141\n","Train Epoch: 29 [10000/60000 (17%)]\tLoss: 97.147275\n","Train Epoch: 29 [20000/60000 (33%)]\tLoss: 104.864199\n","Train Epoch: 29 [30000/60000 (50%)]\tLoss: 108.473066\n","Train Epoch: 29 [40000/60000 (67%)]\tLoss: 108.785557\n","Train Epoch: 29 [50000/60000 (83%)]\tLoss: 99.645840\n","====> Epoch: 29 Average loss: 103.9590\n","====> Test set loss: 108.2046\n","_________________________________\n","Train Epoch: 30 [0/60000 (0%)]\tLoss: 99.754492\n","Train Epoch: 30 [10000/60000 (17%)]\tLoss: 101.546943\n","Train Epoch: 30 [20000/60000 (33%)]\tLoss: 106.682109\n","Train Epoch: 30 [30000/60000 (50%)]\tLoss: 105.272363\n","Train Epoch: 30 [40000/60000 (67%)]\tLoss: 108.028711\n","Train Epoch: 30 [50000/60000 (83%)]\tLoss: 99.094443\n","====> Epoch: 30 Average loss: 103.7569\n","====> Test set loss: 108.4339\n"]}]},{"cell_type":"markdown","source":["# CIFAR10"],"metadata":{"id":"zlcXYe0bFHfZ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"SGnkny1KBFw-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8fa35575-4bf4-445e-95cf-58ea7334cc10","executionInfo":{"status":"ok","timestamp":1668899107996,"user_tz":300,"elapsed":1592284,"user":{"displayName":"Alexandra Sklokin","userId":"06841525493811584768"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 64, 16, 16]           3,136\n","            Conv2d-2            [-1, 128, 8, 8]         131,200\n","            Linear-3                    [-1, 5]          10,245\n","            Linear-4                    [-1, 5]          10,245\n","            Linear-5                 [-1, 2048]          12,288\n","   ConvTranspose2d-6           [-1, 64, 64, 64]           8,256\n","   ConvTranspose2d-7          [-1, 3, 128, 128]           3,075\n","         MaxPool2d-8            [-1, 3, 32, 32]               0\n","================================================================\n","Total params: 178,445\n","Trainable params: 178,445\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 2.60\n","Params size (MB): 0.68\n","Estimated Total Size (MB): 3.29\n","----------------------------------------------------------------\n","_________________________________________________________________ TRAINING\n","_________________________________\n","Train Epoch: 1 [0/50000 (0%)]\tLoss: 2136.285781\n","Train Epoch: 1 [10000/50000 (20%)]\tLoss: 2129.378750\n","Train Epoch: 1 [20000/50000 (40%)]\tLoss: 2129.348437\n","Train Epoch: 1 [30000/50000 (60%)]\tLoss: 2105.103594\n","Train Epoch: 1 [40000/50000 (80%)]\tLoss: 2072.947969\n","====> Epoch: 1 Average loss: 2097.7796\n","====> Test set loss: 2037.0080\n","_________________________________\n","Train Epoch: 2 [0/50000 (0%)]\tLoss: 2024.011250\n","Train Epoch: 2 [10000/50000 (20%)]\tLoss: 2055.976562\n","Train Epoch: 2 [20000/50000 (40%)]\tLoss: 2041.524688\n","Train Epoch: 2 [30000/50000 (60%)]\tLoss: 2034.966563\n","Train Epoch: 2 [40000/50000 (80%)]\tLoss: 2049.066094\n","====> Epoch: 2 Average loss: 2029.7250\n","====> Test set loss: 2020.0091\n","_________________________________\n","Train Epoch: 3 [0/50000 (0%)]\tLoss: 2061.017188\n","Train Epoch: 3 [10000/50000 (20%)]\tLoss: 2022.155000\n","Train Epoch: 3 [20000/50000 (40%)]\tLoss: 1994.588594\n","Train Epoch: 3 [30000/50000 (60%)]\tLoss: 2016.437500\n","Train Epoch: 3 [40000/50000 (80%)]\tLoss: 2034.924062\n","====> Epoch: 3 Average loss: 2017.6251\n","====> Test set loss: 2013.2298\n","_________________________________\n","Train Epoch: 4 [0/50000 (0%)]\tLoss: 2006.222500\n","Train Epoch: 4 [10000/50000 (20%)]\tLoss: 2038.277188\n","Train Epoch: 4 [20000/50000 (40%)]\tLoss: 1996.939375\n","Train Epoch: 4 [30000/50000 (60%)]\tLoss: 1986.985000\n","Train Epoch: 4 [40000/50000 (80%)]\tLoss: 2010.118438\n","====> Epoch: 4 Average loss: 2014.1627\n","====> Test set loss: 2011.7636\n","_________________________________\n","Train Epoch: 5 [0/50000 (0%)]\tLoss: 2016.914531\n","Train Epoch: 5 [10000/50000 (20%)]\tLoss: 2007.246250\n","Train Epoch: 5 [20000/50000 (40%)]\tLoss: 2020.564687\n","Train Epoch: 5 [30000/50000 (60%)]\tLoss: 2020.505156\n","Train Epoch: 5 [40000/50000 (80%)]\tLoss: 1973.689687\n","====> Epoch: 5 Average loss: 2012.9107\n","====> Test set loss: 2011.0415\n","_________________________________\n","Train Epoch: 6 [0/50000 (0%)]\tLoss: 1985.552031\n","Train Epoch: 6 [10000/50000 (20%)]\tLoss: 2017.032187\n","Train Epoch: 6 [20000/50000 (40%)]\tLoss: 2009.528906\n","Train Epoch: 6 [30000/50000 (60%)]\tLoss: 2030.521250\n","Train Epoch: 6 [40000/50000 (80%)]\tLoss: 2026.986562\n","====> Epoch: 6 Average loss: 2012.1039\n","====> Test set loss: 2010.2203\n","_________________________________\n","Train Epoch: 7 [0/50000 (0%)]\tLoss: 1989.438437\n","Train Epoch: 7 [10000/50000 (20%)]\tLoss: 2025.144375\n","Train Epoch: 7 [20000/50000 (40%)]\tLoss: 1985.000156\n","Train Epoch: 7 [30000/50000 (60%)]\tLoss: 2029.212656\n","Train Epoch: 7 [40000/50000 (80%)]\tLoss: 2004.603594\n","====> Epoch: 7 Average loss: 2011.7322\n","====> Test set loss: 2009.3040\n","_________________________________\n","Train Epoch: 8 [0/50000 (0%)]\tLoss: 1993.242344\n","Train Epoch: 8 [10000/50000 (20%)]\tLoss: 2045.864219\n","Train Epoch: 8 [20000/50000 (40%)]\tLoss: 1980.837344\n","Train Epoch: 8 [30000/50000 (60%)]\tLoss: 1994.977344\n","Train Epoch: 8 [40000/50000 (80%)]\tLoss: 2023.020469\n","====> Epoch: 8 Average loss: 2011.2059\n","====> Test set loss: 2009.2952\n","_________________________________\n","Train Epoch: 9 [0/50000 (0%)]\tLoss: 2030.078438\n","Train Epoch: 9 [10000/50000 (20%)]\tLoss: 1977.650625\n","Train Epoch: 9 [20000/50000 (40%)]\tLoss: 1981.760156\n","Train Epoch: 9 [30000/50000 (60%)]\tLoss: 2028.729375\n","Train Epoch: 9 [40000/50000 (80%)]\tLoss: 2020.759219\n","====> Epoch: 9 Average loss: 2010.8253\n","====> Test set loss: 2008.9123\n","_________________________________\n","Train Epoch: 10 [0/50000 (0%)]\tLoss: 2021.708125\n","Train Epoch: 10 [10000/50000 (20%)]\tLoss: 2029.923125\n","Train Epoch: 10 [20000/50000 (40%)]\tLoss: 2042.449063\n","Train Epoch: 10 [30000/50000 (60%)]\tLoss: 1988.165937\n","Train Epoch: 10 [40000/50000 (80%)]\tLoss: 2031.493125\n","====> Epoch: 10 Average loss: 2010.5545\n","====> Test set loss: 2008.5566\n","_________________________________\n","Train Epoch: 11 [0/50000 (0%)]\tLoss: 1957.769063\n","Train Epoch: 11 [10000/50000 (20%)]\tLoss: 2014.338438\n","Train Epoch: 11 [20000/50000 (40%)]\tLoss: 1994.473281\n","Train Epoch: 11 [30000/50000 (60%)]\tLoss: 1978.487031\n","Train Epoch: 11 [40000/50000 (80%)]\tLoss: 2000.197656\n","====> Epoch: 11 Average loss: 2010.2253\n","====> Test set loss: 2008.1050\n","_________________________________\n","Train Epoch: 12 [0/50000 (0%)]\tLoss: 1995.783437\n","Train Epoch: 12 [10000/50000 (20%)]\tLoss: 2009.688750\n","Train Epoch: 12 [20000/50000 (40%)]\tLoss: 1986.195625\n","Train Epoch: 12 [30000/50000 (60%)]\tLoss: 2020.568750\n","Train Epoch: 12 [40000/50000 (80%)]\tLoss: 2015.285625\n","====> Epoch: 12 Average loss: 2010.0094\n","====> Test set loss: 2007.9043\n","_________________________________\n","Train Epoch: 13 [0/50000 (0%)]\tLoss: 1988.951250\n","Train Epoch: 13 [10000/50000 (20%)]\tLoss: 2006.803906\n","Train Epoch: 13 [20000/50000 (40%)]\tLoss: 2003.342500\n","Train Epoch: 13 [30000/50000 (60%)]\tLoss: 1993.600781\n","Train Epoch: 13 [40000/50000 (80%)]\tLoss: 2002.675156\n","====> Epoch: 13 Average loss: 2009.7799\n","====> Test set loss: 2007.5736\n","_________________________________\n","Train Epoch: 14 [0/50000 (0%)]\tLoss: 2012.967813\n","Train Epoch: 14 [10000/50000 (20%)]\tLoss: 2027.802969\n","Train Epoch: 14 [20000/50000 (40%)]\tLoss: 2012.662344\n","Train Epoch: 14 [30000/50000 (60%)]\tLoss: 1989.366250\n","Train Epoch: 14 [40000/50000 (80%)]\tLoss: 1997.504531\n","====> Epoch: 14 Average loss: 2009.5962\n","====> Test set loss: 2007.6592\n","_________________________________\n","Train Epoch: 15 [0/50000 (0%)]\tLoss: 2020.725938\n","Train Epoch: 15 [10000/50000 (20%)]\tLoss: 1987.224531\n","Train Epoch: 15 [20000/50000 (40%)]\tLoss: 1990.622031\n","Train Epoch: 15 [30000/50000 (60%)]\tLoss: 2033.274531\n","Train Epoch: 15 [40000/50000 (80%)]\tLoss: 1996.979531\n","====> Epoch: 15 Average loss: 2009.4426\n","====> Test set loss: 2007.2817\n","_________________________________\n","Train Epoch: 16 [0/50000 (0%)]\tLoss: 2005.162500\n","Train Epoch: 16 [10000/50000 (20%)]\tLoss: 2005.108281\n","Train Epoch: 16 [20000/50000 (40%)]\tLoss: 2031.760937\n","Train Epoch: 16 [30000/50000 (60%)]\tLoss: 1997.302969\n","Train Epoch: 16 [40000/50000 (80%)]\tLoss: 2015.597344\n","====> Epoch: 16 Average loss: 2009.2707\n","====> Test set loss: 2007.0470\n","_________________________________\n","Train Epoch: 17 [0/50000 (0%)]\tLoss: 1996.896250\n","Train Epoch: 17 [10000/50000 (20%)]\tLoss: 2028.559531\n","Train Epoch: 17 [20000/50000 (40%)]\tLoss: 2017.992344\n","Train Epoch: 17 [30000/50000 (60%)]\tLoss: 2017.436094\n","Train Epoch: 17 [40000/50000 (80%)]\tLoss: 2008.018437\n","====> Epoch: 17 Average loss: 2009.1337\n","====> Test set loss: 2006.9737\n","_________________________________\n","Train Epoch: 18 [0/50000 (0%)]\tLoss: 1998.093750\n","Train Epoch: 18 [10000/50000 (20%)]\tLoss: 1950.628125\n","Train Epoch: 18 [20000/50000 (40%)]\tLoss: 2044.865313\n","Train Epoch: 18 [30000/50000 (60%)]\tLoss: 2012.635469\n","Train Epoch: 18 [40000/50000 (80%)]\tLoss: 2006.016250\n","====> Epoch: 18 Average loss: 2009.0477\n","====> Test set loss: 2006.8713\n","_________________________________\n","Train Epoch: 19 [0/50000 (0%)]\tLoss: 2044.730937\n","Train Epoch: 19 [10000/50000 (20%)]\tLoss: 2009.683281\n","Train Epoch: 19 [20000/50000 (40%)]\tLoss: 2021.268750\n","Train Epoch: 19 [30000/50000 (60%)]\tLoss: 1999.520000\n","Train Epoch: 19 [40000/50000 (80%)]\tLoss: 2005.382031\n","====> Epoch: 19 Average loss: 2008.8229\n","====> Test set loss: 2006.7065\n","_________________________________\n","Train Epoch: 20 [0/50000 (0%)]\tLoss: 2010.382812\n","Train Epoch: 20 [10000/50000 (20%)]\tLoss: 2004.144063\n","Train Epoch: 20 [20000/50000 (40%)]\tLoss: 2038.187969\n","Train Epoch: 20 [30000/50000 (60%)]\tLoss: 2009.850938\n","Train Epoch: 20 [40000/50000 (80%)]\tLoss: 2003.070625\n","====> Epoch: 20 Average loss: 2008.7492\n","====> Test set loss: 2006.9895\n","_________________________________\n","Train Epoch: 21 [0/50000 (0%)]\tLoss: 2018.431250\n","Train Epoch: 21 [10000/50000 (20%)]\tLoss: 2008.084219\n","Train Epoch: 21 [20000/50000 (40%)]\tLoss: 2019.399375\n","Train Epoch: 21 [30000/50000 (60%)]\tLoss: 1995.448750\n","Train Epoch: 21 [40000/50000 (80%)]\tLoss: 1987.006562\n","====> Epoch: 21 Average loss: 2008.6956\n","====> Test set loss: 2006.3837\n","_________________________________\n","Train Epoch: 22 [0/50000 (0%)]\tLoss: 2010.633125\n","Train Epoch: 22 [10000/50000 (20%)]\tLoss: 2011.369531\n","Train Epoch: 22 [20000/50000 (40%)]\tLoss: 2025.400625\n","Train Epoch: 22 [30000/50000 (60%)]\tLoss: 2036.522500\n","Train Epoch: 22 [40000/50000 (80%)]\tLoss: 1987.934219\n","====> Epoch: 22 Average loss: 2008.5680\n","====> Test set loss: 2006.5059\n","_________________________________\n","Train Epoch: 23 [0/50000 (0%)]\tLoss: 2006.700000\n","Train Epoch: 23 [10000/50000 (20%)]\tLoss: 1994.272187\n","Train Epoch: 23 [20000/50000 (40%)]\tLoss: 2009.497344\n","Train Epoch: 23 [30000/50000 (60%)]\tLoss: 2028.997031\n","Train Epoch: 23 [40000/50000 (80%)]\tLoss: 2021.913906\n","====> Epoch: 23 Average loss: 2008.5134\n","====> Test set loss: 2006.4219\n","_________________________________\n","Train Epoch: 24 [0/50000 (0%)]\tLoss: 2015.463281\n","Train Epoch: 24 [10000/50000 (20%)]\tLoss: 1970.345000\n","Train Epoch: 24 [20000/50000 (40%)]\tLoss: 1998.914375\n","Train Epoch: 24 [30000/50000 (60%)]\tLoss: 2017.460313\n","Train Epoch: 24 [40000/50000 (80%)]\tLoss: 2000.364844\n","====> Epoch: 24 Average loss: 2008.3791\n","====> Test set loss: 2006.1043\n","_________________________________\n","Train Epoch: 25 [0/50000 (0%)]\tLoss: 2035.956563\n","Train Epoch: 25 [10000/50000 (20%)]\tLoss: 2025.826250\n","Train Epoch: 25 [20000/50000 (40%)]\tLoss: 2007.477813\n","Train Epoch: 25 [30000/50000 (60%)]\tLoss: 2044.657969\n","Train Epoch: 25 [40000/50000 (80%)]\tLoss: 2033.062344\n","====> Epoch: 25 Average loss: 2008.2667\n","====> Test set loss: 2005.9545\n","_________________________________\n","Train Epoch: 26 [0/50000 (0%)]\tLoss: 2007.830000\n","Train Epoch: 26 [10000/50000 (20%)]\tLoss: 2001.816875\n","Train Epoch: 26 [20000/50000 (40%)]\tLoss: 2020.987344\n","Train Epoch: 26 [30000/50000 (60%)]\tLoss: 2045.972031\n","Train Epoch: 26 [40000/50000 (80%)]\tLoss: 2002.051875\n","====> Epoch: 26 Average loss: 2007.7065\n","====> Test set loss: 2005.0544\n","_________________________________\n","Train Epoch: 27 [0/50000 (0%)]\tLoss: 1975.069219\n","Train Epoch: 27 [10000/50000 (20%)]\tLoss: 2011.387344\n","Train Epoch: 27 [20000/50000 (40%)]\tLoss: 1987.413750\n","Train Epoch: 27 [30000/50000 (60%)]\tLoss: 2015.450469\n","Train Epoch: 27 [40000/50000 (80%)]\tLoss: 2008.909844\n","====> Epoch: 27 Average loss: 2006.4292\n","====> Test set loss: 2003.9553\n","_________________________________\n","Train Epoch: 28 [0/50000 (0%)]\tLoss: 2001.355000\n","Train Epoch: 28 [10000/50000 (20%)]\tLoss: 2015.737500\n","Train Epoch: 28 [20000/50000 (40%)]\tLoss: 2021.683906\n","Train Epoch: 28 [30000/50000 (60%)]\tLoss: 2042.616250\n","Train Epoch: 28 [40000/50000 (80%)]\tLoss: 2018.242031\n","====> Epoch: 28 Average loss: 2005.4927\n","====> Test set loss: 2002.8736\n","_________________________________\n","Train Epoch: 29 [0/50000 (0%)]\tLoss: 2014.726875\n","Train Epoch: 29 [10000/50000 (20%)]\tLoss: 2010.632031\n","Train Epoch: 29 [20000/50000 (40%)]\tLoss: 2040.915937\n","Train Epoch: 29 [30000/50000 (60%)]\tLoss: 1993.244375\n","Train Epoch: 29 [40000/50000 (80%)]\tLoss: 1949.037344\n","====> Epoch: 29 Average loss: 2004.9290\n","====> Test set loss: 2002.7350\n","_________________________________\n","Train Epoch: 30 [0/50000 (0%)]\tLoss: 2012.097344\n","Train Epoch: 30 [10000/50000 (20%)]\tLoss: 2005.821875\n","Train Epoch: 30 [20000/50000 (40%)]\tLoss: 1981.450469\n","Train Epoch: 30 [30000/50000 (60%)]\tLoss: 2018.131562\n","Train Epoch: 30 [40000/50000 (80%)]\tLoss: 1996.017500\n","====> Epoch: 30 Average loss: 2004.5428\n","====> Test set loss: 2002.7649\n","_________________________________\n","Train Epoch: 31 [0/50000 (0%)]\tLoss: 1973.939375\n","Train Epoch: 31 [10000/50000 (20%)]\tLoss: 2022.814687\n","Train Epoch: 31 [20000/50000 (40%)]\tLoss: 1997.116250\n","Train Epoch: 31 [30000/50000 (60%)]\tLoss: 2038.553281\n","Train Epoch: 31 [40000/50000 (80%)]\tLoss: 2016.617812\n","====> Epoch: 31 Average loss: 2004.3915\n","====> Test set loss: 2002.3434\n","_________________________________\n","Train Epoch: 32 [0/50000 (0%)]\tLoss: 2012.510469\n","Train Epoch: 32 [10000/50000 (20%)]\tLoss: 1996.384219\n","Train Epoch: 32 [20000/50000 (40%)]\tLoss: 2025.467656\n","Train Epoch: 32 [30000/50000 (60%)]\tLoss: 2006.174062\n","Train Epoch: 32 [40000/50000 (80%)]\tLoss: 2032.769063\n","====> Epoch: 32 Average loss: 2004.2058\n","====> Test set loss: 2001.9547\n","_________________________________\n","Train Epoch: 33 [0/50000 (0%)]\tLoss: 2004.789062\n","Train Epoch: 33 [10000/50000 (20%)]\tLoss: 2016.605625\n","Train Epoch: 33 [20000/50000 (40%)]\tLoss: 1981.745469\n","Train Epoch: 33 [30000/50000 (60%)]\tLoss: 2021.878594\n","Train Epoch: 33 [40000/50000 (80%)]\tLoss: 2038.272344\n","====> Epoch: 33 Average loss: 2004.0677\n","====> Test set loss: 2001.9865\n","_________________________________\n","Train Epoch: 34 [0/50000 (0%)]\tLoss: 2004.588906\n","Train Epoch: 34 [10000/50000 (20%)]\tLoss: 2007.480625\n","Train Epoch: 34 [20000/50000 (40%)]\tLoss: 1978.302813\n","Train Epoch: 34 [30000/50000 (60%)]\tLoss: 2033.730625\n","Train Epoch: 34 [40000/50000 (80%)]\tLoss: 2005.826094\n","====> Epoch: 34 Average loss: 2003.8636\n","====> Test set loss: 2001.6035\n","_________________________________\n","Train Epoch: 35 [0/50000 (0%)]\tLoss: 1999.275625\n","Train Epoch: 35 [10000/50000 (20%)]\tLoss: 2017.223125\n","Train Epoch: 35 [20000/50000 (40%)]\tLoss: 1989.766250\n","Train Epoch: 35 [30000/50000 (60%)]\tLoss: 2031.019063\n","Train Epoch: 35 [40000/50000 (80%)]\tLoss: 2025.301250\n","====> Epoch: 35 Average loss: 2003.7843\n","====> Test set loss: 2001.5611\n","_________________________________\n","Train Epoch: 36 [0/50000 (0%)]\tLoss: 2008.718594\n","Train Epoch: 36 [10000/50000 (20%)]\tLoss: 2032.186719\n","Train Epoch: 36 [20000/50000 (40%)]\tLoss: 2030.068594\n","Train Epoch: 36 [30000/50000 (60%)]\tLoss: 1979.944062\n","Train Epoch: 36 [40000/50000 (80%)]\tLoss: 1954.864063\n","====> Epoch: 36 Average loss: 2003.7263\n","====> Test set loss: 2001.8861\n","_________________________________\n","Train Epoch: 37 [0/50000 (0%)]\tLoss: 1994.268906\n","Train Epoch: 37 [10000/50000 (20%)]\tLoss: 2024.037969\n","Train Epoch: 37 [20000/50000 (40%)]\tLoss: 1995.605625\n","Train Epoch: 37 [30000/50000 (60%)]\tLoss: 2013.970781\n","Train Epoch: 37 [40000/50000 (80%)]\tLoss: 1995.883125\n","====> Epoch: 37 Average loss: 2003.5889\n","====> Test set loss: 2001.1990\n","_________________________________\n","Train Epoch: 38 [0/50000 (0%)]\tLoss: 1952.436563\n","Train Epoch: 38 [10000/50000 (20%)]\tLoss: 2048.192187\n","Train Epoch: 38 [20000/50000 (40%)]\tLoss: 1984.814687\n","Train Epoch: 38 [30000/50000 (60%)]\tLoss: 2016.442344\n","Train Epoch: 38 [40000/50000 (80%)]\tLoss: 1992.274688\n","====> Epoch: 38 Average loss: 2003.4479\n","====> Test set loss: 2001.2495\n","_________________________________\n","Train Epoch: 39 [0/50000 (0%)]\tLoss: 2008.302031\n","Train Epoch: 39 [10000/50000 (20%)]\tLoss: 2008.363281\n","Train Epoch: 39 [20000/50000 (40%)]\tLoss: 2003.270937\n","Train Epoch: 39 [30000/50000 (60%)]\tLoss: 1995.409531\n","Train Epoch: 39 [40000/50000 (80%)]\tLoss: 2020.592656\n","====> Epoch: 39 Average loss: 2003.4132\n","====> Test set loss: 2001.2164\n","_________________________________\n","Train Epoch: 40 [0/50000 (0%)]\tLoss: 1993.970625\n","Train Epoch: 40 [10000/50000 (20%)]\tLoss: 2048.268281\n","Train Epoch: 40 [20000/50000 (40%)]\tLoss: 2020.368750\n","Train Epoch: 40 [30000/50000 (60%)]\tLoss: 1982.040000\n","Train Epoch: 40 [40000/50000 (80%)]\tLoss: 1988.054063\n","====> Epoch: 40 Average loss: 2003.4565\n","====> Test set loss: 2001.2261\n","_________________________________\n","Train Epoch: 41 [0/50000 (0%)]\tLoss: 2007.342187\n","Train Epoch: 41 [10000/50000 (20%)]\tLoss: 2016.628125\n","Train Epoch: 41 [20000/50000 (40%)]\tLoss: 2009.150938\n","Train Epoch: 41 [30000/50000 (60%)]\tLoss: 2013.846250\n","Train Epoch: 41 [40000/50000 (80%)]\tLoss: 1986.978125\n","====> Epoch: 41 Average loss: 2003.3239\n","====> Test set loss: 2001.1890\n","_________________________________\n","Train Epoch: 42 [0/50000 (0%)]\tLoss: 1959.186875\n","Train Epoch: 42 [10000/50000 (20%)]\tLoss: 2003.757812\n","Train Epoch: 42 [20000/50000 (40%)]\tLoss: 1993.337812\n","Train Epoch: 42 [30000/50000 (60%)]\tLoss: 2000.569688\n","Train Epoch: 42 [40000/50000 (80%)]\tLoss: 1980.240937\n","====> Epoch: 42 Average loss: 2003.2331\n","====> Test set loss: 2001.4072\n","_________________________________\n","Train Epoch: 43 [0/50000 (0%)]\tLoss: 1986.181719\n","Train Epoch: 43 [10000/50000 (20%)]\tLoss: 1982.391875\n","Train Epoch: 43 [20000/50000 (40%)]\tLoss: 2027.019375\n","Train Epoch: 43 [30000/50000 (60%)]\tLoss: 1985.175937\n","Train Epoch: 43 [40000/50000 (80%)]\tLoss: 2004.068281\n","====> Epoch: 43 Average loss: 2003.2535\n","====> Test set loss: 2001.1277\n","_________________________________\n","Train Epoch: 44 [0/50000 (0%)]\tLoss: 1994.745469\n","Train Epoch: 44 [10000/50000 (20%)]\tLoss: 1992.182812\n","Train Epoch: 44 [20000/50000 (40%)]\tLoss: 1951.993906\n","Train Epoch: 44 [30000/50000 (60%)]\tLoss: 1975.010000\n","Train Epoch: 44 [40000/50000 (80%)]\tLoss: 2014.128438\n","====> Epoch: 44 Average loss: 2003.1919\n","====> Test set loss: 2001.0506\n","_________________________________\n","Train Epoch: 45 [0/50000 (0%)]\tLoss: 1979.855313\n","Train Epoch: 45 [10000/50000 (20%)]\tLoss: 2021.285781\n","Train Epoch: 45 [20000/50000 (40%)]\tLoss: 1995.179844\n","Train Epoch: 45 [30000/50000 (60%)]\tLoss: 1970.282187\n","Train Epoch: 45 [40000/50000 (80%)]\tLoss: 2001.340781\n","====> Epoch: 45 Average loss: 2003.1366\n","====> Test set loss: 2001.0842\n","_________________________________\n","Train Epoch: 46 [0/50000 (0%)]\tLoss: 2012.882344\n","Train Epoch: 46 [10000/50000 (20%)]\tLoss: 2005.386719\n","Train Epoch: 46 [20000/50000 (40%)]\tLoss: 2021.689844\n","Train Epoch: 46 [30000/50000 (60%)]\tLoss: 2009.097188\n","Train Epoch: 46 [40000/50000 (80%)]\tLoss: 1998.891562\n","====> Epoch: 46 Average loss: 2003.0959\n","====> Test set loss: 2000.9078\n","_________________________________\n","Train Epoch: 47 [0/50000 (0%)]\tLoss: 2006.525469\n","Train Epoch: 47 [10000/50000 (20%)]\tLoss: 2028.537344\n","Train Epoch: 47 [20000/50000 (40%)]\tLoss: 1988.587812\n","Train Epoch: 47 [30000/50000 (60%)]\tLoss: 1985.120938\n","Train Epoch: 47 [40000/50000 (80%)]\tLoss: 1999.993281\n","====> Epoch: 47 Average loss: 2003.0309\n","====> Test set loss: 2001.0975\n","_________________________________\n","Train Epoch: 48 [0/50000 (0%)]\tLoss: 1972.415625\n","Train Epoch: 48 [10000/50000 (20%)]\tLoss: 1941.946094\n","Train Epoch: 48 [20000/50000 (40%)]\tLoss: 1989.839844\n","Train Epoch: 48 [30000/50000 (60%)]\tLoss: 2051.556719\n","Train Epoch: 48 [40000/50000 (80%)]\tLoss: 1997.773906\n","====> Epoch: 48 Average loss: 2003.0095\n","====> Test set loss: 2000.9608\n","_________________________________\n","Train Epoch: 49 [0/50000 (0%)]\tLoss: 2003.331250\n","Train Epoch: 49 [10000/50000 (20%)]\tLoss: 2012.893594\n","Train Epoch: 49 [20000/50000 (40%)]\tLoss: 1989.846562\n","Train Epoch: 49 [30000/50000 (60%)]\tLoss: 1970.614063\n","Train Epoch: 49 [40000/50000 (80%)]\tLoss: 2030.752812\n","====> Epoch: 49 Average loss: 2002.9416\n","====> Test set loss: 2000.7976\n","_________________________________\n","Train Epoch: 50 [0/50000 (0%)]\tLoss: 1958.407500\n","Train Epoch: 50 [10000/50000 (20%)]\tLoss: 2010.963906\n","Train Epoch: 50 [20000/50000 (40%)]\tLoss: 2031.993906\n","Train Epoch: 50 [30000/50000 (60%)]\tLoss: 2035.374844\n","Train Epoch: 50 [40000/50000 (80%)]\tLoss: 2014.093281\n","====> Epoch: 50 Average loss: 2002.9439\n","====> Test set loss: 2000.7887\n","_________________________________\n","Train Epoch: 51 [0/50000 (0%)]\tLoss: 1994.415625\n","Train Epoch: 51 [10000/50000 (20%)]\tLoss: 1961.309219\n","Train Epoch: 51 [20000/50000 (40%)]\tLoss: 2006.848594\n","Train Epoch: 51 [30000/50000 (60%)]\tLoss: 1971.129219\n","Train Epoch: 51 [40000/50000 (80%)]\tLoss: 2012.033750\n","====> Epoch: 51 Average loss: 2002.9152\n","====> Test set loss: 2000.6713\n","_________________________________\n","Train Epoch: 52 [0/50000 (0%)]\tLoss: 1971.652344\n","Train Epoch: 52 [10000/50000 (20%)]\tLoss: 2003.003438\n","Train Epoch: 52 [20000/50000 (40%)]\tLoss: 2001.824844\n","Train Epoch: 52 [30000/50000 (60%)]\tLoss: 2016.246562\n","Train Epoch: 52 [40000/50000 (80%)]\tLoss: 1973.200313\n","====> Epoch: 52 Average loss: 2002.9648\n","====> Test set loss: 2000.8758\n","_________________________________\n","Train Epoch: 53 [0/50000 (0%)]\tLoss: 2010.974531\n","Train Epoch: 53 [10000/50000 (20%)]\tLoss: 1968.784375\n","Train Epoch: 53 [20000/50000 (40%)]\tLoss: 2006.402656\n","Train Epoch: 53 [30000/50000 (60%)]\tLoss: 1989.842344\n","Train Epoch: 53 [40000/50000 (80%)]\tLoss: 1996.654531\n","====> Epoch: 53 Average loss: 2002.8304\n","====> Test set loss: 2000.9379\n","_________________________________\n","Train Epoch: 54 [0/50000 (0%)]\tLoss: 1980.300313\n","Train Epoch: 54 [10000/50000 (20%)]\tLoss: 2014.823594\n","Train Epoch: 54 [20000/50000 (40%)]\tLoss: 2014.752656\n","Train Epoch: 54 [30000/50000 (60%)]\tLoss: 1992.930312\n","Train Epoch: 54 [40000/50000 (80%)]\tLoss: 1996.202656\n","====> Epoch: 54 Average loss: 2002.8376\n","====> Test set loss: 2000.9234\n","_________________________________\n","Train Epoch: 55 [0/50000 (0%)]\tLoss: 1977.204844\n","Train Epoch: 55 [10000/50000 (20%)]\tLoss: 1984.982812\n","Train Epoch: 55 [20000/50000 (40%)]\tLoss: 2010.452812\n","Train Epoch: 55 [30000/50000 (60%)]\tLoss: 2026.276875\n","Train Epoch: 55 [40000/50000 (80%)]\tLoss: 2024.512813\n","====> Epoch: 55 Average loss: 2002.8369\n","====> Test set loss: 2000.8046\n","_________________________________\n","Train Epoch: 56 [0/50000 (0%)]\tLoss: 1998.327188\n","Train Epoch: 56 [10000/50000 (20%)]\tLoss: 1983.412344\n","Train Epoch: 56 [20000/50000 (40%)]\tLoss: 2014.844687\n","Train Epoch: 56 [30000/50000 (60%)]\tLoss: 1945.943594\n","Train Epoch: 56 [40000/50000 (80%)]\tLoss: 2007.752031\n","====> Epoch: 56 Average loss: 2002.8069\n","====> Test set loss: 2000.9418\n","_________________________________\n","Train Epoch: 57 [0/50000 (0%)]\tLoss: 1974.235000\n","Train Epoch: 57 [10000/50000 (20%)]\tLoss: 2028.594687\n","Train Epoch: 57 [20000/50000 (40%)]\tLoss: 2036.378125\n","Train Epoch: 57 [30000/50000 (60%)]\tLoss: 2005.959375\n","Train Epoch: 57 [40000/50000 (80%)]\tLoss: 2009.534531\n","====> Epoch: 57 Average loss: 2002.7900\n","====> Test set loss: 2000.9900\n","_________________________________\n","Train Epoch: 58 [0/50000 (0%)]\tLoss: 2009.243750\n","Train Epoch: 58 [10000/50000 (20%)]\tLoss: 1949.239063\n","Train Epoch: 58 [20000/50000 (40%)]\tLoss: 2002.581094\n","Train Epoch: 58 [30000/50000 (60%)]\tLoss: 1984.017031\n","Train Epoch: 58 [40000/50000 (80%)]\tLoss: 1993.051250\n","====> Epoch: 58 Average loss: 2002.7278\n","====> Test set loss: 2000.7740\n","_________________________________\n","Train Epoch: 59 [0/50000 (0%)]\tLoss: 2024.517969\n","Train Epoch: 59 [10000/50000 (20%)]\tLoss: 2026.579375\n","Train Epoch: 59 [20000/50000 (40%)]\tLoss: 1984.327656\n","Train Epoch: 59 [30000/50000 (60%)]\tLoss: 2010.139219\n","Train Epoch: 59 [40000/50000 (80%)]\tLoss: 2020.203281\n","====> Epoch: 59 Average loss: 2002.7835\n","====> Test set loss: 2000.6605\n","_________________________________\n","Train Epoch: 60 [0/50000 (0%)]\tLoss: 1989.650625\n","Train Epoch: 60 [10000/50000 (20%)]\tLoss: 2001.400625\n","Train Epoch: 60 [20000/50000 (40%)]\tLoss: 1997.111719\n","Train Epoch: 60 [30000/50000 (60%)]\tLoss: 1984.222188\n","Train Epoch: 60 [40000/50000 (80%)]\tLoss: 2017.429063\n","====> Epoch: 60 Average loss: 2002.7477\n","====> Test set loss: 2000.7724\n","_________________________________\n","Train Epoch: 61 [0/50000 (0%)]\tLoss: 2025.744531\n","Train Epoch: 61 [10000/50000 (20%)]\tLoss: 1969.121406\n","Train Epoch: 61 [20000/50000 (40%)]\tLoss: 1989.388125\n","Train Epoch: 61 [30000/50000 (60%)]\tLoss: 1991.213750\n","Train Epoch: 61 [40000/50000 (80%)]\tLoss: 1994.796875\n","====> Epoch: 61 Average loss: 2002.6918\n","====> Test set loss: 2000.6063\n","_________________________________\n","Train Epoch: 62 [0/50000 (0%)]\tLoss: 1986.126250\n","Train Epoch: 62 [10000/50000 (20%)]\tLoss: 1972.531563\n","Train Epoch: 62 [20000/50000 (40%)]\tLoss: 2025.492969\n","Train Epoch: 62 [30000/50000 (60%)]\tLoss: 2045.198750\n","Train Epoch: 62 [40000/50000 (80%)]\tLoss: 2007.572500\n","====> Epoch: 62 Average loss: 2002.6816\n","====> Test set loss: 2000.7076\n","_________________________________\n","Train Epoch: 63 [0/50000 (0%)]\tLoss: 1974.430938\n","Train Epoch: 63 [10000/50000 (20%)]\tLoss: 2006.078594\n","Train Epoch: 63 [20000/50000 (40%)]\tLoss: 1978.028594\n","Train Epoch: 63 [30000/50000 (60%)]\tLoss: 2021.108281\n","Train Epoch: 63 [40000/50000 (80%)]\tLoss: 1947.672812\n","====> Epoch: 63 Average loss: 2002.6539\n","====> Test set loss: 2000.5666\n","_________________________________\n","Train Epoch: 64 [0/50000 (0%)]\tLoss: 1997.029375\n","Train Epoch: 64 [10000/50000 (20%)]\tLoss: 2040.937656\n","Train Epoch: 64 [20000/50000 (40%)]\tLoss: 2029.973281\n","Train Epoch: 64 [30000/50000 (60%)]\tLoss: 1993.307656\n","Train Epoch: 64 [40000/50000 (80%)]\tLoss: 2019.019844\n","====> Epoch: 64 Average loss: 2002.6546\n","====> Test set loss: 2000.7590\n","_________________________________\n","Train Epoch: 65 [0/50000 (0%)]\tLoss: 1989.630156\n","Train Epoch: 65 [10000/50000 (20%)]\tLoss: 2018.590469\n","Train Epoch: 65 [20000/50000 (40%)]\tLoss: 2031.939375\n","Train Epoch: 65 [30000/50000 (60%)]\tLoss: 1930.850156\n","Train Epoch: 65 [40000/50000 (80%)]\tLoss: 2003.635469\n","====> Epoch: 65 Average loss: 2002.6062\n","====> Test set loss: 2000.5070\n","_________________________________\n","Train Epoch: 66 [0/50000 (0%)]\tLoss: 1995.172812\n","Train Epoch: 66 [10000/50000 (20%)]\tLoss: 1964.064219\n","Train Epoch: 66 [20000/50000 (40%)]\tLoss: 1990.617969\n","Train Epoch: 66 [30000/50000 (60%)]\tLoss: 1978.087500\n","Train Epoch: 66 [40000/50000 (80%)]\tLoss: 1975.208594\n","====> Epoch: 66 Average loss: 2002.6185\n","====> Test set loss: 2000.6131\n","_________________________________\n","Train Epoch: 67 [0/50000 (0%)]\tLoss: 2003.991406\n","Train Epoch: 67 [10000/50000 (20%)]\tLoss: 2030.892031\n","Train Epoch: 67 [20000/50000 (40%)]\tLoss: 1999.654531\n","Train Epoch: 67 [30000/50000 (60%)]\tLoss: 2004.396094\n","Train Epoch: 67 [40000/50000 (80%)]\tLoss: 2013.032813\n","====> Epoch: 67 Average loss: 2002.6302\n","====> Test set loss: 2000.5853\n","_________________________________\n","Train Epoch: 68 [0/50000 (0%)]\tLoss: 2007.102031\n","Train Epoch: 68 [10000/50000 (20%)]\tLoss: 2003.837656\n","Train Epoch: 68 [20000/50000 (40%)]\tLoss: 2004.066875\n","Train Epoch: 68 [30000/50000 (60%)]\tLoss: 2010.812500\n","Train Epoch: 68 [40000/50000 (80%)]\tLoss: 1994.647969\n","====> Epoch: 68 Average loss: 2002.5734\n","====> Test set loss: 2000.9767\n","_________________________________\n","Train Epoch: 69 [0/50000 (0%)]\tLoss: 2020.047656\n","Train Epoch: 69 [10000/50000 (20%)]\tLoss: 2025.230469\n","Train Epoch: 69 [20000/50000 (40%)]\tLoss: 1983.270469\n","Train Epoch: 69 [30000/50000 (60%)]\tLoss: 2013.958125\n","Train Epoch: 69 [40000/50000 (80%)]\tLoss: 1996.346562\n","====> Epoch: 69 Average loss: 2002.5819\n","====> Test set loss: 2000.4731\n","_________________________________\n","Train Epoch: 70 [0/50000 (0%)]\tLoss: 1992.512031\n","Train Epoch: 70 [10000/50000 (20%)]\tLoss: 1990.075000\n","Train Epoch: 70 [20000/50000 (40%)]\tLoss: 2005.463438\n","Train Epoch: 70 [30000/50000 (60%)]\tLoss: 2024.166250\n","Train Epoch: 70 [40000/50000 (80%)]\tLoss: 2006.452500\n","====> Epoch: 70 Average loss: 2002.5712\n","====> Test set loss: 2000.4874\n","_________________________________\n","Train Epoch: 71 [0/50000 (0%)]\tLoss: 2016.595313\n","Train Epoch: 71 [10000/50000 (20%)]\tLoss: 2033.304219\n","Train Epoch: 71 [20000/50000 (40%)]\tLoss: 2015.223438\n","Train Epoch: 71 [30000/50000 (60%)]\tLoss: 1979.002500\n","Train Epoch: 71 [40000/50000 (80%)]\tLoss: 1995.103281\n","====> Epoch: 71 Average loss: 2002.5654\n","====> Test set loss: 2000.5534\n","_________________________________\n","Train Epoch: 72 [0/50000 (0%)]\tLoss: 2014.587500\n","Train Epoch: 72 [10000/50000 (20%)]\tLoss: 2010.335469\n","Train Epoch: 72 [20000/50000 (40%)]\tLoss: 1986.095313\n","Train Epoch: 72 [30000/50000 (60%)]\tLoss: 1977.130781\n","Train Epoch: 72 [40000/50000 (80%)]\tLoss: 1994.263750\n","====> Epoch: 72 Average loss: 2002.5410\n","====> Test set loss: 2001.7074\n","_________________________________\n","Train Epoch: 73 [0/50000 (0%)]\tLoss: 1993.122500\n","Train Epoch: 73 [10000/50000 (20%)]\tLoss: 1998.752500\n","Train Epoch: 73 [20000/50000 (40%)]\tLoss: 2009.215625\n","Train Epoch: 73 [30000/50000 (60%)]\tLoss: 2017.612031\n","Train Epoch: 73 [40000/50000 (80%)]\tLoss: 2018.554688\n","====> Epoch: 73 Average loss: 2002.5122\n","====> Test set loss: 2000.4681\n","_________________________________\n","Train Epoch: 74 [0/50000 (0%)]\tLoss: 1963.304375\n","Train Epoch: 74 [10000/50000 (20%)]\tLoss: 1995.172500\n","Train Epoch: 74 [20000/50000 (40%)]\tLoss: 1993.844375\n","Train Epoch: 74 [30000/50000 (60%)]\tLoss: 2011.279375\n","Train Epoch: 74 [40000/50000 (80%)]\tLoss: 2027.566406\n","====> Epoch: 74 Average loss: 2002.4747\n","====> Test set loss: 2000.4251\n","_________________________________\n","Train Epoch: 75 [0/50000 (0%)]\tLoss: 2013.030937\n","Train Epoch: 75 [10000/50000 (20%)]\tLoss: 2021.761406\n","Train Epoch: 75 [20000/50000 (40%)]\tLoss: 1994.060469\n","Train Epoch: 75 [30000/50000 (60%)]\tLoss: 2027.629062\n","Train Epoch: 75 [40000/50000 (80%)]\tLoss: 2029.173125\n","====> Epoch: 75 Average loss: 2002.4902\n","====> Test set loss: 2000.4681\n","_________________________________\n","Train Epoch: 76 [0/50000 (0%)]\tLoss: 1993.529219\n","Train Epoch: 76 [10000/50000 (20%)]\tLoss: 1995.038594\n","Train Epoch: 76 [20000/50000 (40%)]\tLoss: 2004.448750\n","Train Epoch: 76 [30000/50000 (60%)]\tLoss: 1980.456250\n","Train Epoch: 76 [40000/50000 (80%)]\tLoss: 2006.556719\n","====> Epoch: 76 Average loss: 2002.4754\n","====> Test set loss: 2000.4659\n","_________________________________\n","Train Epoch: 77 [0/50000 (0%)]\tLoss: 2005.561406\n","Train Epoch: 77 [10000/50000 (20%)]\tLoss: 2005.085938\n","Train Epoch: 77 [20000/50000 (40%)]\tLoss: 2034.937813\n","Train Epoch: 77 [30000/50000 (60%)]\tLoss: 2013.185313\n","Train Epoch: 77 [40000/50000 (80%)]\tLoss: 1980.471875\n","====> Epoch: 77 Average loss: 2002.4801\n","====> Test set loss: 2000.4662\n","_________________________________\n","Train Epoch: 78 [0/50000 (0%)]\tLoss: 1998.678750\n","Train Epoch: 78 [10000/50000 (20%)]\tLoss: 1993.842969\n","Train Epoch: 78 [20000/50000 (40%)]\tLoss: 1946.604063\n","Train Epoch: 78 [30000/50000 (60%)]\tLoss: 1987.455937\n","Train Epoch: 78 [40000/50000 (80%)]\tLoss: 1988.833594\n","====> Epoch: 78 Average loss: 2002.4599\n","====> Test set loss: 2000.6177\n","_________________________________\n","Train Epoch: 79 [0/50000 (0%)]\tLoss: 2017.834219\n","Train Epoch: 79 [10000/50000 (20%)]\tLoss: 1933.604375\n","Train Epoch: 79 [20000/50000 (40%)]\tLoss: 1983.527031\n","Train Epoch: 79 [30000/50000 (60%)]\tLoss: 1997.714219\n","Train Epoch: 79 [40000/50000 (80%)]\tLoss: 2018.499375\n","====> Epoch: 79 Average loss: 2002.4447\n","====> Test set loss: 2000.5109\n","_________________________________\n","Train Epoch: 80 [0/50000 (0%)]\tLoss: 1983.713906\n","Train Epoch: 80 [10000/50000 (20%)]\tLoss: 2008.525625\n","Train Epoch: 80 [20000/50000 (40%)]\tLoss: 2001.283594\n","Train Epoch: 80 [30000/50000 (60%)]\tLoss: 2025.576094\n","Train Epoch: 80 [40000/50000 (80%)]\tLoss: 2013.050937\n","====> Epoch: 80 Average loss: 2002.4534\n","====> Test set loss: 2000.9002\n","_________________________________\n","Train Epoch: 81 [0/50000 (0%)]\tLoss: 1999.179844\n","Train Epoch: 81 [10000/50000 (20%)]\tLoss: 1977.919844\n","Train Epoch: 81 [20000/50000 (40%)]\tLoss: 1987.067969\n","Train Epoch: 81 [30000/50000 (60%)]\tLoss: 1971.771094\n","Train Epoch: 81 [40000/50000 (80%)]\tLoss: 1983.957031\n","====> Epoch: 81 Average loss: 2002.4119\n","====> Test set loss: 2000.4878\n","_________________________________\n","Train Epoch: 82 [0/50000 (0%)]\tLoss: 2015.564844\n","Train Epoch: 82 [10000/50000 (20%)]\tLoss: 1981.947031\n","Train Epoch: 82 [20000/50000 (40%)]\tLoss: 2002.614375\n","Train Epoch: 82 [30000/50000 (60%)]\tLoss: 1993.656406\n","Train Epoch: 82 [40000/50000 (80%)]\tLoss: 1990.859062\n","====> Epoch: 82 Average loss: 2002.4064\n","====> Test set loss: 2000.7161\n","_________________________________\n","Train Epoch: 83 [0/50000 (0%)]\tLoss: 1992.208125\n","Train Epoch: 83 [10000/50000 (20%)]\tLoss: 1995.104531\n","Train Epoch: 83 [20000/50000 (40%)]\tLoss: 1994.430312\n","Train Epoch: 83 [30000/50000 (60%)]\tLoss: 1998.923125\n","Train Epoch: 83 [40000/50000 (80%)]\tLoss: 2009.680625\n","====> Epoch: 83 Average loss: 2002.3647\n","====> Test set loss: 2000.3877\n","_________________________________\n","Train Epoch: 84 [0/50000 (0%)]\tLoss: 2027.303594\n","Train Epoch: 84 [10000/50000 (20%)]\tLoss: 1983.100781\n","Train Epoch: 84 [20000/50000 (40%)]\tLoss: 2012.677187\n","Train Epoch: 84 [30000/50000 (60%)]\tLoss: 1969.290313\n","Train Epoch: 84 [40000/50000 (80%)]\tLoss: 2005.917344\n","====> Epoch: 84 Average loss: 2002.3895\n","====> Test set loss: 2000.3884\n","_________________________________\n","Train Epoch: 85 [0/50000 (0%)]\tLoss: 2025.664531\n","Train Epoch: 85 [10000/50000 (20%)]\tLoss: 2032.431094\n","Train Epoch: 85 [20000/50000 (40%)]\tLoss: 1979.153750\n","Train Epoch: 85 [30000/50000 (60%)]\tLoss: 1997.935000\n","Train Epoch: 85 [40000/50000 (80%)]\tLoss: 2004.303125\n","====> Epoch: 85 Average loss: 2002.3450\n","====> Test set loss: 2000.2186\n","_________________________________\n","Train Epoch: 86 [0/50000 (0%)]\tLoss: 1956.860156\n","Train Epoch: 86 [10000/50000 (20%)]\tLoss: 2026.319844\n","Train Epoch: 86 [20000/50000 (40%)]\tLoss: 2021.287188\n","Train Epoch: 86 [30000/50000 (60%)]\tLoss: 2034.541094\n","Train Epoch: 86 [40000/50000 (80%)]\tLoss: 2012.228437\n","====> Epoch: 86 Average loss: 2002.3514\n","====> Test set loss: 2000.4326\n","_________________________________\n","Train Epoch: 87 [0/50000 (0%)]\tLoss: 1975.938594\n","Train Epoch: 87 [10000/50000 (20%)]\tLoss: 1969.524375\n","Train Epoch: 87 [20000/50000 (40%)]\tLoss: 2027.850625\n","Train Epoch: 87 [30000/50000 (60%)]\tLoss: 2009.402500\n","Train Epoch: 87 [40000/50000 (80%)]\tLoss: 1985.405313\n","====> Epoch: 87 Average loss: 2002.3410\n","====> Test set loss: 2000.2803\n","_________________________________\n","Train Epoch: 88 [0/50000 (0%)]\tLoss: 2010.015625\n","Train Epoch: 88 [10000/50000 (20%)]\tLoss: 1982.922188\n","Train Epoch: 88 [20000/50000 (40%)]\tLoss: 2016.382812\n","Train Epoch: 88 [30000/50000 (60%)]\tLoss: 1972.141719\n","Train Epoch: 88 [40000/50000 (80%)]\tLoss: 1983.439531\n","====> Epoch: 88 Average loss: 2002.3656\n","====> Test set loss: 2000.5560\n","_________________________________\n","Train Epoch: 89 [0/50000 (0%)]\tLoss: 2010.239219\n","Train Epoch: 89 [10000/50000 (20%)]\tLoss: 2002.972344\n","Train Epoch: 89 [20000/50000 (40%)]\tLoss: 1994.241875\n","Train Epoch: 89 [30000/50000 (60%)]\tLoss: 1999.709375\n","Train Epoch: 89 [40000/50000 (80%)]\tLoss: 2015.549375\n","====> Epoch: 89 Average loss: 2002.3082\n","====> Test set loss: 2000.2957\n","_________________________________\n","Train Epoch: 90 [0/50000 (0%)]\tLoss: 2003.787188\n","Train Epoch: 90 [10000/50000 (20%)]\tLoss: 2024.698281\n","Train Epoch: 90 [20000/50000 (40%)]\tLoss: 1950.134219\n","Train Epoch: 90 [30000/50000 (60%)]\tLoss: 2008.238437\n","Train Epoch: 90 [40000/50000 (80%)]\tLoss: 1992.041875\n","====> Epoch: 90 Average loss: 2002.3126\n","====> Test set loss: 2000.5302\n","_________________________________\n","Train Epoch: 91 [0/50000 (0%)]\tLoss: 2018.849375\n","Train Epoch: 91 [10000/50000 (20%)]\tLoss: 2002.235000\n","Train Epoch: 91 [20000/50000 (40%)]\tLoss: 2045.499844\n","Train Epoch: 91 [30000/50000 (60%)]\tLoss: 2002.046406\n","Train Epoch: 91 [40000/50000 (80%)]\tLoss: 1998.667500\n","====> Epoch: 91 Average loss: 2002.3515\n","====> Test set loss: 2000.7513\n","_________________________________\n","Train Epoch: 92 [0/50000 (0%)]\tLoss: 2008.413281\n","Train Epoch: 92 [10000/50000 (20%)]\tLoss: 2005.861875\n","Train Epoch: 92 [20000/50000 (40%)]\tLoss: 1955.901406\n","Train Epoch: 92 [30000/50000 (60%)]\tLoss: 1993.035000\n","Train Epoch: 92 [40000/50000 (80%)]\tLoss: 1993.877187\n","====> Epoch: 92 Average loss: 2002.3232\n","====> Test set loss: 2000.2884\n","_________________________________\n","Train Epoch: 93 [0/50000 (0%)]\tLoss: 2017.985938\n","Train Epoch: 93 [10000/50000 (20%)]\tLoss: 1997.181094\n","Train Epoch: 93 [20000/50000 (40%)]\tLoss: 2016.319219\n","Train Epoch: 93 [30000/50000 (60%)]\tLoss: 2013.859688\n","Train Epoch: 93 [40000/50000 (80%)]\tLoss: 2008.609375\n","====> Epoch: 93 Average loss: 2002.3038\n","====> Test set loss: 2000.2591\n","_________________________________\n","Train Epoch: 94 [0/50000 (0%)]\tLoss: 1973.729063\n","Train Epoch: 94 [10000/50000 (20%)]\tLoss: 1974.614219\n","Train Epoch: 94 [20000/50000 (40%)]\tLoss: 1995.464531\n","Train Epoch: 94 [30000/50000 (60%)]\tLoss: 1990.516719\n","Train Epoch: 94 [40000/50000 (80%)]\tLoss: 1997.752812\n","====> Epoch: 94 Average loss: 2002.3050\n","====> Test set loss: 2000.1755\n","_________________________________\n","Train Epoch: 95 [0/50000 (0%)]\tLoss: 2004.199219\n","Train Epoch: 95 [10000/50000 (20%)]\tLoss: 1989.815000\n","Train Epoch: 95 [20000/50000 (40%)]\tLoss: 1981.949219\n","Train Epoch: 95 [30000/50000 (60%)]\tLoss: 2012.697031\n","Train Epoch: 95 [40000/50000 (80%)]\tLoss: 2017.013125\n","====> Epoch: 95 Average loss: 2002.2298\n","====> Test set loss: 2000.2666\n","_________________________________\n","Train Epoch: 96 [0/50000 (0%)]\tLoss: 1993.300625\n","Train Epoch: 96 [10000/50000 (20%)]\tLoss: 1982.659375\n","Train Epoch: 96 [20000/50000 (40%)]\tLoss: 1956.530625\n","Train Epoch: 96 [30000/50000 (60%)]\tLoss: 2021.221250\n","Train Epoch: 96 [40000/50000 (80%)]\tLoss: 1965.879688\n","====> Epoch: 96 Average loss: 2002.3189\n","====> Test set loss: 2000.8275\n","_________________________________\n","Train Epoch: 97 [0/50000 (0%)]\tLoss: 2004.649219\n","Train Epoch: 97 [10000/50000 (20%)]\tLoss: 1981.470469\n","Train Epoch: 97 [20000/50000 (40%)]\tLoss: 1994.425469\n","Train Epoch: 97 [30000/50000 (60%)]\tLoss: 1987.275469\n","Train Epoch: 97 [40000/50000 (80%)]\tLoss: 1987.357031\n","====> Epoch: 97 Average loss: 2002.2611\n","====> Test set loss: 2000.4398\n","_________________________________\n","Train Epoch: 98 [0/50000 (0%)]\tLoss: 1972.276406\n","Train Epoch: 98 [10000/50000 (20%)]\tLoss: 2000.173750\n","Train Epoch: 98 [20000/50000 (40%)]\tLoss: 1987.529531\n","Train Epoch: 98 [30000/50000 (60%)]\tLoss: 2009.207500\n","Train Epoch: 98 [40000/50000 (80%)]\tLoss: 2020.435781\n","====> Epoch: 98 Average loss: 2002.3259\n","====> Test set loss: 2000.5127\n","_________________________________\n","Train Epoch: 99 [0/50000 (0%)]\tLoss: 1989.308125\n","Train Epoch: 99 [10000/50000 (20%)]\tLoss: 1989.901875\n","Train Epoch: 99 [20000/50000 (40%)]\tLoss: 2020.061406\n","Train Epoch: 99 [30000/50000 (60%)]\tLoss: 2033.413750\n","Train Epoch: 99 [40000/50000 (80%)]\tLoss: 1982.821875\n","====> Epoch: 99 Average loss: 2002.2257\n","====> Test set loss: 2000.4717\n","_________________________________\n","Train Epoch: 100 [0/50000 (0%)]\tLoss: 1983.806250\n","Train Epoch: 100 [10000/50000 (20%)]\tLoss: 2013.274062\n","Train Epoch: 100 [20000/50000 (40%)]\tLoss: 2027.421562\n","Train Epoch: 100 [30000/50000 (60%)]\tLoss: 2007.708125\n","Train Epoch: 100 [40000/50000 (80%)]\tLoss: 2022.951250\n","====> Epoch: 100 Average loss: 2002.2955\n","====> Test set loss: 2000.3838\n","_________________________________\n","Train Epoch: 101 [0/50000 (0%)]\tLoss: 1996.977969\n","Train Epoch: 101 [10000/50000 (20%)]\tLoss: 2037.030156\n","Train Epoch: 101 [20000/50000 (40%)]\tLoss: 1983.212031\n","Train Epoch: 101 [30000/50000 (60%)]\tLoss: 2008.894375\n","Train Epoch: 101 [40000/50000 (80%)]\tLoss: 1985.900781\n","====> Epoch: 101 Average loss: 2002.2206\n","====> Test set loss: 2000.6837\n","_________________________________\n","Train Epoch: 102 [0/50000 (0%)]\tLoss: 2008.610938\n","Train Epoch: 102 [10000/50000 (20%)]\tLoss: 2020.987344\n","Train Epoch: 102 [20000/50000 (40%)]\tLoss: 2012.820781\n","Train Epoch: 102 [30000/50000 (60%)]\tLoss: 1991.337344\n","Train Epoch: 102 [40000/50000 (80%)]\tLoss: 2021.446250\n","====> Epoch: 102 Average loss: 2002.2817\n","====> Test set loss: 2000.2282\n","_________________________________\n","Train Epoch: 103 [0/50000 (0%)]\tLoss: 2028.501875\n","Train Epoch: 103 [10000/50000 (20%)]\tLoss: 2022.325781\n","Train Epoch: 103 [20000/50000 (40%)]\tLoss: 2009.181406\n","Train Epoch: 103 [30000/50000 (60%)]\tLoss: 2000.004688\n","Train Epoch: 103 [40000/50000 (80%)]\tLoss: 2020.504219\n","====> Epoch: 103 Average loss: 2002.2116\n","====> Test set loss: 2000.2022\n","_________________________________\n","Train Epoch: 104 [0/50000 (0%)]\tLoss: 1990.161875\n","Train Epoch: 104 [10000/50000 (20%)]\tLoss: 1997.593437\n","Train Epoch: 104 [20000/50000 (40%)]\tLoss: 2032.006719\n","Train Epoch: 104 [30000/50000 (60%)]\tLoss: 2022.055312\n","Train Epoch: 104 [40000/50000 (80%)]\tLoss: 2028.552500\n","====> Epoch: 104 Average loss: 2002.2739\n","====> Test set loss: 2000.6476\n","_________________________________\n","Train Epoch: 105 [0/50000 (0%)]\tLoss: 2022.830000\n","Train Epoch: 105 [10000/50000 (20%)]\tLoss: 1992.289062\n","Train Epoch: 105 [20000/50000 (40%)]\tLoss: 1980.912031\n","Train Epoch: 105 [30000/50000 (60%)]\tLoss: 1993.285781\n","Train Epoch: 105 [40000/50000 (80%)]\tLoss: 2013.180469\n","====> Epoch: 105 Average loss: 2002.2078\n","====> Test set loss: 2000.2043\n","_________________________________\n","Train Epoch: 106 [0/50000 (0%)]\tLoss: 1999.747188\n","Train Epoch: 106 [10000/50000 (20%)]\tLoss: 1962.796250\n","Train Epoch: 106 [20000/50000 (40%)]\tLoss: 2013.641094\n","Train Epoch: 106 [30000/50000 (60%)]\tLoss: 2022.815156\n","Train Epoch: 106 [40000/50000 (80%)]\tLoss: 1997.134687\n","====> Epoch: 106 Average loss: 2002.1962\n","====> Test set loss: 2000.4280\n","_________________________________\n","Train Epoch: 107 [0/50000 (0%)]\tLoss: 1960.271719\n","Train Epoch: 107 [10000/50000 (20%)]\tLoss: 2037.134219\n","Train Epoch: 107 [20000/50000 (40%)]\tLoss: 2006.493906\n","Train Epoch: 107 [30000/50000 (60%)]\tLoss: 1982.967969\n","Train Epoch: 107 [40000/50000 (80%)]\tLoss: 1983.642500\n","====> Epoch: 107 Average loss: 2002.1585\n","====> Test set loss: 2000.2971\n","_________________________________\n","Train Epoch: 108 [0/50000 (0%)]\tLoss: 2003.663125\n","Train Epoch: 108 [10000/50000 (20%)]\tLoss: 1990.536875\n","Train Epoch: 108 [20000/50000 (40%)]\tLoss: 2001.570469\n","Train Epoch: 108 [30000/50000 (60%)]\tLoss: 2011.552500\n","Train Epoch: 108 [40000/50000 (80%)]\tLoss: 2010.391719\n","====> Epoch: 108 Average loss: 2002.1962\n","====> Test set loss: 2000.1927\n","_________________________________\n","Train Epoch: 109 [0/50000 (0%)]\tLoss: 2017.689063\n","Train Epoch: 109 [10000/50000 (20%)]\tLoss: 1984.319375\n","Train Epoch: 109 [20000/50000 (40%)]\tLoss: 2007.902656\n","Train Epoch: 109 [30000/50000 (60%)]\tLoss: 2001.492969\n","Train Epoch: 109 [40000/50000 (80%)]\tLoss: 2016.445625\n","====> Epoch: 109 Average loss: 2002.1446\n","====> Test set loss: 2000.3310\n","_________________________________\n","Train Epoch: 110 [0/50000 (0%)]\tLoss: 2008.638125\n","Train Epoch: 110 [10000/50000 (20%)]\tLoss: 1997.633437\n","Train Epoch: 110 [20000/50000 (40%)]\tLoss: 2036.539687\n","Train Epoch: 110 [30000/50000 (60%)]\tLoss: 2033.024062\n","Train Epoch: 110 [40000/50000 (80%)]\tLoss: 1986.462656\n","====> Epoch: 110 Average loss: 2002.1511\n","====> Test set loss: 2000.3294\n","_________________________________\n","Train Epoch: 111 [0/50000 (0%)]\tLoss: 1999.793750\n","Train Epoch: 111 [10000/50000 (20%)]\tLoss: 1994.655937\n","Train Epoch: 111 [20000/50000 (40%)]\tLoss: 2010.537031\n","Train Epoch: 111 [30000/50000 (60%)]\tLoss: 2017.554688\n","Train Epoch: 111 [40000/50000 (80%)]\tLoss: 2002.880156\n","====> Epoch: 111 Average loss: 2002.1139\n","====> Test set loss: 2000.2478\n","_________________________________\n","Train Epoch: 112 [0/50000 (0%)]\tLoss: 2017.929219\n","Train Epoch: 112 [10000/50000 (20%)]\tLoss: 2014.777500\n","Train Epoch: 112 [20000/50000 (40%)]\tLoss: 1996.401875\n","Train Epoch: 112 [30000/50000 (60%)]\tLoss: 1968.902812\n","Train Epoch: 112 [40000/50000 (80%)]\tLoss: 2000.985000\n","====> Epoch: 112 Average loss: 2002.1292\n","====> Test set loss: 2000.1769\n","_________________________________\n","Train Epoch: 113 [0/50000 (0%)]\tLoss: 2012.216563\n","Train Epoch: 113 [10000/50000 (20%)]\tLoss: 1961.914219\n","Train Epoch: 113 [20000/50000 (40%)]\tLoss: 2003.495469\n","Train Epoch: 113 [30000/50000 (60%)]\tLoss: 1984.061250\n","Train Epoch: 113 [40000/50000 (80%)]\tLoss: 1974.723594\n","====> Epoch: 113 Average loss: 2002.1792\n","====> Test set loss: 2000.2565\n","_________________________________\n","Train Epoch: 114 [0/50000 (0%)]\tLoss: 2014.665469\n","Train Epoch: 114 [10000/50000 (20%)]\tLoss: 2018.630781\n","Train Epoch: 114 [20000/50000 (40%)]\tLoss: 2044.368750\n","Train Epoch: 114 [30000/50000 (60%)]\tLoss: 1989.472500\n","Train Epoch: 114 [40000/50000 (80%)]\tLoss: 2025.775938\n","====> Epoch: 114 Average loss: 2002.1399\n","====> Test set loss: 2000.1082\n","_________________________________\n","Train Epoch: 115 [0/50000 (0%)]\tLoss: 1975.520156\n","Train Epoch: 115 [10000/50000 (20%)]\tLoss: 1987.608750\n","Train Epoch: 115 [20000/50000 (40%)]\tLoss: 2023.481250\n","Train Epoch: 115 [30000/50000 (60%)]\tLoss: 2006.625156\n","Train Epoch: 115 [40000/50000 (80%)]\tLoss: 1982.225000\n","====> Epoch: 115 Average loss: 2002.1106\n","====> Test set loss: 2000.1798\n","_________________________________\n","Train Epoch: 116 [0/50000 (0%)]\tLoss: 2035.549844\n","Train Epoch: 116 [10000/50000 (20%)]\tLoss: 1983.882656\n","Train Epoch: 116 [20000/50000 (40%)]\tLoss: 2019.712500\n","Train Epoch: 116 [30000/50000 (60%)]\tLoss: 2026.666094\n","Train Epoch: 116 [40000/50000 (80%)]\tLoss: 2000.934531\n","====> Epoch: 116 Average loss: 2002.1478\n","====> Test set loss: 2000.3083\n","_________________________________\n","Train Epoch: 117 [0/50000 (0%)]\tLoss: 2019.935469\n","Train Epoch: 117 [10000/50000 (20%)]\tLoss: 1951.340312\n","Train Epoch: 117 [20000/50000 (40%)]\tLoss: 1989.320000\n","Train Epoch: 117 [30000/50000 (60%)]\tLoss: 1988.863750\n","Train Epoch: 117 [40000/50000 (80%)]\tLoss: 1969.588281\n","====> Epoch: 117 Average loss: 2002.0932\n","====> Test set loss: 2000.1937\n","_________________________________\n","Train Epoch: 118 [0/50000 (0%)]\tLoss: 2033.672656\n","Train Epoch: 118 [10000/50000 (20%)]\tLoss: 2027.284375\n","Train Epoch: 118 [20000/50000 (40%)]\tLoss: 2029.747813\n","Train Epoch: 118 [30000/50000 (60%)]\tLoss: 2029.467969\n","Train Epoch: 118 [40000/50000 (80%)]\tLoss: 1995.747656\n","====> Epoch: 118 Average loss: 2002.1611\n","====> Test set loss: 2000.3129\n","_________________________________\n","Train Epoch: 119 [0/50000 (0%)]\tLoss: 1991.104375\n","Train Epoch: 119 [10000/50000 (20%)]\tLoss: 2022.352656\n","Train Epoch: 119 [20000/50000 (40%)]\tLoss: 2033.640938\n","Train Epoch: 119 [30000/50000 (60%)]\tLoss: 1993.792656\n","Train Epoch: 119 [40000/50000 (80%)]\tLoss: 1995.485000\n","====> Epoch: 119 Average loss: 2002.1061\n","====> Test set loss: 2000.2751\n","_________________________________\n","Train Epoch: 120 [0/50000 (0%)]\tLoss: 2010.793437\n","Train Epoch: 120 [10000/50000 (20%)]\tLoss: 1983.240156\n","Train Epoch: 120 [20000/50000 (40%)]\tLoss: 1990.478750\n","Train Epoch: 120 [30000/50000 (60%)]\tLoss: 2027.306719\n","Train Epoch: 120 [40000/50000 (80%)]\tLoss: 1978.209687\n","====> Epoch: 120 Average loss: 2002.0912\n","====> Test set loss: 2000.2781\n","_________________________________\n","Train Epoch: 121 [0/50000 (0%)]\tLoss: 2010.366875\n","Train Epoch: 121 [10000/50000 (20%)]\tLoss: 2024.880781\n","Train Epoch: 121 [20000/50000 (40%)]\tLoss: 1991.868906\n","Train Epoch: 121 [30000/50000 (60%)]\tLoss: 2026.364531\n","Train Epoch: 121 [40000/50000 (80%)]\tLoss: 1984.872656\n","====> Epoch: 121 Average loss: 2002.1229\n","====> Test set loss: 2000.3012\n","_________________________________\n","Train Epoch: 122 [0/50000 (0%)]\tLoss: 1991.327812\n","Train Epoch: 122 [10000/50000 (20%)]\tLoss: 1989.987031\n","Train Epoch: 122 [20000/50000 (40%)]\tLoss: 1977.002344\n","Train Epoch: 122 [30000/50000 (60%)]\tLoss: 1999.392969\n","Train Epoch: 122 [40000/50000 (80%)]\tLoss: 2025.824375\n","====> Epoch: 122 Average loss: 2002.1227\n","====> Test set loss: 1999.9850\n","_________________________________\n","Train Epoch: 123 [0/50000 (0%)]\tLoss: 1995.989375\n","Train Epoch: 123 [10000/50000 (20%)]\tLoss: 1985.648906\n","Train Epoch: 123 [20000/50000 (40%)]\tLoss: 1962.913594\n","Train Epoch: 123 [30000/50000 (60%)]\tLoss: 1981.150312\n","Train Epoch: 123 [40000/50000 (80%)]\tLoss: 2006.909688\n","====> Epoch: 123 Average loss: 2002.0701\n","====> Test set loss: 2000.2589\n","_________________________________\n","Train Epoch: 124 [0/50000 (0%)]\tLoss: 1991.022500\n","Train Epoch: 124 [10000/50000 (20%)]\tLoss: 1987.822813\n","Train Epoch: 124 [20000/50000 (40%)]\tLoss: 1987.214844\n","Train Epoch: 124 [30000/50000 (60%)]\tLoss: 1997.755156\n","Train Epoch: 124 [40000/50000 (80%)]\tLoss: 1967.357031\n","====> Epoch: 124 Average loss: 2002.1195\n","====> Test set loss: 2000.1696\n","_________________________________\n","Train Epoch: 125 [0/50000 (0%)]\tLoss: 2001.968594\n","Train Epoch: 125 [10000/50000 (20%)]\tLoss: 2009.593750\n","Train Epoch: 125 [20000/50000 (40%)]\tLoss: 1978.925000\n","Train Epoch: 125 [30000/50000 (60%)]\tLoss: 1961.701250\n","Train Epoch: 125 [40000/50000 (80%)]\tLoss: 1996.434375\n","====> Epoch: 125 Average loss: 2002.0442\n","====> Test set loss: 1999.9995\n","_________________________________\n","Train Epoch: 126 [0/50000 (0%)]\tLoss: 1953.730156\n","Train Epoch: 126 [10000/50000 (20%)]\tLoss: 1986.259844\n","Train Epoch: 126 [20000/50000 (40%)]\tLoss: 1983.823281\n","Train Epoch: 126 [30000/50000 (60%)]\tLoss: 2020.005469\n","Train Epoch: 126 [40000/50000 (80%)]\tLoss: 2018.159531\n","====> Epoch: 126 Average loss: 2002.0886\n","====> Test set loss: 2000.2847\n","_________________________________\n","Train Epoch: 127 [0/50000 (0%)]\tLoss: 1996.380469\n","Train Epoch: 127 [10000/50000 (20%)]\tLoss: 1995.414844\n","Train Epoch: 127 [20000/50000 (40%)]\tLoss: 1974.796250\n","Train Epoch: 127 [30000/50000 (60%)]\tLoss: 1984.084531\n","Train Epoch: 127 [40000/50000 (80%)]\tLoss: 1954.246719\n","====> Epoch: 127 Average loss: 2002.0468\n","====> Test set loss: 2000.6465\n","_________________________________\n","Train Epoch: 128 [0/50000 (0%)]\tLoss: 1987.237656\n","Train Epoch: 128 [10000/50000 (20%)]\tLoss: 2033.308125\n","Train Epoch: 128 [20000/50000 (40%)]\tLoss: 2012.099062\n","Train Epoch: 128 [30000/50000 (60%)]\tLoss: 1984.750781\n","Train Epoch: 128 [40000/50000 (80%)]\tLoss: 1964.049375\n","====> Epoch: 128 Average loss: 2002.0177\n","====> Test set loss: 2000.2017\n","_________________________________\n","Train Epoch: 129 [0/50000 (0%)]\tLoss: 1984.338125\n","Train Epoch: 129 [10000/50000 (20%)]\tLoss: 2029.223750\n","Train Epoch: 129 [20000/50000 (40%)]\tLoss: 1988.953125\n","Train Epoch: 129 [30000/50000 (60%)]\tLoss: 1993.817500\n","Train Epoch: 129 [40000/50000 (80%)]\tLoss: 2006.077969\n","====> Epoch: 129 Average loss: 2002.0293\n","====> Test set loss: 2000.3150\n","_________________________________\n","Train Epoch: 130 [0/50000 (0%)]\tLoss: 2029.528125\n","Train Epoch: 130 [10000/50000 (20%)]\tLoss: 2016.983281\n","Train Epoch: 130 [20000/50000 (40%)]\tLoss: 1978.498906\n","Train Epoch: 130 [30000/50000 (60%)]\tLoss: 2021.770000\n","Train Epoch: 130 [40000/50000 (80%)]\tLoss: 1985.564844\n","====> Epoch: 130 Average loss: 2002.0659\n","====> Test set loss: 2000.0396\n","_________________________________\n","Train Epoch: 131 [0/50000 (0%)]\tLoss: 2012.414219\n","Train Epoch: 131 [10000/50000 (20%)]\tLoss: 1998.575625\n","Train Epoch: 131 [20000/50000 (40%)]\tLoss: 1946.165625\n","Train Epoch: 131 [30000/50000 (60%)]\tLoss: 2013.639375\n","Train Epoch: 131 [40000/50000 (80%)]\tLoss: 2011.074531\n","====> Epoch: 131 Average loss: 2002.0492\n","====> Test set loss: 2000.1055\n","_________________________________\n","Train Epoch: 132 [0/50000 (0%)]\tLoss: 1999.912812\n","Train Epoch: 132 [10000/50000 (20%)]\tLoss: 1999.070000\n","Train Epoch: 132 [20000/50000 (40%)]\tLoss: 2010.072969\n","Train Epoch: 132 [30000/50000 (60%)]\tLoss: 2010.342500\n","Train Epoch: 132 [40000/50000 (80%)]\tLoss: 2006.220781\n","====> Epoch: 132 Average loss: 2002.0340\n","====> Test set loss: 2000.0257\n","_________________________________\n","Train Epoch: 133 [0/50000 (0%)]\tLoss: 1937.325313\n","Train Epoch: 133 [10000/50000 (20%)]\tLoss: 2006.630000\n","Train Epoch: 133 [20000/50000 (40%)]\tLoss: 2034.510469\n","Train Epoch: 133 [30000/50000 (60%)]\tLoss: 1988.128281\n","Train Epoch: 133 [40000/50000 (80%)]\tLoss: 2010.319844\n","====> Epoch: 133 Average loss: 2002.1013\n","====> Test set loss: 2000.0968\n","_________________________________\n","Train Epoch: 134 [0/50000 (0%)]\tLoss: 1996.058594\n","Train Epoch: 134 [10000/50000 (20%)]\tLoss: 2029.505156\n","Train Epoch: 134 [20000/50000 (40%)]\tLoss: 1989.446719\n","Train Epoch: 134 [30000/50000 (60%)]\tLoss: 2011.462344\n","Train Epoch: 134 [40000/50000 (80%)]\tLoss: 2012.041406\n","====> Epoch: 134 Average loss: 2001.9966\n","====> Test set loss: 2000.4207\n","_________________________________\n","Train Epoch: 135 [0/50000 (0%)]\tLoss: 1993.844844\n","Train Epoch: 135 [10000/50000 (20%)]\tLoss: 1992.963438\n","Train Epoch: 135 [20000/50000 (40%)]\tLoss: 2015.836562\n","Train Epoch: 135 [30000/50000 (60%)]\tLoss: 2021.912031\n","Train Epoch: 135 [40000/50000 (80%)]\tLoss: 2005.232031\n","====> Epoch: 135 Average loss: 2002.0016\n","====> Test set loss: 2001.8379\n","_________________________________\n","Train Epoch: 136 [0/50000 (0%)]\tLoss: 2008.388438\n","Train Epoch: 136 [10000/50000 (20%)]\tLoss: 2016.452031\n","Train Epoch: 136 [20000/50000 (40%)]\tLoss: 1973.527656\n","Train Epoch: 136 [30000/50000 (60%)]\tLoss: 2033.011563\n","Train Epoch: 136 [40000/50000 (80%)]\tLoss: 1996.874844\n","====> Epoch: 136 Average loss: 2001.9972\n","====> Test set loss: 2000.4536\n","_________________________________\n","Train Epoch: 137 [0/50000 (0%)]\tLoss: 2018.105313\n","Train Epoch: 137 [10000/50000 (20%)]\tLoss: 1999.180000\n","Train Epoch: 137 [20000/50000 (40%)]\tLoss: 2052.995625\n","Train Epoch: 137 [30000/50000 (60%)]\tLoss: 1993.504531\n","Train Epoch: 137 [40000/50000 (80%)]\tLoss: 2016.426563\n","====> Epoch: 137 Average loss: 2001.9308\n","====> Test set loss: 2000.1674\n","_________________________________\n","Train Epoch: 138 [0/50000 (0%)]\tLoss: 2005.042187\n","Train Epoch: 138 [10000/50000 (20%)]\tLoss: 2009.717969\n","Train Epoch: 138 [20000/50000 (40%)]\tLoss: 2002.614375\n","Train Epoch: 138 [30000/50000 (60%)]\tLoss: 2013.696563\n","Train Epoch: 138 [40000/50000 (80%)]\tLoss: 1994.204688\n","====> Epoch: 138 Average loss: 2002.0087\n","====> Test set loss: 2000.0745\n","_________________________________\n","Train Epoch: 139 [0/50000 (0%)]\tLoss: 2007.668750\n","Train Epoch: 139 [10000/50000 (20%)]\tLoss: 1992.221562\n","Train Epoch: 139 [20000/50000 (40%)]\tLoss: 1989.172188\n","Train Epoch: 139 [30000/50000 (60%)]\tLoss: 1994.380938\n","Train Epoch: 139 [40000/50000 (80%)]\tLoss: 1972.413906\n","====> Epoch: 139 Average loss: 2001.9844\n","====> Test set loss: 2000.1022\n","_________________________________\n","Train Epoch: 140 [0/50000 (0%)]\tLoss: 2005.599062\n","Train Epoch: 140 [10000/50000 (20%)]\tLoss: 2028.957969\n","Train Epoch: 140 [20000/50000 (40%)]\tLoss: 2024.015781\n","Train Epoch: 140 [30000/50000 (60%)]\tLoss: 2008.225938\n","Train Epoch: 140 [40000/50000 (80%)]\tLoss: 2029.914687\n","====> Epoch: 140 Average loss: 2001.9795\n","====> Test set loss: 2000.0672\n","_________________________________\n","Train Epoch: 141 [0/50000 (0%)]\tLoss: 1987.442344\n","Train Epoch: 141 [10000/50000 (20%)]\tLoss: 1990.090938\n","Train Epoch: 141 [20000/50000 (40%)]\tLoss: 2007.178125\n","Train Epoch: 141 [30000/50000 (60%)]\tLoss: 2006.037500\n","Train Epoch: 141 [40000/50000 (80%)]\tLoss: 2026.953594\n","====> Epoch: 141 Average loss: 2001.9955\n","====> Test set loss: 2000.2230\n","_________________________________\n","Train Epoch: 142 [0/50000 (0%)]\tLoss: 2002.552500\n","Train Epoch: 142 [10000/50000 (20%)]\tLoss: 1998.238906\n","Train Epoch: 142 [20000/50000 (40%)]\tLoss: 2016.987656\n","Train Epoch: 142 [30000/50000 (60%)]\tLoss: 1983.479063\n","Train Epoch: 142 [40000/50000 (80%)]\tLoss: 2016.901250\n","====> Epoch: 142 Average loss: 2002.0081\n","====> Test set loss: 2000.0129\n","_________________________________\n","Train Epoch: 143 [0/50000 (0%)]\tLoss: 2016.949844\n","Train Epoch: 143 [10000/50000 (20%)]\tLoss: 2006.831563\n","Train Epoch: 143 [20000/50000 (40%)]\tLoss: 2003.357031\n","Train Epoch: 143 [30000/50000 (60%)]\tLoss: 1978.482188\n","Train Epoch: 143 [40000/50000 (80%)]\tLoss: 2003.231250\n","====> Epoch: 143 Average loss: 2001.9570\n","====> Test set loss: 2000.0489\n","_________________________________\n","Train Epoch: 144 [0/50000 (0%)]\tLoss: 2026.215625\n","Train Epoch: 144 [10000/50000 (20%)]\tLoss: 2002.105469\n","Train Epoch: 144 [20000/50000 (40%)]\tLoss: 2012.475469\n","Train Epoch: 144 [30000/50000 (60%)]\tLoss: 2002.588281\n","Train Epoch: 144 [40000/50000 (80%)]\tLoss: 1971.506875\n","====> Epoch: 144 Average loss: 2001.9522\n","====> Test set loss: 2000.0109\n","_________________________________\n","Train Epoch: 145 [0/50000 (0%)]\tLoss: 1981.750781\n","Train Epoch: 145 [10000/50000 (20%)]\tLoss: 1998.835156\n","Train Epoch: 145 [20000/50000 (40%)]\tLoss: 2021.852031\n","Train Epoch: 145 [30000/50000 (60%)]\tLoss: 1966.611719\n","Train Epoch: 145 [40000/50000 (80%)]\tLoss: 1991.911406\n","====> Epoch: 145 Average loss: 2002.0297\n","====> Test set loss: 2000.0208\n","_________________________________\n","Train Epoch: 146 [0/50000 (0%)]\tLoss: 2036.476875\n","Train Epoch: 146 [10000/50000 (20%)]\tLoss: 2008.625625\n","Train Epoch: 146 [20000/50000 (40%)]\tLoss: 2007.456406\n","Train Epoch: 146 [30000/50000 (60%)]\tLoss: 2013.375781\n","Train Epoch: 146 [40000/50000 (80%)]\tLoss: 1988.126250\n","====> Epoch: 146 Average loss: 2001.9521\n","====> Test set loss: 1999.8960\n","_________________________________\n","Train Epoch: 147 [0/50000 (0%)]\tLoss: 1999.334219\n","Train Epoch: 147 [10000/50000 (20%)]\tLoss: 1967.650156\n","Train Epoch: 147 [20000/50000 (40%)]\tLoss: 2006.036406\n","Train Epoch: 147 [30000/50000 (60%)]\tLoss: 2017.423750\n","Train Epoch: 147 [40000/50000 (80%)]\tLoss: 1964.140781\n","====> Epoch: 147 Average loss: 2001.9590\n","====> Test set loss: 2000.3109\n","_________________________________\n","Train Epoch: 148 [0/50000 (0%)]\tLoss: 1997.257188\n","Train Epoch: 148 [10000/50000 (20%)]\tLoss: 1981.715156\n","Train Epoch: 148 [20000/50000 (40%)]\tLoss: 1968.409062\n","Train Epoch: 148 [30000/50000 (60%)]\tLoss: 1960.345781\n","Train Epoch: 148 [40000/50000 (80%)]\tLoss: 1997.721406\n","====> Epoch: 148 Average loss: 2001.9554\n","====> Test set loss: 2000.1210\n","_________________________________\n","Train Epoch: 149 [0/50000 (0%)]\tLoss: 2006.603750\n","Train Epoch: 149 [10000/50000 (20%)]\tLoss: 2017.439375\n","Train Epoch: 149 [20000/50000 (40%)]\tLoss: 2000.773750\n","Train Epoch: 149 [30000/50000 (60%)]\tLoss: 2007.726250\n","Train Epoch: 149 [40000/50000 (80%)]\tLoss: 2016.265469\n","====> Epoch: 149 Average loss: 2001.9592\n","====> Test set loss: 2000.0129\n","_________________________________\n","Train Epoch: 150 [0/50000 (0%)]\tLoss: 1994.896250\n","Train Epoch: 150 [10000/50000 (20%)]\tLoss: 2019.573906\n","Train Epoch: 150 [20000/50000 (40%)]\tLoss: 2026.561875\n","Train Epoch: 150 [30000/50000 (60%)]\tLoss: 2004.836875\n","Train Epoch: 150 [40000/50000 (80%)]\tLoss: 2049.649844\n","====> Epoch: 150 Average loss: 2001.9477\n","====> Test set loss: 1999.9502\n","_________________________________\n","Train Epoch: 151 [0/50000 (0%)]\tLoss: 1988.211250\n","Train Epoch: 151 [10000/50000 (20%)]\tLoss: 2000.622969\n","Train Epoch: 151 [20000/50000 (40%)]\tLoss: 2006.989219\n","Train Epoch: 151 [30000/50000 (60%)]\tLoss: 2004.499219\n","Train Epoch: 151 [40000/50000 (80%)]\tLoss: 2010.974375\n","====> Epoch: 151 Average loss: 2001.8737\n","====> Test set loss: 1999.9463\n","_________________________________\n","Train Epoch: 152 [0/50000 (0%)]\tLoss: 1994.964688\n","Train Epoch: 152 [10000/50000 (20%)]\tLoss: 1970.242500\n","Train Epoch: 152 [20000/50000 (40%)]\tLoss: 2007.614063\n","Train Epoch: 152 [30000/50000 (60%)]\tLoss: 2041.992812\n","Train Epoch: 152 [40000/50000 (80%)]\tLoss: 2021.682969\n","====> Epoch: 152 Average loss: 2001.9627\n","====> Test set loss: 1999.9966\n","_________________________________\n","Train Epoch: 153 [0/50000 (0%)]\tLoss: 1994.512813\n","Train Epoch: 153 [10000/50000 (20%)]\tLoss: 2011.877187\n","Train Epoch: 153 [20000/50000 (40%)]\tLoss: 2029.937187\n","Train Epoch: 153 [30000/50000 (60%)]\tLoss: 2004.189844\n","Train Epoch: 153 [40000/50000 (80%)]\tLoss: 1988.031563\n","====> Epoch: 153 Average loss: 2001.9031\n","====> Test set loss: 2000.1652\n","_________________________________\n","Train Epoch: 154 [0/50000 (0%)]\tLoss: 1970.913438\n","Train Epoch: 154 [10000/50000 (20%)]\tLoss: 1994.962812\n","Train Epoch: 154 [20000/50000 (40%)]\tLoss: 2001.331875\n","Train Epoch: 154 [30000/50000 (60%)]\tLoss: 2025.090625\n","Train Epoch: 154 [40000/50000 (80%)]\tLoss: 2004.640625\n","====> Epoch: 154 Average loss: 2001.9470\n","====> Test set loss: 2000.0445\n","_________________________________\n","Train Epoch: 155 [0/50000 (0%)]\tLoss: 1967.526562\n","Train Epoch: 155 [10000/50000 (20%)]\tLoss: 1999.551563\n","Train Epoch: 155 [20000/50000 (40%)]\tLoss: 2018.033594\n","Train Epoch: 155 [30000/50000 (60%)]\tLoss: 1982.731719\n","Train Epoch: 155 [40000/50000 (80%)]\tLoss: 1981.153281\n","====> Epoch: 155 Average loss: 2001.9446\n","====> Test set loss: 2000.1829\n","_________________________________\n","Train Epoch: 156 [0/50000 (0%)]\tLoss: 1997.883750\n","Train Epoch: 156 [10000/50000 (20%)]\tLoss: 2019.845937\n","Train Epoch: 156 [20000/50000 (40%)]\tLoss: 2024.008437\n","Train Epoch: 156 [30000/50000 (60%)]\tLoss: 1989.654219\n","Train Epoch: 156 [40000/50000 (80%)]\tLoss: 1991.953594\n","====> Epoch: 156 Average loss: 2001.9191\n","====> Test set loss: 1999.9085\n","_________________________________\n","Train Epoch: 157 [0/50000 (0%)]\tLoss: 1983.878594\n","Train Epoch: 157 [10000/50000 (20%)]\tLoss: 2002.453750\n","Train Epoch: 157 [20000/50000 (40%)]\tLoss: 1992.153750\n","Train Epoch: 157 [30000/50000 (60%)]\tLoss: 1986.378594\n","Train Epoch: 157 [40000/50000 (80%)]\tLoss: 2026.100781\n","====> Epoch: 157 Average loss: 2001.9239\n","====> Test set loss: 2000.2704\n","_________________________________\n","Train Epoch: 158 [0/50000 (0%)]\tLoss: 2016.570312\n","Train Epoch: 158 [10000/50000 (20%)]\tLoss: 2002.578594\n","Train Epoch: 158 [20000/50000 (40%)]\tLoss: 1986.442188\n","Train Epoch: 158 [30000/50000 (60%)]\tLoss: 1982.059219\n","Train Epoch: 158 [40000/50000 (80%)]\tLoss: 2017.676563\n","====> Epoch: 158 Average loss: 2001.9128\n","====> Test set loss: 2000.1279\n","_________________________________\n","Train Epoch: 159 [0/50000 (0%)]\tLoss: 1994.876563\n","Train Epoch: 159 [10000/50000 (20%)]\tLoss: 2028.900938\n","Train Epoch: 159 [20000/50000 (40%)]\tLoss: 1974.849375\n","Train Epoch: 159 [30000/50000 (60%)]\tLoss: 2020.959375\n","Train Epoch: 159 [40000/50000 (80%)]\tLoss: 1994.150156\n","====> Epoch: 159 Average loss: 2001.9105\n","====> Test set loss: 2000.1523\n","_________________________________\n","Train Epoch: 160 [0/50000 (0%)]\tLoss: 2021.443906\n","Train Epoch: 160 [10000/50000 (20%)]\tLoss: 2029.187500\n","Train Epoch: 160 [20000/50000 (40%)]\tLoss: 2000.685625\n","Train Epoch: 160 [30000/50000 (60%)]\tLoss: 2030.916563\n","Train Epoch: 160 [40000/50000 (80%)]\tLoss: 1988.042344\n","====> Epoch: 160 Average loss: 2001.9218\n","====> Test set loss: 2000.1111\n","_________________________________\n","Train Epoch: 161 [0/50000 (0%)]\tLoss: 1996.332031\n","Train Epoch: 161 [10000/50000 (20%)]\tLoss: 2003.833594\n","Train Epoch: 161 [20000/50000 (40%)]\tLoss: 2010.144687\n","Train Epoch: 161 [30000/50000 (60%)]\tLoss: 2004.336562\n","Train Epoch: 161 [40000/50000 (80%)]\tLoss: 2005.746562\n","====> Epoch: 161 Average loss: 2001.8522\n","====> Test set loss: 1999.9296\n","_________________________________\n","Train Epoch: 162 [0/50000 (0%)]\tLoss: 1973.812187\n","Train Epoch: 162 [10000/50000 (20%)]\tLoss: 2040.992344\n","Train Epoch: 162 [20000/50000 (40%)]\tLoss: 1996.790156\n","Train Epoch: 162 [30000/50000 (60%)]\tLoss: 2012.824531\n","Train Epoch: 162 [40000/50000 (80%)]\tLoss: 1983.327188\n","====> Epoch: 162 Average loss: 2001.9097\n","====> Test set loss: 2000.1828\n","_________________________________\n","Train Epoch: 163 [0/50000 (0%)]\tLoss: 1991.975938\n","Train Epoch: 163 [10000/50000 (20%)]\tLoss: 2021.398594\n","Train Epoch: 163 [20000/50000 (40%)]\tLoss: 1986.699687\n","Train Epoch: 163 [30000/50000 (60%)]\tLoss: 1999.586250\n","Train Epoch: 163 [40000/50000 (80%)]\tLoss: 1991.083125\n","====> Epoch: 163 Average loss: 2001.9027\n","====> Test set loss: 1999.9259\n","_________________________________\n","Train Epoch: 164 [0/50000 (0%)]\tLoss: 1986.010469\n","Train Epoch: 164 [10000/50000 (20%)]\tLoss: 1992.770313\n","Train Epoch: 164 [20000/50000 (40%)]\tLoss: 2028.303750\n","Train Epoch: 164 [30000/50000 (60%)]\tLoss: 1992.899531\n","Train Epoch: 164 [40000/50000 (80%)]\tLoss: 1998.219375\n","====> Epoch: 164 Average loss: 2001.8398\n","====> Test set loss: 1999.8984\n","_________________________________\n","Train Epoch: 165 [0/50000 (0%)]\tLoss: 2002.109219\n","Train Epoch: 165 [10000/50000 (20%)]\tLoss: 1997.801406\n","Train Epoch: 165 [20000/50000 (40%)]\tLoss: 1999.630938\n","Train Epoch: 165 [30000/50000 (60%)]\tLoss: 1984.960781\n","Train Epoch: 165 [40000/50000 (80%)]\tLoss: 1992.823594\n","====> Epoch: 165 Average loss: 2001.8767\n","====> Test set loss: 2000.1236\n","_________________________________\n","Train Epoch: 166 [0/50000 (0%)]\tLoss: 2003.084531\n","Train Epoch: 166 [10000/50000 (20%)]\tLoss: 2016.177656\n","Train Epoch: 166 [20000/50000 (40%)]\tLoss: 1994.328906\n","Train Epoch: 166 [30000/50000 (60%)]\tLoss: 2014.095469\n","Train Epoch: 166 [40000/50000 (80%)]\tLoss: 1972.438437\n","====> Epoch: 166 Average loss: 2001.8614\n","====> Test set loss: 2000.0480\n","_________________________________\n","Train Epoch: 167 [0/50000 (0%)]\tLoss: 1989.023750\n","Train Epoch: 167 [10000/50000 (20%)]\tLoss: 1975.458906\n","Train Epoch: 167 [20000/50000 (40%)]\tLoss: 2012.725781\n","Train Epoch: 167 [30000/50000 (60%)]\tLoss: 2028.666406\n","Train Epoch: 167 [40000/50000 (80%)]\tLoss: 2015.720156\n","====> Epoch: 167 Average loss: 2001.8823\n","====> Test set loss: 1999.8917\n","_________________________________\n","Train Epoch: 168 [0/50000 (0%)]\tLoss: 2010.101094\n","Train Epoch: 168 [10000/50000 (20%)]\tLoss: 2001.901094\n","Train Epoch: 168 [20000/50000 (40%)]\tLoss: 1999.384687\n","Train Epoch: 168 [30000/50000 (60%)]\tLoss: 1988.132188\n","Train Epoch: 168 [40000/50000 (80%)]\tLoss: 2022.689063\n","====> Epoch: 168 Average loss: 2001.8818\n","====> Test set loss: 1999.8819\n","_________________________________\n","Train Epoch: 169 [0/50000 (0%)]\tLoss: 2032.145469\n","Train Epoch: 169 [10000/50000 (20%)]\tLoss: 2028.943281\n","Train Epoch: 169 [20000/50000 (40%)]\tLoss: 1995.477344\n","Train Epoch: 169 [30000/50000 (60%)]\tLoss: 2030.490313\n","Train Epoch: 169 [40000/50000 (80%)]\tLoss: 2012.537188\n","====> Epoch: 169 Average loss: 2001.9128\n","====> Test set loss: 2000.2709\n","_________________________________\n","Train Epoch: 170 [0/50000 (0%)]\tLoss: 2033.684688\n","Train Epoch: 170 [10000/50000 (20%)]\tLoss: 1984.195781\n","Train Epoch: 170 [20000/50000 (40%)]\tLoss: 2004.685781\n","Train Epoch: 170 [30000/50000 (60%)]\tLoss: 1994.584844\n","Train Epoch: 170 [40000/50000 (80%)]\tLoss: 2030.231563\n","====> Epoch: 170 Average loss: 2001.8654\n","====> Test set loss: 1999.9395\n","_________________________________\n","Train Epoch: 171 [0/50000 (0%)]\tLoss: 2032.130469\n","Train Epoch: 171 [10000/50000 (20%)]\tLoss: 2015.864844\n","Train Epoch: 171 [20000/50000 (40%)]\tLoss: 1989.589219\n","Train Epoch: 171 [30000/50000 (60%)]\tLoss: 2015.158594\n","Train Epoch: 171 [40000/50000 (80%)]\tLoss: 2028.847500\n","====> Epoch: 171 Average loss: 2001.8694\n","====> Test set loss: 1999.9877\n","_________________________________\n","Train Epoch: 172 [0/50000 (0%)]\tLoss: 1996.845313\n","Train Epoch: 172 [10000/50000 (20%)]\tLoss: 2023.203438\n","Train Epoch: 172 [20000/50000 (40%)]\tLoss: 1971.344063\n","Train Epoch: 172 [30000/50000 (60%)]\tLoss: 2026.431094\n","Train Epoch: 172 [40000/50000 (80%)]\tLoss: 2013.912188\n","====> Epoch: 172 Average loss: 2001.8084\n","====> Test set loss: 1999.8601\n","_________________________________\n","Train Epoch: 173 [0/50000 (0%)]\tLoss: 1990.063906\n","Train Epoch: 173 [10000/50000 (20%)]\tLoss: 1992.898438\n","Train Epoch: 173 [20000/50000 (40%)]\tLoss: 2009.203750\n","Train Epoch: 173 [30000/50000 (60%)]\tLoss: 1991.849219\n","Train Epoch: 173 [40000/50000 (80%)]\tLoss: 1992.314531\n","====> Epoch: 173 Average loss: 2001.8668\n","====> Test set loss: 2000.0710\n","_________________________________\n","Train Epoch: 174 [0/50000 (0%)]\tLoss: 2014.185625\n","Train Epoch: 174 [10000/50000 (20%)]\tLoss: 2008.624063\n","Train Epoch: 174 [20000/50000 (40%)]\tLoss: 2017.678125\n","Train Epoch: 174 [30000/50000 (60%)]\tLoss: 1999.264375\n","Train Epoch: 174 [40000/50000 (80%)]\tLoss: 2010.859844\n","====> Epoch: 174 Average loss: 2001.8541\n","====> Test set loss: 1999.8279\n","_________________________________\n","Train Epoch: 175 [0/50000 (0%)]\tLoss: 2012.998906\n","Train Epoch: 175 [10000/50000 (20%)]\tLoss: 1945.287344\n","Train Epoch: 175 [20000/50000 (40%)]\tLoss: 2005.466406\n","Train Epoch: 175 [30000/50000 (60%)]\tLoss: 2021.205312\n","Train Epoch: 175 [40000/50000 (80%)]\tLoss: 2033.131406\n","====> Epoch: 175 Average loss: 2001.8810\n","====> Test set loss: 2000.0003\n","_________________________________\n","Train Epoch: 176 [0/50000 (0%)]\tLoss: 2027.575469\n","Train Epoch: 176 [10000/50000 (20%)]\tLoss: 1987.337500\n","Train Epoch: 176 [20000/50000 (40%)]\tLoss: 2025.178906\n","Train Epoch: 176 [30000/50000 (60%)]\tLoss: 1999.012031\n","Train Epoch: 176 [40000/50000 (80%)]\tLoss: 1963.455625\n","====> Epoch: 176 Average loss: 2001.8750\n","====> Test set loss: 1999.8917\n","_________________________________\n","Train Epoch: 177 [0/50000 (0%)]\tLoss: 1986.348750\n","Train Epoch: 177 [10000/50000 (20%)]\tLoss: 1993.887344\n","Train Epoch: 177 [20000/50000 (40%)]\tLoss: 2016.246250\n","Train Epoch: 177 [30000/50000 (60%)]\tLoss: 1971.388750\n","Train Epoch: 177 [40000/50000 (80%)]\tLoss: 1986.771250\n","====> Epoch: 177 Average loss: 2001.8059\n","====> Test set loss: 2000.1069\n","_________________________________\n","Train Epoch: 178 [0/50000 (0%)]\tLoss: 2016.112187\n","Train Epoch: 178 [10000/50000 (20%)]\tLoss: 1984.119844\n","Train Epoch: 178 [20000/50000 (40%)]\tLoss: 2011.970781\n","Train Epoch: 178 [30000/50000 (60%)]\tLoss: 1966.654375\n","Train Epoch: 178 [40000/50000 (80%)]\tLoss: 2003.675000\n","====> Epoch: 178 Average loss: 2001.8587\n","====> Test set loss: 2000.0069\n","_________________________________\n","Train Epoch: 179 [0/50000 (0%)]\tLoss: 1980.457031\n","Train Epoch: 179 [10000/50000 (20%)]\tLoss: 2001.227656\n","Train Epoch: 179 [20000/50000 (40%)]\tLoss: 1967.148125\n","Train Epoch: 179 [30000/50000 (60%)]\tLoss: 2004.047344\n","Train Epoch: 179 [40000/50000 (80%)]\tLoss: 2006.100938\n","====> Epoch: 179 Average loss: 2001.8340\n","====> Test set loss: 2000.0062\n","_________________________________\n","Train Epoch: 180 [0/50000 (0%)]\tLoss: 2026.040937\n","Train Epoch: 180 [10000/50000 (20%)]\tLoss: 1997.976719\n","Train Epoch: 180 [20000/50000 (40%)]\tLoss: 1963.030625\n","Train Epoch: 180 [30000/50000 (60%)]\tLoss: 1981.390000\n","Train Epoch: 180 [40000/50000 (80%)]\tLoss: 2002.158750\n","====> Epoch: 180 Average loss: 2001.8482\n","====> Test set loss: 1999.8651\n","_________________________________\n","Train Epoch: 181 [0/50000 (0%)]\tLoss: 1962.772187\n","Train Epoch: 181 [10000/50000 (20%)]\tLoss: 2035.583437\n","Train Epoch: 181 [20000/50000 (40%)]\tLoss: 2010.147344\n","Train Epoch: 181 [30000/50000 (60%)]\tLoss: 1999.165781\n","Train Epoch: 181 [40000/50000 (80%)]\tLoss: 1988.085469\n","====> Epoch: 181 Average loss: 2001.8378\n","====> Test set loss: 1999.9254\n","_________________________________\n","Train Epoch: 182 [0/50000 (0%)]\tLoss: 1947.187187\n","Train Epoch: 182 [10000/50000 (20%)]\tLoss: 2002.220000\n","Train Epoch: 182 [20000/50000 (40%)]\tLoss: 2023.287500\n","Train Epoch: 182 [30000/50000 (60%)]\tLoss: 1997.230000\n","Train Epoch: 182 [40000/50000 (80%)]\tLoss: 2019.141562\n","====> Epoch: 182 Average loss: 2001.8120\n","====> Test set loss: 1999.9193\n","_________________________________\n","Train Epoch: 183 [0/50000 (0%)]\tLoss: 1989.052344\n","Train Epoch: 183 [10000/50000 (20%)]\tLoss: 1996.500625\n","Train Epoch: 183 [20000/50000 (40%)]\tLoss: 2028.863594\n","Train Epoch: 183 [30000/50000 (60%)]\tLoss: 2013.806094\n","Train Epoch: 183 [40000/50000 (80%)]\tLoss: 2034.846406\n","====> Epoch: 183 Average loss: 2001.8548\n","====> Test set loss: 1999.9547\n","_________________________________\n","Train Epoch: 184 [0/50000 (0%)]\tLoss: 2030.103594\n","Train Epoch: 184 [10000/50000 (20%)]\tLoss: 1997.754688\n","Train Epoch: 184 [20000/50000 (40%)]\tLoss: 2023.392188\n","Train Epoch: 184 [30000/50000 (60%)]\tLoss: 1978.084844\n","Train Epoch: 184 [40000/50000 (80%)]\tLoss: 2031.852187\n","====> Epoch: 184 Average loss: 2001.8392\n","====> Test set loss: 1999.9981\n","_________________________________\n","Train Epoch: 185 [0/50000 (0%)]\tLoss: 2013.297031\n","Train Epoch: 185 [10000/50000 (20%)]\tLoss: 1966.399062\n","Train Epoch: 185 [20000/50000 (40%)]\tLoss: 2020.822813\n","Train Epoch: 185 [30000/50000 (60%)]\tLoss: 2025.444219\n","Train Epoch: 185 [40000/50000 (80%)]\tLoss: 2024.071406\n","====> Epoch: 185 Average loss: 2001.8505\n","====> Test set loss: 2000.1310\n","_________________________________\n","Train Epoch: 186 [0/50000 (0%)]\tLoss: 1998.287188\n","Train Epoch: 186 [10000/50000 (20%)]\tLoss: 2019.762187\n","Train Epoch: 186 [20000/50000 (40%)]\tLoss: 2005.070000\n","Train Epoch: 186 [30000/50000 (60%)]\tLoss: 2002.864063\n","Train Epoch: 186 [40000/50000 (80%)]\tLoss: 2008.306719\n","====> Epoch: 186 Average loss: 2001.8226\n","====> Test set loss: 1999.8972\n","_________________________________\n","Train Epoch: 187 [0/50000 (0%)]\tLoss: 1986.085313\n","Train Epoch: 187 [10000/50000 (20%)]\tLoss: 2009.187813\n","Train Epoch: 187 [20000/50000 (40%)]\tLoss: 1984.518594\n","Train Epoch: 187 [30000/50000 (60%)]\tLoss: 1996.798594\n","Train Epoch: 187 [40000/50000 (80%)]\tLoss: 2013.786875\n","====> Epoch: 187 Average loss: 2001.8174\n","====> Test set loss: 1999.8496\n","_________________________________\n","Train Epoch: 188 [0/50000 (0%)]\tLoss: 1988.291875\n","Train Epoch: 188 [10000/50000 (20%)]\tLoss: 2003.153906\n","Train Epoch: 188 [20000/50000 (40%)]\tLoss: 2040.182812\n","Train Epoch: 188 [30000/50000 (60%)]\tLoss: 2010.575156\n","Train Epoch: 188 [40000/50000 (80%)]\tLoss: 2009.062187\n","====> Epoch: 188 Average loss: 2001.8789\n","====> Test set loss: 1999.8977\n","_________________________________\n","Train Epoch: 189 [0/50000 (0%)]\tLoss: 2004.022031\n","Train Epoch: 189 [10000/50000 (20%)]\tLoss: 1990.820000\n","Train Epoch: 189 [20000/50000 (40%)]\tLoss: 2001.084687\n","Train Epoch: 189 [30000/50000 (60%)]\tLoss: 2007.576250\n","Train Epoch: 189 [40000/50000 (80%)]\tLoss: 2003.723750\n","====> Epoch: 189 Average loss: 2001.7868\n","====> Test set loss: 2000.0894\n","_________________________________\n","Train Epoch: 190 [0/50000 (0%)]\tLoss: 2018.528750\n","Train Epoch: 190 [10000/50000 (20%)]\tLoss: 1995.541719\n","Train Epoch: 190 [20000/50000 (40%)]\tLoss: 1991.875000\n","Train Epoch: 190 [30000/50000 (60%)]\tLoss: 1958.292969\n","Train Epoch: 190 [40000/50000 (80%)]\tLoss: 2020.595000\n","====> Epoch: 190 Average loss: 2001.8257\n","====> Test set loss: 1999.7634\n","_________________________________\n","Train Epoch: 191 [0/50000 (0%)]\tLoss: 2013.970000\n","Train Epoch: 191 [10000/50000 (20%)]\tLoss: 2010.068594\n","Train Epoch: 191 [20000/50000 (40%)]\tLoss: 1990.906406\n","Train Epoch: 191 [30000/50000 (60%)]\tLoss: 2000.233438\n","Train Epoch: 191 [40000/50000 (80%)]\tLoss: 2004.207187\n","====> Epoch: 191 Average loss: 2001.8048\n","====> Test set loss: 1999.9722\n","_________________________________\n","Train Epoch: 192 [0/50000 (0%)]\tLoss: 2026.335625\n","Train Epoch: 192 [10000/50000 (20%)]\tLoss: 1992.887969\n","Train Epoch: 192 [20000/50000 (40%)]\tLoss: 2001.276250\n","Train Epoch: 192 [30000/50000 (60%)]\tLoss: 1999.543750\n","Train Epoch: 192 [40000/50000 (80%)]\tLoss: 2026.636406\n","====> Epoch: 192 Average loss: 2001.7629\n","====> Test set loss: 2000.1851\n","_________________________________\n","Train Epoch: 193 [0/50000 (0%)]\tLoss: 1987.402188\n","Train Epoch: 193 [10000/50000 (20%)]\tLoss: 2048.117031\n","Train Epoch: 193 [20000/50000 (40%)]\tLoss: 1988.056094\n","Train Epoch: 193 [30000/50000 (60%)]\tLoss: 1992.399531\n","Train Epoch: 193 [40000/50000 (80%)]\tLoss: 2029.128906\n","====> Epoch: 193 Average loss: 2001.8260\n","====> Test set loss: 1999.9980\n","_________________________________\n","Train Epoch: 194 [0/50000 (0%)]\tLoss: 1996.649531\n","Train Epoch: 194 [10000/50000 (20%)]\tLoss: 2009.232656\n","Train Epoch: 194 [20000/50000 (40%)]\tLoss: 1984.079688\n","Train Epoch: 194 [30000/50000 (60%)]\tLoss: 2023.022500\n","Train Epoch: 194 [40000/50000 (80%)]\tLoss: 2001.320625\n","====> Epoch: 194 Average loss: 2001.7813\n","====> Test set loss: 1999.9448\n","_________________________________\n","Train Epoch: 195 [0/50000 (0%)]\tLoss: 1992.696406\n","Train Epoch: 195 [10000/50000 (20%)]\tLoss: 1981.633750\n","Train Epoch: 195 [20000/50000 (40%)]\tLoss: 2027.611562\n","Train Epoch: 195 [30000/50000 (60%)]\tLoss: 2040.352344\n","Train Epoch: 195 [40000/50000 (80%)]\tLoss: 1985.893125\n","====> Epoch: 195 Average loss: 2001.7805\n","====> Test set loss: 1999.8263\n","_________________________________\n","Train Epoch: 196 [0/50000 (0%)]\tLoss: 2003.028125\n","Train Epoch: 196 [10000/50000 (20%)]\tLoss: 1992.450000\n","Train Epoch: 196 [20000/50000 (40%)]\tLoss: 1975.083437\n","Train Epoch: 196 [30000/50000 (60%)]\tLoss: 1997.375000\n","Train Epoch: 196 [40000/50000 (80%)]\tLoss: 2017.320781\n","====> Epoch: 196 Average loss: 2001.7730\n","====> Test set loss: 2000.3300\n","_________________________________\n","Train Epoch: 197 [0/50000 (0%)]\tLoss: 2004.350000\n","Train Epoch: 197 [10000/50000 (20%)]\tLoss: 2011.218594\n","Train Epoch: 197 [20000/50000 (40%)]\tLoss: 1996.076719\n","Train Epoch: 197 [30000/50000 (60%)]\tLoss: 2036.923125\n","Train Epoch: 197 [40000/50000 (80%)]\tLoss: 2026.775312\n","====> Epoch: 197 Average loss: 2001.8174\n","====> Test set loss: 2000.0137\n","_________________________________\n","Train Epoch: 198 [0/50000 (0%)]\tLoss: 1991.939063\n","Train Epoch: 198 [10000/50000 (20%)]\tLoss: 1999.627656\n","Train Epoch: 198 [20000/50000 (40%)]\tLoss: 1978.425469\n","Train Epoch: 198 [30000/50000 (60%)]\tLoss: 2012.016406\n","Train Epoch: 198 [40000/50000 (80%)]\tLoss: 1992.339688\n","====> Epoch: 198 Average loss: 2001.8193\n","====> Test set loss: 1999.8284\n","_________________________________\n","Train Epoch: 199 [0/50000 (0%)]\tLoss: 1994.328281\n","Train Epoch: 199 [10000/50000 (20%)]\tLoss: 1993.667344\n","Train Epoch: 199 [20000/50000 (40%)]\tLoss: 2010.202656\n","Train Epoch: 199 [30000/50000 (60%)]\tLoss: 1991.457500\n","Train Epoch: 199 [40000/50000 (80%)]\tLoss: 1996.969063\n","====> Epoch: 199 Average loss: 2001.7894\n","====> Test set loss: 2000.1718\n","_________________________________\n","Train Epoch: 200 [0/50000 (0%)]\tLoss: 1989.763125\n","Train Epoch: 200 [10000/50000 (20%)]\tLoss: 2026.992656\n","Train Epoch: 200 [20000/50000 (40%)]\tLoss: 2011.162812\n","Train Epoch: 200 [30000/50000 (60%)]\tLoss: 1991.657813\n","Train Epoch: 200 [40000/50000 (80%)]\tLoss: 1985.945625\n","====> Epoch: 200 Average loss: 2001.7441\n","====> Test set loss: 1999.8000\n"]}],"source":["LS = 5\n","MODEL = ConvVAE2\n","\n","cifar10_vae = MODEL(img_dim = 32, latent_size=LS, channels=3)\n","cifar10_vae.cuda()\n","cifar10_optimizer = optim.Adam(cifar10_vae.parameters())\n","summary(cifar10_vae, (3,32,32))\n","\n","(cifar10_train, cifar10_test) = TRAIN_VAE(CIFAR10_EPOCHS, cifar10_vae, cifar10_optimizer, cifar10_data_loader, mnist=False)\n","\n","numpy.savetxt(\"/content/drive/MyDrive/Colab Notebooks/CSI5340_A4/output/vae/cifar10_trainingLoss.csv\", numpy.asarray(cifar10_train), delimiter=\", \")\n","\n","save_Sample_CIFAR10(cifar10_vae, img_dim=32, latent_size=LS, title=\"vae/cifar10_sample\")"]},{"cell_type":"markdown","source":["# Outputing Graphs"],"metadata":{"id":"15pbJnniDEtp"}},{"cell_type":"code","source":["cifar10_trainingLoss=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/CSI5340_A4/output/vae/cifar10_trainingLoss.csv', header=None)\n","mnist_trainingLoss=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/CSI5340_A4/output/vae/mnist_trainingLoss.csv', header=None)"],"metadata":{"id":"Oo8JLGVV7Yju"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hgBkHDe2BFw_","colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"status":"ok","timestamp":1668981536513,"user_tz":300,"elapsed":346,"user":{"displayName":"Alexandra Sklokin","userId":"06841525493811584768"}},"outputId":"1a24284f-3cd2-42da-d932-973c369bbbf0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["               0\n","0    2097.779571\n","1    2029.724962\n","2    2017.625116\n","3    2014.162689\n","4    2012.910650\n","..           ...\n","195  2001.773009\n","196  2001.817387\n","197  2001.819295\n","198  2001.789382\n","199  2001.744138\n","\n","[200 rows x 1 columns]"],"text/html":["\n","  <div id=\"df-ff94b7eb-3bc6-4b43-893b-21c6d280b97e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2097.779571</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2029.724962</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2017.625116</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2014.162689</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2012.910650</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>195</th>\n","      <td>2001.773009</td>\n","    </tr>\n","    <tr>\n","      <th>196</th>\n","      <td>2001.817387</td>\n","    </tr>\n","    <tr>\n","      <th>197</th>\n","      <td>2001.819295</td>\n","    </tr>\n","    <tr>\n","      <th>198</th>\n","      <td>2001.789382</td>\n","    </tr>\n","    <tr>\n","      <th>199</th>\n","      <td>2001.744138</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>200 rows × 1 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ff94b7eb-3bc6-4b43-893b-21c6d280b97e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-ff94b7eb-3bc6-4b43-893b-21c6d280b97e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-ff94b7eb-3bc6-4b43-893b-21c6d280b97e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":12}],"source":["cifar10_trainingLoss"]},{"cell_type":"code","source":["# create a line plot of loss for the gan and save to file\n","def plot_history(lossFunction, title):\n","\tpyplot.plot(lossFunction)\n","\tpyplot.xlabel(\"Epochs\")\n","\tpyplot.ylabel(\"Loss\")\n","\tpyplot.savefig('/content/drive/MyDrive/Colab Notebooks/CSI5340_A4/output/vae/'+title+'_trainingLoss.png')\n","\tpyplot.close()"],"metadata":{"id":"aU8k77GLDUQ5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_history(cifar10_trainingLoss, title=\"cifar10\")"],"metadata":{"id":"ozuDaSulDooR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_history(mnist_trainingLoss, title=\"mnist\")"],"metadata":{"id":"kSk0PlioD5ya"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[{"file_id":"https://github.com/alexandrasklokin/CSI5340/blob/main/CSI5340_A4/VAE.ipynb","timestamp":1668477872364}],"machine_shape":"hm"},"accelerator":"GPU","gpuClass":"premium","widgets":{"application/vnd.jupyter.widget-state+json":{"c364964415704e23b9aa8c3ace60ea4f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_983329236e074cbd9d4645b0fb2e42f7","IPY_MODEL_625ecc0da96840bb99b4cdeda686b248","IPY_MODEL_8cdf869306724d61b26d2a647a522154"],"layout":"IPY_MODEL_2c94b530bf3543fe8552fd44ac416ff9"}},"983329236e074cbd9d4645b0fb2e42f7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e137ddc40b95460d997fac0cdd026608","placeholder":"​","style":"IPY_MODEL_a8e83639a7b24fdf92c8094115139f52","value":"100%"}},"625ecc0da96840bb99b4cdeda686b248":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a14295da453a418c8ae7ba969d3b1afc","max":9912422,"min":0,"orientation":"horizontal","style":"IPY_MODEL_09ac13897046446db3e5acf96c229a45","value":9912422}},"8cdf869306724d61b26d2a647a522154":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f94def75db9c49d6ad7c35155c3fdec6","placeholder":"​","style":"IPY_MODEL_aa8334f7c4f344f3b5c3e3b48411b71a","value":" 9912422/9912422 [00:00&lt;00:00, 18627149.70it/s]"}},"2c94b530bf3543fe8552fd44ac416ff9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e137ddc40b95460d997fac0cdd026608":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8e83639a7b24fdf92c8094115139f52":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a14295da453a418c8ae7ba969d3b1afc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09ac13897046446db3e5acf96c229a45":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f94def75db9c49d6ad7c35155c3fdec6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa8334f7c4f344f3b5c3e3b48411b71a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cd64e1ea6dda4c629e415ff7296f05ca":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b112034aa6ae4287b775773f566304f4","IPY_MODEL_07afb1549962437bb8b328250968fcb4","IPY_MODEL_77f822ae4d3442f8befadcdd49ac715d"],"layout":"IPY_MODEL_e1c2ebebe97042afad332aac9b91cfe5"}},"b112034aa6ae4287b775773f566304f4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b15de020e3c44b08c229cc2884f26df","placeholder":"​","style":"IPY_MODEL_8a8df30d972f4237bda4d3445a4c787d","value":"100%"}},"07afb1549962437bb8b328250968fcb4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5517d976eb7e4b37968907b64f35ba2a","max":28881,"min":0,"orientation":"horizontal","style":"IPY_MODEL_74681e7488b34f69aa5f4d64c916c8fa","value":28881}},"77f822ae4d3442f8befadcdd49ac715d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0fe1368dd74f4ee3a8ccaa13c6e10b89","placeholder":"​","style":"IPY_MODEL_c020d78a300a4c99b02d706fba5a95ad","value":" 28881/28881 [00:00&lt;00:00, 1132469.14it/s]"}},"e1c2ebebe97042afad332aac9b91cfe5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b15de020e3c44b08c229cc2884f26df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a8df30d972f4237bda4d3445a4c787d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5517d976eb7e4b37968907b64f35ba2a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74681e7488b34f69aa5f4d64c916c8fa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0fe1368dd74f4ee3a8ccaa13c6e10b89":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c020d78a300a4c99b02d706fba5a95ad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a417ca31637b40ffa7df456e31dfb92e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8aeb05876d9045fcb88919cfa8a507d0","IPY_MODEL_fddfd58fde344d57a742cedab483cf8c","IPY_MODEL_13e335379f03488e8222fca2cd6bf95d"],"layout":"IPY_MODEL_d47e7f80464b4f5cb982079b707e6844"}},"8aeb05876d9045fcb88919cfa8a507d0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b252d07673f54419a805a3863f29cfce","placeholder":"​","style":"IPY_MODEL_f0894581f0204e44b6dc259eb5e06dde","value":"100%"}},"fddfd58fde344d57a742cedab483cf8c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_599f17a549ba468e813b5fd8c1ccfc18","max":1648877,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b203a662f99f47e6873f5b7aa611c81a","value":1648877}},"13e335379f03488e8222fca2cd6bf95d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d170ed4d5ba3483b8d1132154902075b","placeholder":"​","style":"IPY_MODEL_38f6c8df65df41818551f9c8b75695ee","value":" 1648877/1648877 [00:00&lt;00:00, 11077428.08it/s]"}},"d47e7f80464b4f5cb982079b707e6844":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b252d07673f54419a805a3863f29cfce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f0894581f0204e44b6dc259eb5e06dde":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"599f17a549ba468e813b5fd8c1ccfc18":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b203a662f99f47e6873f5b7aa611c81a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d170ed4d5ba3483b8d1132154902075b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38f6c8df65df41818551f9c8b75695ee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f6821420a79043018d5dc4e34cf9dc28":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0dd75013e1604c229e390726b2f366f0","IPY_MODEL_00dfbdd9725d430db844e518fe658ad7","IPY_MODEL_26e785db62ed4f4285750f452e8fabb9"],"layout":"IPY_MODEL_b98c3457db2b4fdcb7e2808b26a0ff82"}},"0dd75013e1604c229e390726b2f366f0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2c0509387e744fc389773cffb5a60de4","placeholder":"​","style":"IPY_MODEL_e03ec87ea4e64940a94a095255ead0b6","value":"100%"}},"00dfbdd9725d430db844e518fe658ad7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4273fa76c3124dd7b8c01dd6ef80f664","max":4542,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6787d5907ef34d4184e426dd248ea1a5","value":4542}},"26e785db62ed4f4285750f452e8fabb9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5873a605b5544ffdb3d6e09b4a7adfaa","placeholder":"​","style":"IPY_MODEL_f6a82f6398a24e60a50da95d9fdca93b","value":" 4542/4542 [00:00&lt;00:00, 175958.78it/s]"}},"b98c3457db2b4fdcb7e2808b26a0ff82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c0509387e744fc389773cffb5a60de4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e03ec87ea4e64940a94a095255ead0b6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4273fa76c3124dd7b8c01dd6ef80f664":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6787d5907ef34d4184e426dd248ea1a5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5873a605b5544ffdb3d6e09b4a7adfaa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6a82f6398a24e60a50da95d9fdca93b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9c5d12627ce14498b9fc110238dd3b31":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_57d39357cf3d4ff18668621ef92b46ab","IPY_MODEL_1a5a3b28e30943d395bc57238f628f59","IPY_MODEL_3447d84201c643b9b4d81d25baa79963"],"layout":"IPY_MODEL_5f01e62f4406442e84b09d235d43281f"}},"57d39357cf3d4ff18668621ef92b46ab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c57dbf51c32a445aab578600e804f57b","placeholder":"​","style":"IPY_MODEL_a7a7493969144a4ab0a4e0c244b96bdf","value":"100%"}},"1a5a3b28e30943d395bc57238f628f59":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ab4bdb5c4a2347d5ac2d461546d1389d","max":170498071,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f427149eeace48e1ade9d797d2b52ebe","value":170498071}},"3447d84201c643b9b4d81d25baa79963":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1aa657bcffa849ac802bfa4ad8cc7cb9","placeholder":"​","style":"IPY_MODEL_83349211fa1d49019258c11f251e5d01","value":" 170498071/170498071 [00:04&lt;00:00, 54857952.19it/s]"}},"5f01e62f4406442e84b09d235d43281f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c57dbf51c32a445aab578600e804f57b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7a7493969144a4ab0a4e0c244b96bdf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ab4bdb5c4a2347d5ac2d461546d1389d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f427149eeace48e1ade9d797d2b52ebe":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1aa657bcffa849ac802bfa4ad8cc7cb9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83349211fa1d49019258c11f251e5d01":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}